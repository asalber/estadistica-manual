[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manual de Estadística",
    "section": "",
    "text": "Prefacio\n¡Bienvenida/os al manual de Estadística!\nEste libro es una introducción a la Estadística básica y el cálculo de probabilidades para alumnos de grados de ciencias e ingenierías.\nEste libro se complementa con los siguientes recursos:",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#licencia",
    "href": "index.html#licencia",
    "title": "Manual de Estadística",
    "section": "Licencia",
    "text": "Licencia\nEsta obra está bajo una licencia Reconocimiento – No comercial – Compartir bajo la misma licencia 3.0 España de Creative Commons. Para ver una copia de esta licencia, visite https://creativecommons.org/licenses/by-nc-sa/3.0/es/.\nCon esta licencia eres libre de:\n\nCopiar, distribuir y mostrar este trabajo.\nRealizar modificaciones de este trabajo.\n\nBajo las siguientes condiciones:\n\n**Reconocimiento Debe reconocer los créditos de la obra de la manera especificada por el autor o el licenciador (pero no de una manera que sugiera que tiene su apoyo o apoyan el uso que hace de su obra).\n**No comercial No puede utilizar esta obra para fines comerciales.\n**Compartir bajo la misma licencia Si altera o transforma esta obra, o genera una obra derivada, sólo puede distribuir la obra generada bajo una licencia idéntica a ésta.\n\nAl reutilizar o distribuir la obra, tiene que dejar bien claro los términos de la licencia de esta obra.\nEstas condiciones pueden no aplicarse si se obtiene el permiso del titular de los derechos de autor.\nNada en esta licencia menoscaba o restringe los derechos morales del autor.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción a la Estadística",
    "section": "",
    "text": "1.1 La estadística como herramienta científica",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a la Estadística</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#la-estadística-como-herramienta-científica",
    "href": "01-introduccion.html#la-estadística-como-herramienta-científica",
    "title": "1  Introducción a la Estadística",
    "section": "",
    "text": "1.1.1 ¿Qué es la estadística?\n\nDefinición 1.1 (Estadística) La estadística es una rama de las matemáticas que se encarga de la recogida, análisis e interpretación de datos.\n\nEl papel de la Estadística es extraer información de los datos para adquirir el conocimiento necesario para tomar decisiones.\n\n\n\nPropósito de la Estadística\n\n\nLa estadística es imprescindible en cualquier disciplina científica o técnica donde se manejen datos, especialmente si son grandes volúmenes de datos, como por ejemplo en Física, Química, Medicina, Psicología, Economía o Ciencias Sociales.\nPero, ¿por qué es necesaria la Estadística?\n\n\n1.1.2 La variabilidad de nuestro mundo\nEl científico trata de estudiar el mundo que le rodea; un mundo que está lleno de variaciones que dificultan la determinación del comportamiento de las cosas.\nLa estadística actúa como disciplina puente entre la realidad del mundo y los modelos matemáticos que tratan de explicarla, proporcionando una metodología para evaluar las discrepancias entre la realidad y los modelos teóricos.\nEsto la convierte en una herramienta indispensable en las ciencias aplicadas que requieran el análisis de datos y el diseño de experimentos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a la Estadística</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#población-y-muestra",
    "href": "01-introduccion.html#población-y-muestra",
    "title": "1  Introducción a la Estadística",
    "section": "1.2 Población y muestra",
    "text": "1.2 Población y muestra\n\n1.2.1 Población estadística\n\nDefinición 1.2 (Población) Una población es un conjunto de elementos definido por una o más características que tienen todos los elementos, y sólo ellos. Cada elemento de la población se llama individuo.\n\n\nDefinición 1.3 (Tamaño poblacional) El número de individuos de una población se conoce como tamaño poblacional y se representa como \\(N\\).\n\n\nEjemplo 1.1 En unas elecciones generales a la presidencia del gobierno, la población serían todos los individuos del estado con derecho a voto. En el estudio de una enfermedad, la población sería todas las personas que tienen la enfermedad. Y en un proceso de control de calidad en la fabricación de un fármaco, la población estaría formada por todos los fármacos que se producen en la fábrica.\n\nA veces, no todos los elementos de la población están accesibles para su estudio. Entonces se distingue entre:\n\nPoblación Teórica: Conjunto de elementos a los que se quiere extrapolar los resultados del estudio.\nPoblación Estudiada: Conjunto de elementos realmente accesibles en el estudio.\n\n\nEjemplo 1.2 En el caso del estudio de una enfermedad, la población teórica sería todas las personas que contraigan la enfermedad, incluso si aún no han nacido, mientras que la población estudiada se limitaría al número de personas enfermas que realmente podemos estudiar (obsérvese que incluso quedarían fuera las personas enfermas pero de las que no podemos conseguir información).\n\n\n\n1.2.2 Inconvenientes en el estudio de la población\nEl científico estudia un determinado fenómeno en una población para comprenderlo, obtener conocimiento sobre el mismo, y así poder controlarlo. Pero, para tener un conocimiento completo de la población es necesario estudiar todos los individuos de la misma. Sin embargo, esto no siempre es posible por distintos motivos:\n\nEl tamaño de la población es infinito, o bien es finito pero demasiado grande.\nLas pruebas a que se someten los individuos son destructivas.\nEl coste, tanto de dinero como de tiempo, que supondría estudiar a todos los individuos es excesivo.\n\n\n\n1.2.3 Muestra estadística\nCuando no es posible o conveniente estudiar todos los individuos de la población, se estudia sólo una parte de la misma.\n\nDefinición 1.4 (Muestra) Una muestra es un subconjunto de la población.\n\n\nDefinición 1.5 (Tamaño muestral) Al número de individuos que componen la muestra se le llama tamaño muestral y se representa por \\(n\\).\n\nHabitualmente, el estudio de una población se realiza a partir de muestras extraídas de dicha población.\nGeneralmente, el estudio de la muestra sólo aporta conocimiento aproximado de la población. Pero en muchos casos es suficiente.\n\n\n1.2.4 Determinación del tamaño muestral\nUna de las preguntas más interesantes que surge inmediatamente es: ¿cuántos individuos es necesario tomar en la muestra para tener un conocimiento aproximado pero suficiente de la población?\nLa respuesta depende de varios factores, como la variabilidad de la población o la fiabilidad deseada para las extrapolaciones que se hagan hacia la población.\nPor desgracia no se podrá responder hasta casi el final del curso, pero en general, cuantos más individuos haya en la muestra, más fiables serán las conclusiones sobre la población, pero también será más lento y costoso el estudio.\n\nEjemplo 1.3 Para entender a qué nos referimos cuando hablamos de un tamaño muestral suficiente para comprender lo que ocurre en la población, podemos utilizar el siguiente símil en que se trata de comprender el motivo que representa una fotografía.\nUna fotografía digital está formada por multitud de pequeños puntitos llamados pixels que se dispone en una enorme tabla de filas y columnas (cuantas más filas y columnas haya se habla de que la foto tiene más resolución). Aquí la población estaría formada por todos y cada uno de los píxeles que forman la foto. Por otro lado cada pixel tiene un color y es la variedad de colores a lo largo de los pixels la que permite formar la imagen de la fotografía.\n¿Cuántos píxeles debemos tomar en una muestra para averiguar la imagen de la foto?\nLa respuesta depende de la variabilidad de colores en la foto. Si todos los pixels de la foto son del mismo color, entonces un sólo pixel basta para desvelar la imagen. Pero, si la foto tiene mucha variabilidad de colores, necesitaremos muchos más pixels en la muestra para descubrir el motivo de la foto.\nLa imagen siguiente contiene una muestra pequeña de píxeles de una foto. ¿Puedes averiguar el motivo de a foto?\n\n\n\nMuestra pequeña de píxeles de una foto.\n\n\n¡Con una muestra pequeña es difícil averiguar el contenido de la imagen!\nSeguramente no has podido averiguar el motivo de la fotografía, porque en este caso el número de píxeles que hemos tomado en la muestra es insuficiente para comprender toda la variabilidad de colores que hay en la foto.\nLa siguiente imagen contiene una muestra mayor de píxeles. ¿Eres capaz de adivinar el motivo de la foto ahora?\n\n\n\nMuestra mayor de píxeles de una foto.\n\n\n¡Con una muestra mayor es posible desvelar el motivo de la foto!\nY aquí está la población completa.\n\n\n\nPoblación de píxeles de una foto.\n\n\nLo importante es que ¡No es necesario conocer todos los píxeles para averiguar la imagen!\n\n\n\n1.2.5 Tipos de razonamiento\nAsí pues, habitualmente realizaremos el estudio de la población a partir de muestras y luego trataremos de extrapolar lo observado en la muestra al resto de la población. A este tipo de razonamiento que saca conclusiones desde la muestra hacia la población se le conoce como razonamiento inductivo.\n\n\n\nTipos de razonamiento.\n\n\n\nCaracterísticas de la deducción: Si las premisas son ciertas, garantiza la certeza de las conclusiones (es decir, si algo se cumple en la población, también se cumple en la muestra). Sin embargo, ¡no aporta conocimiento nuevo!\nCaracterísticas de la inducción: No garantiza la certeza de las conclusiones (si algo se cumple en la muestra, puede que no se cumpla en la población, así que ¡cuidado con las extrapolaciones!), pero ¡es la única forma de generar conocimiento nuevo!\n\nLa estadística se apoya fundamentalmente en el razonamiento inductivo ya que utiliza la información obtenida a partir de muestras para sacar conclusiones sobre las poblaciones. A diferencia del razonamiento deductivo que va de lo general a lo particular, o en nuestro caso de la población a la muestra, el razonamiento inductivo no garantiza la certeza de las conclusiones, por lo que debemos ser cuidadosos a la hora de generalizar sobre la población lo observado en al muestra, ya que si la muestra no es representativa de la población o contiene sesgos, las conclusiones pueden ser erróneas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a la Estadística</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#muestreo",
    "href": "01-introduccion.html#muestreo",
    "title": "1  Introducción a la Estadística",
    "section": "1.3 Muestreo",
    "text": "1.3 Muestreo\n\nDefinición 1.6 (Muestreo) El proceso de selección de los elementos que compondrán una muestra se conoce como muestreo.\n\n![](img/introduccion/muestreo.svg” alt=“Muestreo” width=“500px”&gt;\nPara que una muestra refleje información fidedigna sobre la población global debe ser representativa de la misma, lo que significa que debe reproducir a pequeña escala la variabilidad de la población.\nEl objetivo es obtener una muestra representativa de la población.\n\n1.3.1 Modalidades de muestreo\nExisten muchas técnicas de muestreo pero se pueden agrupar en dos categorías:\n\nMuestreo Aleatorio: Elección aleatoria de los individuos de la muestra. Todos tienen la misma probabilidad de ser elegidos (equiprobabilidad).\nMuestreo No Aleatorio: Los individuos se eligen de forma no aleatoria. Algunos individuos tienen más probabilidad de ser seleccionados que otros.\n\nSólo las técnicas aleatorias evitan el sesgo de selección, y por tanto, garantizan la representatividad de la muestra extraída, y en consecuencia la validez de las conclusiones.\nLas técnicas no aleatorias no sirven para hacer generalizaciones, ya que no garantizan la representatividad de la muestra. Sin embargo, son menos costosas y pueden utilizarse en estudios exploratorios.\n\n\n1.3.2 Muestreo aleatorio simple\nDentro de las modalidades de muestreo aleatorio, el tipo más conocido es el muestreo aleatorio simple, caracterizado por:\n\nTodos los individuos de la población tienen la misma probabilidad de ser elegidos para la muestra.\nLa selección de individuos es con reemplazamiento, es decir, cada individuo seleccionado es devuelto a la población antes de seleccionar al siguiente (y por tanto no se altera la población de partida).\nLas sucesivas selecciones de un individuo son independientes.\n\nLa única forma de realizar un muestreo aleatorio es asignar un número a cada individuo de la población (censo) y realizar un sorteo aleatorio.\n\n\n1.3.3 Variables estadísticas\nTodo estudio estadístico comienza por la identificación de las características que interesa estudiar en la población y que se medirán en los individuos de la muestra.\n\nDefinición 1.7 (Variable estadística) Una variable estadística es una propiedad o característica medida en los individuos de la población.\nLos datos son los valores observados en las variables estadísticas.\n\n\n\n\nVariables estadísticas.\n\n\nEstas características pueden ser de distintos tipos de acuerdo a su naturaleza y su escala:\n\nVariables cualitativas o atributos: Miden cualidades no numéricas. Pueden ser:\n\nNominales: No existe un orden entre las categorías.\nEjemplo: El color de pelo o el sexo.\nOrdinales: Existe un orden entre las categorías. Ejemplo: El nivel de estudios o la gravedad de una enfermedad.\n\nVariables cuantitativas: Miden cantidades numéricas. Pueden ser:\n\nDiscretas: Toman valores numéricos aislados (habitualmente números enteros).\nEjemplo: El número de hijos o el número de coches en una familia.\nContinuas: Pueden tomar cualquier valor en un intervalo real.\nEjemplo: El peso o la estatura.\n\n\nLas variables cualitativas y discretas se conocen también con variables categóricas y sus valores categorías.\n\n\n\nTipos de variables estadísticas.\n\n\n\n1.3.3.1 Elección del tipo de variable más apropiado\nEn ocasiones una característica puede medirse mediante variables de distinto tipo.\n\nEjemplo 1.4 Si una persona fuma o no podría medirse de diferentes formas:\n\nFuma: si/no. (Nominal)\nNivel de fumador: No fuma / ocasional / moderado / bastante / empedernido. (Ordinal)\nNúmero de cigarros diarios: 0,1,2,… (Discreta)\n\n\nEn estos casos es preferible usar variables cuantitativas a cualitativas. Dentro de las cuantitativas es preferible usar las continuas a las discretas y dentro de las cualitativas es preferible usar ordinales a nominales pues aportan más información.\n\n\n\nCantidad de información de los tipos de variables estadísticas.\n\n\nDe acuerdo al papel que juegan en el estudio las variables también pueden clasificarse como:\n\nVariables independientes: Variables que supuestamente no dependen de otras variables en el estudio. Habitualmente son las variables manipuladas en el experimento para ver su efecto en las variables dependientes. Se conocen también como variables predictivas.\nVariables dependientes: Variables que supuestamente dependen de otras variables en el estudio. No son manipuladas en el experimento y también se conocen como variables respuesta.\n\n\nEjemplo 1.5 En un estudio sobre el rendimiento de los alumnos de un curso, la inteligencia de los alumnos y el número de horas de estudio diarias serían variables independientes y la nota del curso sería una variable dependiente.\n\n\n\n\n1.3.4 Tipos de estudios estadísticos\nDependiendo de si se manipulan las variables independientes existen dos tipos de estudios:\n\nExperimentales: Cuando las variables independientes son manipuladas para ver el efecto que producen en las variables dependientes.\n\n\nEjemplo 1.6 En un estudio sobre el rendimiento de los estudiantes en un test, el profesor manipula la metodología de estudio para crear dos o más grupos con metodologías de estudio distintas.\n\n\nNo experimentales: Cuando las variables independientes no son manipuladas. Esto no significa que sea imposible hacerlo, sino que es difícil o poco ético hacerlo.\n\n\nEjemplo 1.7 En un estudio un investigador puede estar interesado en el efecto de fumar sobre el cáncer de pulmón. Aunque es posible, no sería ético pedirle a los pacientes que fumasen para ver el efecto que tiene sobre sus pulmones. En este caso, el investigador podría estudiar dos grupos de pacientes, uno con cáncer de pulmón y otro sin cáncer, y observar en cada grupo cuántos fuman o no.\n\nLos estudios experimentales permiten identificar causas y efectos entre las variables del estudio, mientras que los no experimentales sólo permiten identificar relaciones de asociación entre las variables.\n\n\n1.3.5 La tabla de datos\nLas variables a estudiar se medirán en cada uno de los individuos de la muestra, obteniendo un conjunto de datos que suele organizarse en forma de matriz que se conoce como tabla de datos_.\nEn esta tabla cada columna contiene la información de una variable y cada fila la información de un individuo.\n\nEjemplo 1.8 La siguiente tabla contiene información de las variables Nombre, Edad, Sexo, Peso y Altura de una muestra de 6 personas.\n\n\n\nNombre\nEdad\nSexo\nPeso(Kg)\nAltura(cm)\n\n\n\n\nJosé Luis Martínez\n18\nH\n85\n179\n\n\nRosa Díaz\n32\nM\n65\n173\n\n\nJavier García\n24\nH\n71\n181\n\n\nCarmen López\n35\nM\n65\n170\n\n\nMarisa López\n46\nM\n51\n158\n\n\nAntonio Ruiz\n68\nH\n66\n174\n\n\n\n\n\n\n1.3.6 Fases del análisis estadístico\nNormalmente un estudio estadístico pasa por las siguientes etapas:\n\nEl estudio comienza por el diseño previo del mismo en el que se establezcan los objetivos del mismo, la población, las variables que se medirán y el tamaño muestral requerido.\nA continuación se seleccionará una muestra representativa del tamaño establecido y se medirán las variables en los individuos de la muestra obteniendo la tabla de datos. De esto se encarga el Muestreo.\nEl siguiente paso consiste en describir y resumir la información que contiene la muestra. De esto se encarga la Estadística Descriptiva.\nLa información obtenida es proyectada sobre un modelo matemático que intenta explicar el comportamiento de la población y el modelo se valida. De todo esto se encarga la Estadística Inferencial.\nFinalmente, el modelo validado nos permite hacer predicciones y sacar conclusiones sobre la población de partida con cierta confianza.\n\n\n1.3.6.1 El ciclo estadístico\n\n\n\nEl ciclo estadístico.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a la Estadística</span>"
    ]
  },
  {
    "objectID": "02-estadistica-descriptiva.html",
    "href": "02-estadistica-descriptiva.html",
    "title": "2  Estadística Descriptiva",
    "section": "",
    "text": "2.1 Distribución de frecuencias\nEl estudio de una variable estadística comienza por medir la variable en los individuos de la muestra y clasificar los valores obtenidos.\nExisten dos formas de clasificar estos valores:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estadística Descriptiva</span>"
    ]
  },
  {
    "objectID": "02-estadistica-descriptiva.html#distribución-de-frecuencias",
    "href": "02-estadistica-descriptiva.html#distribución-de-frecuencias",
    "title": "2  Estadística Descriptiva",
    "section": "",
    "text": "Sin agrupar: Ordenar todos los valores obtenidos en la muestra de menor a mayor. Se utiliza con atributos y variables discretas con pocos valores diferentes.\nAgrupados: Agrupar los valores en clases (intervalos) y ordenar dichas clases de menor a mayor. Se utiliza con variables continuas y con variables discretas con muchos valores diferentes.\n\n\n2.1.1 Clasificación de la muestra\nConsiste colocar juntos los valores iguales y ordenarlos si existe un orden entre ellos.\n\n\n\nClasificación de la muestra.\n\n\n\n\n2.1.2 Recuento de frecuencias\n\n\n\nRecuento de frecuencias",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estadística Descriptiva</span>"
    ]
  },
  {
    "objectID": "02-estadistica-descriptiva.html#frecuencias-muestrales",
    "href": "02-estadistica-descriptiva.html#frecuencias-muestrales",
    "title": "2  Estadística Descriptiva",
    "section": "2.2 Frecuencias muestrales",
    "text": "2.2 Frecuencias muestrales\n\nDefinición 2.1 (Frecuencias muestrales) Dada una muestra de tamaño \\(n\\) de una variable \\(X\\), para cada valor de la variable \\(x_i\\) observado en la muestra, se define\n\nFrecuencia Absoluta \\(n_i\\): Es el número de veces que el valor \\(x_i\\) aparece en la muestra.\nFrecuencia Relativa \\(f_i\\): Es la proporción de veces que el valor \\(x_i\\) aparece en la muestra. \\[f_i = \\frac{n_i}{n}\\]\nFrecuencia Absoluta Acumulada \\(N_i\\): Es el número de valores en la muestra menores o iguales que \\(x_i\\). \\[N_i = n_1 + \\cdots + n_i = N_{i-1}+n_i\\]\nFrecuencia Relativa Acumulada \\(F_i\\): Es la proporción de valores en la muestra menores o iguales que \\(x_i\\). \\[F_i = \\frac{N_i}{n}\\]\n\n\n\n2.2.1 Tabla de frecuencias\nAl conjunto de valores observados en la muestra junto a sus respectivas frecuencias se le denomina distribución de frecuencias y suele representarse mediante una tabla de frecuencias.\n\n\n\n\n\n\n\n\n\n\nValores de \\(X\\)\nFrecuencia Absoluta\nFrecuencia Relativa\nFrecuencia Absoluta Acumulada\nFrecuencia Relativa Acumulada\n\n\n\n\n\\(x_1\\)\n\\(n_1\\)\n\\(f_1\\)\n\\(N_1\\)\n\\(F_1\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_i\\)\n\\(n_i\\)\n\\(f_i\\)\n\\(N_i\\)\n\\(F_i\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_k\\)\n\\(n_k\\)\n\\(f_k\\)\n\\(N_k\\)\n\\(F_k\\)\n\n\n\n\nEjemplo 2.1 (Variable cuantitativa y datos no agrupados) El número de hijos en 25 familias es:\n\n1, 2, 4, 2, 2, 2, 3, 2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 2, 3, 1, 2, 2, 1, 2\n\nLa tabla de frecuencias del número de hijos en esta muestra es\n\\[\n\\begin{array}{rrrrr}\n\\hline\nx_i & n_i & f_i & N_i & F_i\\\\\n\\hline\n0 & 2 & 0.08 & 2 & 0.08\\\\\n1 & 6 & 0.24 & 8 & 0.32\\\\\n2 & 14 & 0.56 & 22 & 0.88\\\\\n3 & 2 & 0.08 & 24 & 0.96\\\\\n4 & 1 & 0.04 & 25 & 1 \\\\\n\\hline\n\\sum & 25 & 1 \\\\\n\\hline\n\\end{array}\n\\]\n\n\nEjemplo 2.2 (Variable cuantitativa y datos agrupados) Se ha medido la estatura (en cm) de 30 universitarios obteniendo:\n\n179, 173, 181, 170, 158, 174, 172, 166, 194, 185, 162, 187, 198, 177, 178, 165, 154, 188, 166, 171, 175, 182, 167, 169, 172, 186, 172, 176, 168, 187.\n\nLa tabla de frecuencias de la estatura en a esta muestra es\n\\[\n\\begin{array}{crrrr}\n\\hline\nx_i & n_i & f_i & N_i & F_i\\\\\n\\hline\n(150,160] & 2 & 0.07 & 2 & 0.07\\\\\n(160,170] & 8 & 0.27 & 10 & 0.34\\\\\n(170,180] & 11 & 0.36 & 21 & 0.70\\\\\n(180,190] & 7 & 0.23 & 28 & 0.93\\\\\n(190,200] & 2 & 0.07 & 30 & 1 \\\\\n\\hline\n\\sum & 30 & 1 \\\\\n\\hline\n\\end{array}\n\\]\n\n\n\n2.2.2 Construcción de clases\nCada intervalo de agrupación de datos se denomina clase y el centro del intervalo se llama marca de clase.\nA la hora de agrupar los datos en clases hay que tener en cuenta lo siguiente:\n\nEl número de intervalos no debe ser muy grande ni muy pequeño. Una regla orientativa es tomar un número de intervalos próximo a \\(\\sqrt{n}\\) o \\(\\log_2(n)\\).\nLos intervalos no deben solaparse y deben cubrir todo el rango de valores. Es indiferente si se abren por la izquierda y se cierran por la derecha o al revés.\nEl valor más pequeño debe caer dentro del primer intervalo y el más grande dentro del último.\n\n\nEjemplo 2.3 (Variable cualitativa) Los grupos sanguíneos de una muestra de 30 personas son:\n\nA, B, B, A, AB, 0, 0, A, B, B, A, A, A, A, AB, A, A, A, B, 0, B, B, B, A, A, A, 0, A, AB, 0.\n\nLa tabla de frecuencias del grupo sanguíneo en esta muestra es\n\\[\n\\begin{array}{crr}\n\\hline\nx_i & n_i & f_i \\\\\n\\hline\n\\mbox{0} & 5 & 0.16 \\\\\n\\mbox{A} & 14 & 0.47 \\\\\n\\mbox{B} & 8 & 0.27 \\\\\n\\mbox{AB} & 3 & 0.10 \\\\\n\\hline\n\\sum & 30 & 1 \\\\\n\\hline\n\\end{array}\n\\]\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nObsérvese que en este caso las frecuencias acumuladas no tienen sentido al no existir un orden entre los valores de la variable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estadística Descriptiva</span>"
    ]
  },
  {
    "objectID": "02-estadistica-descriptiva.html#representaciones-gráficas",
    "href": "02-estadistica-descriptiva.html#representaciones-gráficas",
    "title": "2  Estadística Descriptiva",
    "section": "2.3 Representaciones gráficas",
    "text": "2.3 Representaciones gráficas\nLa tabla de frecuencias también suele representarse gráficamente. Dependiendo del tipo de variable y de si se han agrupado o no los datos, se utilizan distintos tipos de gráficos:\n\nDiagrama de barras\nHistograma\nDiagrama de líneas o polígonos.\nDiagrama de sectores.\n\n\n2.3.1 Diagrama de barras\nUn diagrama de barras consiste en un conjunto de barras, una para cada valor o categoría de la variable, dibujadas sobre unos ejes cartesianos.\nHabitualmente los valores o categorías de la variable se representan en eje \\(X\\), y las frecuencias en el eje \\(Y\\). Para cada valor o categoría se dibuja una barra con la altura correspondiente a su frecuencia. La anchura de la barra no es importante pero las barras deben aparecer claramente separadas unas de otras.\nDependiendo del tipo de frecuencia representada en el eje \\(Y\\) se tienen diferentes tipos de diagramas de barras.\nEn ocasiones se dibuja un polígono, conocido como polígono de frecuencias, uniendo mediante segmentos los puntos más altos de cada barra.\n\nEjemplo 2.4 El diagrama de barras que aparece a continuación muestra la distribución de frecuencias absolutas del número de hijos en la muestra anterior.\n\ndf &lt;- read.csv(\"datos/hijos-coches.csv\")\np &lt;- ggplot(df, aes(x=Hijos)) + \n    geom_bar(fill=blueceu, width = 0.5) +\n    ylab(\"Frecuencia\")\np\n\n\n\n\n\n\n\n\nY a continuación se muestra el polígono de frecuencias.\n\np &lt;- p +  \n    geom_freqpoly(bins=5, col=redceu)\np\n\n\n\n\n\n\n\n\nEl diagrama de barras que aparece a continuación muestra la distribución de frecuencias relativas del número de hijos en la muestra anterior.\n\np &lt;- ggplot(df, aes(x=Hijos)) + \n    geom_bar(aes(y = ..prop..), fill=blueceu, width = 0.5) +\n    ylab(\"Frecuencia Relativa\")\np\n\nWarning: The dot-dot notation (`..prop..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(prop)` instead.\n\n\n\n\n\n\n\n\n\nEl diagrama de barras que aparece a continuación muestra la distribución de frecuencias absolutas acumuladas del número de hijos en la muestra anterior.\n\np &lt;- ggplot(df, aes(x=Hijos)) + \n    geom_bar(aes(y = cumsum(..count..)), fill=blueceu, width = 0.5) +\n    ylab(\"Frecuencia acumulada\")\np\n\n\n\n\n\n\n\n\nY el diagrama de barras que aparece a continuación muestra la distribución de frecuencias relativas acumuladas del número de hijos en la muestra anterior.\n\np &lt;- ggplot(df, aes(x=Hijos)) + \n    geom_bar(aes(y = cumsum(..prop..)), fill=blueceu, width = 0.5) +\n    ylab(\"Frecuencia relativa acumulada\")\np\n\n\n\n\n\n\n\n\nFinalmente, el último diagrama muestra el polígono de frecuencias relativas acumuladas.\n\ndf.freq &lt;- count(df, Hijos) %&gt;%\n    mutate(f = n /sum(n), N = cumsum(n), F = N / sum(n))\nx &lt;- unlist(lapply(df.freq$Hijos, rep, 2))\nF &lt;- c(0, head(unlist(lapply(df.freq$F, rep, 2)),-1))\ndf2 &lt;- data.frame(Hijos = x, F = F)\np &lt;- ggplot(df, aes(x=Hijos)) + \n    geom_bar(aes(y = cumsum(..prop..)), fill=blueceu, width = 0.5) +\n    geom_line(data = df2, aes(x=Hijos, y=F, group=1), col=redceu) +\n    ylab(\"Frecuencia relativa acumulada\")\np\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Histograma\nUn histograma es similar a un diagrama de barras pero para datos agrupados.\nHabitualmente las clases o intervalos de agrupación se representan en el eje \\(X\\), y las frecuencias en el eje \\(Y\\). Para cada clase se dibuja una barra de altura la correspondiente frecuencia. A diferencia del diagrama de barras, la anchura del la barra coincide con la anchura de las clases y no hay separación entre dos barras consecutivas.\n\n\n\nClasifiación en clases.\n\n\nDependiendo del tipo de frecuencia representada en el eje \\(Y\\) existen distintos tipos de histogramas.\nAl igual que con el diagrama de barras, se puede dibujar un polígono de frecuencias uniendo los puntos centrales más altos de cada barra con segmentos.\n\nEjemplo 2.5 El siguiente histograma muestra la distribución de frecuencias absolutas de las estaturas.\n\ndf &lt;- read.csv(\"datos/estatura-peso.csv\")\np &lt;- ggplot(df, aes(x=Estatura)) + \n    geom_histogram(breaks = seq(150, 200, 10), col=\"white\", fill=blueceu) +\n    ylab(\"Frecuencia\")\np\n\n\n\n\n\n\n\n\nEl siguiente histograma muestra la distribución de frecuencias relativas con el polígono de frecuencias.\n\nbreaks &lt;- seq(150, 200, 10)\np &lt;- ggplot(df, aes(x=Estatura)) + \n    geom_histogram(aes(y = ..density..), breaks = breaks, col=\"white\", fill=blueceu) +\n    geom_freqpoly(aes(y = ..density..), col=redceu, breaks = breaks) +\n    ylab(\"Frecuencia relativa\")\np\n\n\n\n\n\n\n\n\n\nEl polígono de frecuencias acumuladas (absolutas o relativas) se conoce como ojiva.\n\nEjemplo 2.6 El histograma y la ojiva siguientes muestran la distribución de frecuencias relativas acumuladas de estaturas.\n\nbreaks &lt;- seq(150, 200, 10)\np &lt;- ggplot(df, aes(x=Estatura)) + \n    geom_histogram(aes(y = cumsum(..count..)/sum(..count..)), breaks = breaks, col=\"white\", fill=blueceu) +\n    ylab(\"Frecuencia relativa acumulada\")\ndf.p &lt;- ggplot_build(p)$data[[1]]\nx &lt;- c(df.p$xmin[1], df.p$xmax)\ny &lt;- c(0, df.p$ymax)\ndf2 &lt;- data.frame(x, y)\np &lt;- p +\n    geom_line(data = df2, aes(x = x, y = y, group = 1), col=redceu)\np\n\n\n\n\n\n\n\n\n\nObsérvese que en la ojiva se unen los vértices superiores derechos de cada barra con segmentos, en lugar de los puntos centrales, ya que no se consigue alcanzar la frecuencia acumulada correspondiente a la clase hasta que no se alcanza el final del intervalo.\n\n\n2.3.3 Diagrama de sectores\nUn diagrama de sectores consiste en un círculo divido en porciones, uno por cada valor o categoría de la variable. Cada porción se conoce como sector y su ángulo o área es proporcional a la correspondiente frecuencia del valor o categoría.\nLos diagramas de sectores pueden representar frecuencias absolutas o relativas, pero no pueden representar frecuencias acumuladas, y se utilizan sobre todo con atributos nominales. Para atributos ordinales o variables cuantitativas es mejor utilizar diagramas de barras, ya es más fácil percibir las diferencias en una dimensión (altura de las barras) que en dos dimensiones (áreas de los sectores).\n\nEjemplo 2.7 El diagrama de sectores siguiente muestra la distribución de frecuencias relativas de los grupos sanguíneos.\n\ndf &lt;- read.csv(\"datos/grupo-sanguineo.csv\")\ntab &lt;- table(df[[\"Grupo.Sanguineo.Hijo\"]])\nlabels &lt;- names(tab)\npctg &lt;- round(tab/sum(tab)*100, 2)\nlabels &lt;- paste(labels, pctg) # add percents to labels \nlabels &lt;- paste(labels,\"%\",sep=\"\") # ad % to labels \npie(tab, main=\"Distribución de los grupos sanguineos\", labels=labels, col=c(greenceu, redceu, mygreen, blueceu))\n\n\n\n\n\n\n\n\n\n\n\n2.3.4 La distribución Normal\nLas distribuciones con diferentes propiedades presentan formas distintas.\n\nEjemplo 2.8 (Distribución de los ingresos familiares)  \n\nincome &lt;- seq(2500,207500,5000)/1000\ncounts &lt;- c(4235, 4071, 6324, 6470, 6765, 6222, 6354, 5743, 5203, 5002, 5078, 4140, 4367, 3733, 3683, 3650, 3354, 2893, 2850, 2452, 2792, 2232, 2158, 1748, 1987, 1675, 1474, 1380, 1220, 1111, 1313, 1017, 993, 761, 822, 700, 677, 536, 561, 431, 314, 215)\nbreaks &lt;- seq(0,210000,5000)/1000\ndf &lt;- data.frame(Ingresos = rep(income, counts))\np &lt;- ggplot(df, aes(x=Ingresos)) +\n    geom_histogram(aes(y = ..density..), breaks = breaks, col = \"white\", fill = blueceu) +\n    xlab(\"Ingresos anuales (miles de $)\") +\n    ylab(\"Frecuencia relativa\") +\n    ggtitle(\"Distribución de ingresos familiares en USA\")\np\n\n\n\n\n\n\n\n\n\n\nEjemplo 2.9 (Distribución de la edad de fallecimiento)  \n\ncounts &lt;- c(65, 116, 69, 78, 319, 501, 633, 655, 848, 1226, 1633, 2459, 3375, 4669, 6152, 7436, 9526, 12619, 12455, 7113, 2104)\nbreaks &lt;- seq(0,100,5)\ndf &lt;- data.frame(Edad = rep(breaks, counts))\np &lt;- ggplot(df, aes(x=Edad)) +\n    geom_histogram(aes(y = ..density..), breaks = breaks, col = \"white\", fill = blueceu) +\n    xlab(\"Edad de fallecimiento\") +\n    ylab(\"Frecuencia relativa\") +\n    ggtitle(\"Distribución de la edad de fallecimiento de hombres australianos.\")\np\n\n\n\n\n\n\n\n\n\n\nEjemplo 2.10 (Distribución del tiempo de espera del metro)  \n\nset.seed(123)\ntime &lt;- runif(1000, min = 0, max = 15)\nbreaks &lt;- seq(0, 15)\ndf &lt;- data.frame(Tiempo = time)\np &lt;- ggplot(df, aes(x=Tiempo)) +\n    geom_histogram(aes(y = ..density..), breaks = breaks, col = \"white\", fill = blueceu) +\n    xlab(\"Tiempo de espera (min)\") +\n    ylab(\"Frecuencia relativa\") +\n    ggtitle(\"Distribución del tiempo de espera del metro.\")\np\n\n\n\n\n\n\n\n\n\n\nEjemplo 2.11 (Distribución del tiempo de llegada de clientes a un restaurante)  \n\ncounts &lt;- c(35, 20, 18, 48, 75, 67, 43, 22, 14, 21, 23, 47, 63, 44, 25, 15)\nbreaks &lt;- seq(8.5,23.5,1)\ndf &lt;- data.frame(Tiempo = rep(breaks, counts))\nbreaks &lt;- seq(8,24)\np &lt;- ggplot(df, aes(x=Tiempo)) +\n    geom_histogram(aes(y = ..density..), breaks = breaks, col = \"white\", fill = blueceu) +\n    xlab(\"Tiempo\") +\n    ylab(\"Frecuencia relativa\") +\n    ggtitle(\"Distribución del tiempo de llegada de clientes a un restaurante\")\np\n\n\n\n\n\n\n\n\n\nLas distribuciones con forma de campana se presentan muy a menudo en las variables biológicas.\n\nEjemplo 2.12 (Distribución del peso de los hombres)  \n\nset.seed(123)\ndf &lt;- data.frame(Peso = rnorm(10000, mean = 88, sd = 12))\nbreaks &lt;- seq(40, 140, 2)\np &lt;- ggplot(df, aes(x = Peso)) +\n    geom_histogram(aes(y = ..density..), breaks = breaks, col = \"white\", fill = blueceu) +\n    xlab(\"Peso (kg)\") +\n    ylab(\"Frecuencia relativa\") +\n    ggtitle(\"Distribución del peso de los hombres\")\np\n\n\n\n\n\n\n\n\n\n\nEjemplo 2.13 (Distribución de la estatura de las mujeres)  \n\nset.seed(1234)\ndf &lt;- data.frame(Estatura = rnorm(10000, mean = 164, sd = 8))\nbreaks &lt;- seq(130, 200, 2)\np &lt;- ggplot(df, aes(x = Estatura)) +\n    geom_histogram(aes(y = ..density..), breaks = breaks, col = \"white\", fill = blueceu) +\n    xlab(\"Estatura (cm)\") +\n    ylab(\"Frecuencia relativa\") +\n    ggtitle(\"Distribución de la estatura de las mujeres\")\np\n\n\n\n\n\n\n\n\n\n\nEjemplo 2.14 (Distribución de la estatura según el sexo)  \n\nset.seed(1234)\nn &lt;- 10000\nmujeres &lt;- rnorm(n, mean = 164, sd = 8)\nhombres &lt;- rnorm(n, mean = 175, sd = 9)\ndf &lt;- data.frame(Estatura = c(mujeres, hombres), Sexo = c(rep(\"Mujer\",n), rep(\"Hombre\", n)))\nbreaks &lt;- seq(130, 210, 2)\np &lt;- ggplot(df, aes(x = Estatura, fill = Sexo)) +\n    geom_histogram(aes(y = ..density..), breaks = breaks, position = \"identity\", col = \"white\", alpha=0.5) +\n    xlab(\"Estatura (cm)\") +\n    ylab(\"Frecuencia relativa\") +\n    ggtitle(\"Distribución de estaturas según sexo\")\np\n\n\n\n\n\n\n\n\n\n\nEjemplo 2.15 (Distribución de la estatura de hombres y mujeres)  \n\np &lt;- ggplot(df, aes(x = Estatura)) +\n    geom_histogram(aes(y = ..density..), breaks = breaks, col = \"white\", fill = blueceu) +\n    xlab(\"Estatura (cm)\") +\n    ylab(\"Frecuencia relativa\") +\n    ggtitle(\"Distribución de estaturas de hombres y mujeres\")\np\n\n\n\n\n\n\n\n\n\n\nEjemplo 2.16 (Distribución del colesterol)  \n\nset.seed(123)\ndf &lt;- data.frame(Colesterol = rnorm(10000, mean = 192, sd = 18))\nbreaks &lt;- seq(120, 265, 5)\np &lt;- ggplot(df, aes(x = Colesterol)) +\n    geom_histogram(aes(y = ..density..), breaks = breaks, col = \"white\", fill = blueceu) +\n    xlab(\"Colesterol (mg/dl)\") +\n    ylab(\"Frecuencia relativa\") +\n    ggtitle(\"Distribución del colesterol\")\np\n\n\n\n\n\n\n\n\n\n\nEjemplo 2.17 (Distribución de notas)  \n\nset.seed(123)\ndf &lt;- data.frame(Nota = rnorm(1000, mean = 5.5, sd = 1.4))\nbreaks &lt;- seq(0, 10, 0.5)\np &lt;- ggplot(df, aes(x = Nota)) +\n    geom_histogram(aes(y = ..density..), breaks = breaks, col = \"white\", fill = blueceu) +\n    xlab(\"Nota\") +\n    ylab(\"Frecuencia relativa\") +\n    ggtitle(\"Distribución de notas de Estadística\")\np\n\n\n\n\n\n\n\n\n\nLa distribución con forma de campana aparece tan a menudo en la Naturaleza que se conoce como distribución normal o distribución gaussiana.\n\n\n\nCampana de Gauss.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estadística Descriptiva</span>"
    ]
  },
  {
    "objectID": "02-estadistica-descriptiva.html#datos-atípicos",
    "href": "02-estadistica-descriptiva.html#datos-atípicos",
    "title": "2  Estadística Descriptiva",
    "section": "2.4 Datos atípicos",
    "text": "2.4 Datos atípicos\nUno de los principales problemas de las muestras son los datos atípicos, que son valores de la variable que se diferencian mucho del resto de los valores en la muestra.\n\n\n\nDato atípico.\n\n\nEs muy importante detectar los datos atípicos antes de realizar cualquier análisis de los datos, pues suelen distorsionar los resultados.\nAparecen siempre en los extremos de la distribución, y pueden detectarse con un diagrama de caja y bigotes (tal y como veremos más adelante).\n\n2.4.1 Tratamiento de los datos atípicos\nCuando trabajemos con muestras grandes, los datos atípicos tienen menor influencia y pueden dejarse en la muestra.\nCuando trabajemos con muestras pequeñas tenemos varias opciones:\n\nEliminar el dato atípico si se trata de un error.\nSustituir el dato atípico por el menor o el mayor valor de la distribución que no es atípico si no se trata de un error y el dato atípico no concuerda con la distribución teórica.\nDejar el dato atípico si no es un error, y cambiar el modelo de distribución teórico para adecuarlo a los datos atípicos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estadística Descriptiva</span>"
    ]
  },
  {
    "objectID": "02-estadistica-descriptiva.html#estadísticos-muestrales",
    "href": "02-estadistica-descriptiva.html#estadísticos-muestrales",
    "title": "2  Estadística Descriptiva",
    "section": "2.5 Estadísticos muestrales",
    "text": "2.5 Estadísticos muestrales\nLa tabla de frecuencias sintetiza la información de la distribución de valores de la variable estudiada en la muestra, pero en muchas ocasiones es insuficiente para describir determinados aspectos de la distribución, como por ejemplo, cuáles son los valores más representativos de la muestra, cómo es la variabilidad de los datos, qué datos pueden considerarse atípicos, o cómo es la simetría de la distribución.\nPara describir esos aspectos de la distribución muestral se utilizan unas medidas resumen llamadas estadísticos muestrales.\nDe acuerdo al aspecto de las distribución que miden, existen diferentes tipos de estadísticos:\nEstadísticos de Posición: Miden los valores en torno a los que se agrupan los datos o que dividen la distribución en partes iguales.\nEstadísticos de Dispersión: Miden la heterogeneidad de los datos.\nEstadísticos de Forma: Miden aspectos de la forma que tiene la distribución de los datos, como la simetría o el apuntamiento.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estadística Descriptiva</span>"
    ]
  },
  {
    "objectID": "02-estadistica-descriptiva.html#estadísticos-de-posición",
    "href": "02-estadistica-descriptiva.html#estadísticos-de-posición",
    "title": "2  Estadística Descriptiva",
    "section": "2.6 Estadísticos de posición",
    "text": "2.6 Estadísticos de posición\nPueden ser de dos tipos:\nEstadísticos de Tendencia Central: Determinan valores alrededor de los cuales se concentran los datos, habitualmente en el centro de la distribución. Estas medidas suelen utilizarse como valores representativos de la muestra. Las más importantes son:\n\nMedia aritmética\nMediana\nModa\n\nEstadísticos de Posición no centrales: Dividen la distribución en partes con el mismo número de datos. Las más importantes son:\n\nCuartiles.\nDeciles.\nPercentiles.\n\n\n2.6.1 Media aritmética\n\nDefinición 2.2 (Media aritmética muestral \\(\\bar{x}\\)) La media aritmética muestral de una variable \\(X\\) es la suma de los valores observados en la muestra dividida por el tamaño muestral\n\\[\\bar{x} = \\frac{\\sum x_i}{n}\\]\n\nA partir de la tabla de frecuencias puede calcularse con la fórmula\n\\[\\bar{x} = \\frac{\\sum x_in_i}{n} = \\sum x_i f_i\\]\nEn la mayoría de los casos, la media aritmética es la medida que mejor representa a la muestra.\n\n\n\n\n\n\nAdvertencia\n\n\n\nNo puede calcularse para variables cualitativas.\n\n\n\nEjemplo 2.18 (Datos no agrupados) Utilizando los datos de la muestra del número de hijos en las familias, la media aritmética es\n\\[\\begin{align*}\n\\bar{x} &= \\frac{1+2+4+2+2+2+3+2+1+1+0+2+2}{25}+\\\\\n&+\\frac{0+2+2+1+2+2+3+1+2+2+1+2}{25} = \\frac{44}{25} = 1.76 \\mbox{ hijos}.\n\\end{align*}\\]\no bien, desde la tabla de frecuencias\n\\[\n\\begin{array}{rrrrr}\n\\hline\nx_i & n_i & f_i & x_in_i & x_if_i\\\\\n\\hline\n0 & 2 & 0.08 & 0 & 0\\\\\n1 & 6 & 0.24 & 6 & 0.24\\\\\n2 & 14 & 0.56 & 28 & 1.12\\\\\n3 & 2  & 0.08 & 6 & 0.24\\\\\n4 & 1 & 0.04 & 4 & 0.16 \\\\\n\\hline\n\\sum & 25 & 1 & 44 & 1.76 \\\\\n\\hline\n\\end{array}\n\\]\n\\[\n\\bar{x} = \\frac{\\sum x_in_i}{n} = \\frac{44}{25}= 1.76 \\mbox{ hijos}\\qquad \\bar{x}=\\sum{x_if_i} = 1.76 \\mbox{ hijos}.\n\\]\nEsto significa que el valor que mejor representa el número de hijos en las familias de la muestra es 1.76 hijos.\n\n\nEjemplo 2.19 (Datos agrupados) Utilizando los datos de la muestra de estaturas, la media es\n\\[\n\\bar{x} = \\frac{179+173+\\cdots+187}{30} = 175.07 \\mbox{ cm}.\n\\]\no bien, desde la tabla de frecuencias utilizando las marcas de clase \\(x_i\\):\n\\[\n\\begin{array}{crrrrr}\n\\hline\nX & x_i & n_i & f_i & x_in_i & x_if_i\\\\\n\\hline\n(150,160] & 155 & 2 & 0.07 & 310 & 10.33\\\\\n(160,170] & 165 & 8 & 0.27 & 1320 & 44.00\\\\\n(170,180] & 175 & 11 & 0.36 & 1925 & 64.17\\\\\n(180,190] & 185 & 7 & 0.23 & 1295 & 43.17\\\\\n(190,200] & 195 & 2 & 0.07 & 390 & 13 \\\\\n\\hline\n\\sum &  & 30 & 1 & 5240 & 174.67 \\\\\n\\hline\n\\end{array}\n\\]\n\\[\n\\bar{x} = \\frac{\\sum x_in_i}{n} = \\frac{5240}{30}= 174.67 \\mbox{ cm} \\qquad \\bar{x}=\\sum{x_if_i} = 174.67 \\mbox{ cm}.\n\\]\nObsérvese que al calcular la media desde la tabla de frecuencias el resultado difiere ligeramente del valor real obtenido directamente desde la muestra, ya que los valores usados en los cálculos no son los datos reales sino las marcas de clase.\n\n\n2.6.1.1 Media ponderada\nEn algunos casos, los valores de la muestra no tienen la misma importancia. En este caso la importancia o peso de cada valor de la muestra debe tenerse en cuenta al calcular la media.\n\nDefinición 2.3 (Media ponderada muestral \\(\\bar x_p\\)) Dada una muestra de valores \\(x_1,\\ldots, x_n\\) donde cada valor \\(x_i\\) tiene asociado un peso \\(p_i\\), la media ponderada muestral de la variable \\(X\\) es la suma de los productos de cada valor observado en la muestra por su peso, dividida por la suma de todos los pesos\n\\[\\bar{x}_p = \\frac{\\sum x_ip_i}{\\sum p_i}\\]\n\nA partir de la tabla de frecuencias puede calcularse con la fórmula\n\\[\\bar{x}_p = \\frac{\\sum x_ip_in_i}{\\sum p_i}\\]\n\nEjemplo 2.20 Supóngase que un estudiante quiere calcular una medida que represente su rendimiento en el curso. La nota obtenida en cada asignatura y sus créditos son\n\n\n\nAsignatura\nCréditos\nNota\n\n\n\n\nMatemáticas\n6\n5\n\n\nEconomía\n4\n3\n\n\nQuímica\n8\n6\n\n\n\nLa media aritmética vale\n\\[\\bar{x} = \\frac{\\sum x_i}{n} = \\frac{5+3+6}{3}= 4.67 \\text{ puntos}.\\]\nSin embargo, esta nota no representa bien el rendimiento académico del alumno ya que no todas las asignaturas tienen la misma importancia ni requieren el mismo esfuerzo para aprobar. Las asignaturas con más créditos requieren más trabajo y deben tener más peso en el cálculo de la media.\nEs más lógico usar la media ponderada como medida del rendimiento del estudiante, tomando como pesos los créditos de cada asignatura\n\\[\n\\bar{x}_p = \\frac{\\sum x_ip_i}{\\sum p_i} = \\frac{5\\cdot 6+3\\cdot 4+6\\cdot 8}{6+4+8}= \\frac{90}{18} = 5 \\text{ puntos}.\n\\]\n\n\n\n\n2.6.2 Mediana\n\nDefinición 2.4 (Mediana muestral \\(Me\\)) La mediana muestral de una variable \\(X\\) es el valor de la variable que está en el medio de la muestra ordenada.\n\nLa mediana divide la distribución de la muestra en dos partes iguales, es decir, hay el mismo número de valores por debajo y por encima de la mediana. Por tanto, tiene frecuencias acumuladas \\(N_{Me}= n/2\\) y \\(F_{Me}= 0.5\\).\n\n\n\n\n\n\nAdvertencia\n\n\n\nNo puede calcularse para variables nominales.\n\n\nCon datos no agrupados pueden darse varios casos:\n\nTamaño muestral impar: La mediana es el valor que ocupa la posición \\(\\frac{n+1}{2}\\).\nTamaño muestral par: La mediana es la media de los valores que ocupan las posiciones \\(\\frac{n}{2}\\) y \\(\\frac{n}{2}+1\\).\n\n\n\n\nCálculo de la mediana con datos no agrupados.\n\n\n:::{#exm-mediana-datos-no-agrupados} Utilizando los datos del número de hijos de las familias, el tamaño muestral es 25, que es impar, y la mediana es el valor que ocupa la posición \\(\\frac{25+1}{2} = 13\\) de la muestra ordenada.\n\\[\n0,0,1,1,1,1,1,1,2,2,2,2,\\fbox{2},2,2,2,2,2,2,2,2,2,3,3,4\n\\]\ny la mediana es 2 hijos.\nSi se trabaja con la tabla de frecuencias, la mediana es el valor más pequeño con una frecuencia acumulada mayor o igual a \\(13\\), o con una frecuencia relativa acumulada mayor o igual que \\(0.5\\).\n\\[\n\\begin{array}{rrrrr}\n\\hline\nx_i & n_i & f_i & N_i & F_i\\\\\n\\hline\n0 & 2 & 0.08 & 2 & 0.08\\\\\n1 & 6 & 0.24 & 8 & 0.32\\\\\n\\color{red}2 & 14 & 0.56 & 22 & 0.88\\\\\n3 & 2  & 0.08 & 24 & 0.96\\\\\n4 & 1 & 0.04 & 25 & 1 \\\\\n\\hline\n\\sum & 25 & 1 \\\\\n\\hline\n\\end{array}\n\\]\n\n2.6.2.1 Cálculo de la mediana con datos agrupados\nCon datos agrupados la mediana se calcula interpolando en el polígono de frecuencias relativas acumuladas para el valor 0.5.\n\n\n\nCálculo de la mediana con datos agrupados.\n\n\nAmbas expresiones son iguales ya que el ángulo \\(\\alpha\\) es el mismo, y resolviendo la ecuación se tiene la siguiente fórmula para calcular la mediana\n\\[\nMe=l_{i-1}+\\frac{0.5-F_{i-1}}{F_i-F_{i-1}}(l_i-l_{i-1})=l_{i-1}+\\frac{0.5-F_{i-1}}{f_i}a_i\n\\]\n\nEjemplo 2.21 (Datos agrupados) Utilizando los datos de la muestra de las estaturas de estudiantes, la mediana cae en la clase (170,180].\n\n\n\nEjemplo de cálculo de la mediana con datos agrupados.\n\n\nInterpolando en el intervalo (170,180] se tiene\n\n\n\nEjemplo de cálculo de la mediana con datos agrupados.\n\n\nIgualando ambas expresiones y resolviendo la ecuación se obtiene\n\\[\nMe= 170+\\frac{0.5-0.34}{0.7-0.34}(180-170)=170+\\frac{0.16}{0.36}10=174.54 \\mbox{ cm}.\n\\]\nEsto significa que la mitad de los estudiantes tienen estaturas menores o iguales que 174.54 cm y la otra mitad mayores o iguales.\n\n\n\n\n2.6.3 Moda\n\nDefinición 2.5 (Moda muestral \\(Mo\\)) La moda muestral de una variable \\(X\\) es el valor de la variable más frecuente en la muestra.\n\nCon datos agrupados la clase modal es la clase con mayor frecuencia en la muestra.\nPuede calcularse para todos los tipos de variables (cuantitativas y cualitativas).\nLas distribuciones pueden tener más de una moda.\n\n\n\nCálculo de la moda.\n\n\n\nEjemplo 2.22 Utilizando los datos de la muestra del número de hijos en las familias, el valor con mayor frecuencia es 2, y por tanto la moda es \\(Mo=2\\).\n\\[\n\\begin{array}{rr}\n\\hline\nx_i & n_i \\\\\n\\hline\n0 & 2 \\\\\n1 & 6 \\\\\n\\color{red} 2 & 14 \\\\\n3 & 2  \\\\\n4 & 1 \\\\\n\\hline\n\\end{array}\n\\]\n\n\nEjemplo 2.23 Utilizando los datos de la muestra de estaturas de estudiantes, la clase con la mayor frecuencia es \\((170,180]\\), que es la clase modal \\(Mo=(170,180]\\).\n\\[\n\\begin{array}{cr}\n\\hline\nX & n_i \\\\\n\\hline\n(150,160] & 2 \\\\\n(160,170] & 8 \\\\\n\\color{red}{(170,180]} & 11 \\\\\n(180,190] & 7 \\\\\n(190,200] & 2 \\\\\n\\hline\n\\end{array}\n\\]\n\n\n\n2.6.4 ¿Qué estadístico de tendencia central usar?\nEn general, siempre que puedan calcularse los estadísticos de tendencia central, es recomendable utilizarlos como valores representativos en el siguiente orden:\n\nMedia. La media utiliza más información que el resto ya que para calcularla se tiene en cuenta la magnitud de los datos.\nMediana. La mediana utiliza menos información que la media, pero más que la moda, ya que para calcularla se tiene en cuenta el orden de los datos.\nModa. La moda es la que menos información utiliza ya que para calcularla sólo se tienen en cuenta las frecuencias absolutas.\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nHay que tener cuidado con los datos atípicos, ya que la media puede distorsionarse cuando hay datos atípicos. En tal caso es mejor utilizar la mediana como valor más representativo.\n\n\n\nEjemplo 2.24 Si una muestra de número de hijos de 7 familias es\n\n0, 0, 1, 1, 2, 2, 15,\n\nentonces, \\(\\bar{x}=3\\) hijos y \\(Me=1\\) hijo.\n¿Qué medida representa mejor el número de hijos en la muestra?\n\n\n\n2.6.5 Medidas de posición no centrales\nLas medidas de posición no centrales o cuantiles dividen la distribución en partes iguales.\nLos más utilizados son:\nCuartiles: Dividen la distribución en 4 partes iguales. Hay 3 cuartiles: \\(C_1\\) (25% acumulado), \\(C_2\\) (50% acumulado), \\(C_3\\) (75% acumulado).\nDeciles: Dividen la distribución en 10 partes iguales. Hay 9 deciles: \\(D_1\\) (10% acumulado),…, \\(D_9\\) (90% acumulado).\nPercentiles: Dividen la distribución en 100 partes iguales. Hay 99 percentiles: \\(P_1\\) (1% acumulado),…, \\(P_{99}\\) (99% acumulado).\n\n\n\nCuartiles, deciles y percentiles.\n\n\nObsérvese que hay una correspondencia entre los cuartiles, los deciles y los percentiles. Por ejemplo, el primer cuartil coincide con el percentil 25, y el cuarto decil coincide con el percentil 40.\nLos cuantiles se calculan de forma similar a la mediana. La única diferencia es la frecuencia relativa acumulada que corresponde a cada cuantil.\n\n\n\nCálculo de cuartiles, deciles y percentiles.\n\n\n\nEjemplo 2.25 Utilizando los datos de la muestra del número de hijos de las familias, la frecuencia relativa acumulada era\n\\[\n\\begin{array}{rr}\n\\hline\nx_i & F_i \\\\\n\\hline\n0 & 0.08\\\\\n1 & 0.32\\\\\n2 & 0.88\\\\\n3 & 0.96\\\\\n4 & 1\\\\\n\\hline\n\\end{array}\n\\]\n\\[\\begin{align*}\nF_{C_1}=0.25 &\\Rightarrow Q_1 = 1 \\text{ hijos},\\\\\nF_{C_2}=0.5 &\\Rightarrow Q_2 = 2 \\text{ hijos},\\\\\nF_{C_3}=0.75 &\\Rightarrow Q_3 = 2 \\text{ hijos},\\\\\nF_{D_4}=0.4 &\\Rightarrow D_4 = 2 \\text{ hijos},\\\\\nF_{P_{92}}=0.92 &\\Rightarrow P_{92} = 3 \\text{ hijos}.\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estadística Descriptiva</span>"
    ]
  },
  {
    "objectID": "02-estadistica-descriptiva.html#estadísticos-de-dispersión",
    "href": "02-estadistica-descriptiva.html#estadísticos-de-dispersión",
    "title": "2  Estadística Descriptiva",
    "section": "2.7 Estadísticos de dispersión",
    "text": "2.7 Estadísticos de dispersión\nLa dispersión se refiere a la heterogeneidad o variabilidad de los datos. Así pues, los estadísticos de dispersión mide la variabilidad global de los datos, o con respecto a una medida de tendencia central.\nPara las variables cuantitativas, las más empleadas son:\n\nRecorrido.\nRango Intercuartílico.\nVarianza.\nDesviación Típica.\nCoeficiente de Variación.\n\n\n2.7.1 Recorrido\n\nDefinición 2.6 (Recorrido muestral \\(Re\\)) El recorrido muestral o rango muestral de una variable \\(X\\) se define como la diferencia entre el máximo y el mínimo de los valores en la muestra.\n\\[Re = \\max_{x_i} -\\min_{x_i}\\]\n\n\n\n\nRango muestral.\n\n\nEl recorrido mide la máxima variación que hay entre los datos muestrales. No obstante, es muy sensible a datos atípicos ya que suelen aparecer justo en los extremos de la distribución, por lo que no se suele utilizar mucho.\n\n\n2.7.2 Rango intercuartílico\nPara evitar el problema de los datos atípicos en el recorrido, se puede utilizar el primer y tercer cuartil en lugar del mínimo y el máximo.\n\nDefinición 2.7 (Rango intercuartílico muestral \\(RI\\)) El rango intercuartílico muestral de una variable \\(X\\) se define como la diferencia entre el tercer y el primer cuartil de la muestra.\n\\[RI = C_3 -C_1\\]\n\n\n\n\nRango intercuartílico.\n\n\nEl rango intercuartílico mide la dispersión del 50% de los datos centrales.\n\n\n2.7.3 Diagrama de caja y bigotes\nLa dispersión de una variable suele representarse gráficamente mediante un diagrama de caja y bigotes, que representa cinco estadísticos descriptivos (mínimo, cuartiles y máximo) conocidos como los cinco números. Consiste en una caja, dibujada desde el primer al tercer cuartil, que representa el rango intercuartílico, y dos segmentos, conocidos como bigotes inferior y superior. A menudo la caja se divide en dos por la mediana.\nEste diagrama es muy útil y se utiliza para muchos propósitos:\n\nSirve para medir la dispersión de los datos ya que representa el rango y el rango intercuartílico.\nSirve para detectar datos atípicos, que son los valores que quedan fuera del intervalo definido por los bigotes.\nSirve para medir la simetría de la distribución, comparando la longitud de las cajas y de los bigotes por encima y por debajo de la mediana.\n\n:::{#exm-diagrama-caja} El diagrama siguiente muestra el diagrama de caja y bigotes del peso de una muestra de recién nacidos.\n\n\n\nDiagrama de caja y bigotes del peso de recién nacidos.\n\n\nPara construir el diagrama de caja y bigotes hay que seguir los siguientes pasos:\n\nCalcular los cuartiles.\nDibujar una caja de manera que el extremo inferior caiga sobre el primer cuartil y el extremo superior sobre el tercer cuartil.\nDividir la caja con una línea que caiga sobre el segundo cuartil.\nPara los bigotes inicialmente se calculan dos valores llamados vallas \\(v_1\\) y \\(v_2\\). La valla inferior es el primer cuartil menos una vez y media el rango intercuartílico, y la valla superior es el tercer cuartil más una vez y media el rango intercuartílico.\n\\[\n\\begin{aligned}\nv_1&=Q_1-1.5\\,\\text{IQR}\\\\\nv_2&=Q_3+1.5\\,\\text{IQR}\n\\end{aligned}\n\\]\nLas vallas definen el intervalo donde los datos se consideran normales. Cualquier valor fuera de ese intervalo se considera un dato atípico.\nEl bigote superior se dibuja desde el borde inferior de la caja hasta el menor valor de la muestra que es mayor o igual a la valla inferior, y el bigote superior se dibuja desde el borde superior de la caja hasta el mayor valor de la muestra que es menor o igual a la valla superior.\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nLos bigotes no son las vallas.\n\n\n\nFinalmente, si en la muestra hay algún dato atípico, se dibuja un punto para cada uno de ellos.\n\n\nEjemplo 2.26 El diagrama de caja y bigotes de la muestra del número de hijos de las familias se muestra a continuación.\n\n\n\nDiagrama de caja y bigotes del número de hijos.\n\n\n\n\n2.7.3.1 Desviaciones respecto de la media\nOtra forma de medir la variabilidad de una variable es estudiar la concentración de los valores en torno a algún estadístico de tendencia central como por ejemplo la media.\nPara ello se suele medir la distancia de cada valor a la media. A ese valor se le llama desviación de la media.\n\n\n\nDesviaciones con respecto a la media.\n\n\nSi las desviaciones son grandes la media no será tan representativa como cuando la desviaciones sean pequeñas.\n\nEjemplo 2.27 La siguiente tabla contiene las notas de 3 estudiantes en un curso con las asignaturas \\(A\\), \\(B\\) y \\(C\\).\n\\[\n\\begin{array}{cccc}\n\\hline\nA & B & C & \\bar x \\\\\n0 & 5 & 10 & 5 \\\\\n4 & 5 & 6 & 5 \\\\\n5 & 5 & 5 & 5 \\\\\n\\hline\n\\end{array}\n\\]\nTodos los estudiantes tienen la misma media, pero, en qué caso la media representa mejor el rendimiento en el curso?\n\n\n\n\n2.7.4 Varianza y desviación típica\n\nDefinición 2.8 (Varianza \\(s^2\\)) La varianza muestral de una variable \\(X\\) se define como el promedio del cuadrado de las desviaciones de los valores de la muestra respecto de la media muestral.\n\\[s^2 = \\frac{\\sum (x_i-\\bar x)^2n_i}{n} = \\sum (x_i-\\bar x)^2f_i\\]\n\nTambién puede calcularse de manera más sencilla mediante la fórmula\n\\[s^2 = \\frac{\\sum x_i^2n_i}{n} -\\bar x^2= \\sum x_i^2f_i-\\bar x^2\\]\nLa varianza tiene las unidades de la variable al cuadrado, por lo que para facilitar su interpretación se suele utilizar su raíz cuadrada.\n\nDefinición 2.9 (Desviación típica \\(s\\)) La desviación típica muestral de una variable \\(X\\) se define como la raíz cuadrada positiva de su varianza muestral.\n\\[s = +\\sqrt{s^2}\\]\n\n\n\n\n\n\n\nTip\n\n\n\nTanto la varianza como la desviación típica sirven para cuantificar la dispersión de los datos en torno a la media. Cuando la varianza o la desviación típica son pequeñas, los datos de la muestra están concentrados en torno a la media, y la media es una buena medida de representatividad. Por contra, cuando la varianza o la desviación típica son grandes, los datos de la muestra están alejados de la media, y la media ya no representa tan bien.\n\n\n\nDesviación típica pequeña\n\\(\\Rightarrow\\)\nMedia representativa\n\n\nDesviación típica grande\n\\(\\Rightarrow\\)\nMedia no representativa\n\n\n\n\n\n\nEjemplo 2.28 Las siguientes muestras contienen las notas de dos estudiantes en dos asignaturas.\n\n\n\nInterpretación de la desviación típica.\n\n\n¿Qué media es más representativa?\n\n\nEjemplo 2.29 (Datos no agrupados) Utilizando los datos de la muestra del número de hijos de las familias, con una media \\(\\bar x=1.76\\) hijos, y añadiendo una nueva columna a la tabla de frecuencias con los cuadrados de los valores,\n\\[\n\\begin{array}{rrr}\n\\hline\nx_i & n_i & x_i^2n_i \\\\\n\\hline\n0 & 2 & 0 \\\\\n1 & 6 & 6 \\\\\n2 & 14 & 56\\\\\n3 & 2  & 18\\\\\n4 & 1 & 16 \\\\\n\\hline\n\\sum & 25 & 96 \\\\\n\\hline\n\\end{array}\\]\n\\[s^2 = \\frac{\\sum x_i^2n_i}{n}-\\bar x^2 = \\frac{96}{25}-1.76^2= 0.7424 \\mbox{ hijos}^2.\\]\ny la desviación típica es \\(s=\\sqrt{0.7424} = 0.8616\\) hijos.\nComparado este valor con el recorrido, que va de 0 a 4 hijos se observa que no es demasiado grande por lo que se puede concluir que no hay mucha dispersión y en consecuencia la media de \\(1.76\\) hijos representa bien el número de hijos de las familias de la muestra.\n\n\nEjemplo 2.30 (Datos agrupados) Utilizando los datos de la muestra de estaturas de los estudiantes y agrupando las estaturas en clases, se obtenía una media \\(\\bar x = 174.67\\) cm. El cálculo de la varianza se realiza igual que antes pero tomando como valores de la variable las marcas de clase.\n\\[\n\\begin{array}{crrr}\n\\hline\nX & x_i & n_i & x_i^2n_i \\\\\n\\hline\n(150,160] & 155 & 2 & 48050\\\\\n(160,170] & 165 & 8 & 217800\\\\\n(170,180] & 175 & 11 & 336875\\\\\n(180,190] & 185 & 7 & 239575\\\\\n(190,200] & 195 & 2 & 76050\\\\\n\\hline\n\\sum &  & 30 & 918350 \\\\\n\\hline\n\\end{array}\n\\]\n\\[s^2 = \\frac{\\sum x_i^2n_i}{n}-\\bar x^2 = \\frac{918350}{30}-174.67^2= 102.06 \\mbox{ cm}^2,\\]\ny la desviación típica es \\(s=\\sqrt{102.06} = 10.1\\) cm.\nEste valor es bastante pequeño, comparado con el recorrido de la variable, que va de 150 a 200 cm, por lo que la variable tiene poca dispersión y en consecuencia su media es muy representativa.\n\n\n\n2.7.5 Coeficiente de variación\nTanto la varianza como la desviación típica tienen unidades y eso dificulta a veces su interpretación, especialmente cuando se compara la dispersión de variables con diferentes unidades.\nPor este motivo, es también común utilizar la siguiente medida de dispersión que no tiene unidades.\n\nDefinición 2.10 (Coeficiente de variación muestral \\(cv\\)) El coeficiente de variación muestral de una variable \\(X\\) se define como el cociente entre su desviación típica muestral y el valor absoluto de su media muestral.\n\\[cv = \\frac{s}{|\\bar x|}\\]\n\n\n\n\n\n\n\nTip\n\n\n\nEl coeficiente de variación muestral mide la dispersión relativa de los valores de la muestra en torno a la media muestral.\nComo no tiene unidades, es muy sencillo de interpretar: Cuanto mayor sea, mayor será la dispersión relativa con respecto a la media y menos representativa será la media.\n\n\nEl coeficiente de variación es muy útil para comparar la dispersión de distribuciones de variables diferentes, incluso si las variables tienen unidades diferentes.\n\nEjemplo 2.31 En la muestra del número de hijos, donde la media era \\(\\bar x=1.76\\) hijos y la desviación típica \\(s=0.8616\\) hijos, el coeficiente de variación vale\n\\[cv = \\frac{s}{|\\bar x|} = \\frac{0.8616}{|1.76|} = 0.49.\\]\nEn la muestra de las estaturas, donde la media era \\(\\bar x=174.67\\) cm y la desviación típica \\(s=10.1\\) cm, el coeficiente de variación vale\n\\[cv = \\frac{s}{|\\bar x|} = \\frac{10.1}{|174.67|} = 0.06.\\]\nEsto significa que la dispersión relativa en la muestra de estaturas es mucho menor que en la del número de hijos, por lo que la media de las estaturas será más representativa que la media del número de hijos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estadística Descriptiva</span>"
    ]
  },
  {
    "objectID": "02-estadistica-descriptiva.html#estadísticos-de-forma",
    "href": "02-estadistica-descriptiva.html#estadísticos-de-forma",
    "title": "2  Estadística Descriptiva",
    "section": "2.8 Estadísticos de forma",
    "text": "2.8 Estadísticos de forma\nSon medidas que describen la forma de la distribución.\nLos aspectos más relevantes son:\nSimetría Mide la simetría de la distribución de frecuencias en torno a la media. El estadístico más utilizado es el Coeficiente de Asimetría de Fisher.\nApuntamiento Mide el apuntamiento o el grado de concentración de valores en torno a la media de la distribución de frecuencias. El estadístico más utilizado es el Coeficiente de Apuntamiento o Curtosis.\n\n2.8.1 Coeficiente de asimetría\n\nDefinición 2.11 (Coeficiente de asimetría muestral \\(g_1\\)) El coeficiente de asimetría muestral de una variable \\(X\\) es el promedio de las desviaciones de los valores de la muestra respecto de la media muestral, elevadas al cubo, dividido por la desviación típica al cubo.\n\\[g_1 = \\frac{\\sum (x_i-\\bar x)^3 n_i/n}{s^3} = \\frac{\\sum (x_i-\\bar x)^3 f_i}{s^3}\\]\n\n\n\n\n\n\n\nTip\n\n\n\nMide el grado de simetría de los valores de la muestra con respecto a la media muestra, es decir, cuantos valores de la muestra están por encima o por debajo de la media y cómo de alejados de esta.\n\n\\(g_1=0\\) indica que hay el mismo número de valores por encima y por debajo de la media e igualmente alejados de ella (simétrica).\n\n\n\n\nDistribución simétrica.\n\n\n\n\\(g_1&lt;0\\) indica que la mayoría de los valores son mayores que la media, pero los valores menores están más alejados de ella (asimétrica a la izquierda).\n\n\n\n\nDistribución asimétrica hacia la izquierda.\n\n\n\n\\(g_1&gt;0\\) indica que la mayoría de los valores son menores que la media, pero los valores mayores están más alejados de ella (asimétrica a la derecha).\n\n\n\n\nDistribución asimétrica hacia la derecha.\n\n\n\n\n\nEjemplo 2.32 (Datos agrupados) Utilizando la tabla de frecuencias de la muestra de estaturas y añadiendo una nueva columna con las desviaciones de la media \\(\\bar x = 174.67\\) cm al cubo, se tiene\n\\[\n\\begin{array}{crrrr}\n\\hline\nX & x_i & n_i & x_i-\\bar x & (x_i-\\bar x)^3 n_i \\\\\n\\hline\n(150,160] & 155 & 2 & -19.67 & -15221.00\\\\\n(160,170] & 165 & 8 & -9.67 & -7233.85\\\\\n(170,180] & 175 & 11 & 0.33 & 0.40\\\\\n(180,190] & 185 & 7 & 10.33 & 7716.12\\\\\n(190,200] & 195 & 2 & 20.33 & 16805.14\\\\\n\\hline\n\\sum &  & 30 & & 2066.81 \\\\\n\\hline\n\\end{array}\n\\]\n\\[g_1 = \\frac{\\sum (x_i-\\bar x)^3n_i/n}{s^3} = \\frac{2066.81/30}{10.1^3} = 0.07.\\]\nComo está cerca de 0, eso significa que la distribución de las estaturas es casi simétrica.\n\n\n\n2.8.2 Coeficiente de apuntamiento o curtosis\n\nDefinición 2.12 (Coeficiente de apuntamiento muestral \\(g_2\\)) El coeficiente de apuntamiento muestral de una variable \\(X\\) es el promedio de las desviaciones de los valores de la muestra respecto de la media muestral, elevadas a la cuarta, dividido por la desviación típica a la cuarta y al resultado se le resta 3.\n\\[g_2 = \\frac{\\sum (x_i-\\bar x)^4 n_i/n}{s^4}-3 = \\frac{\\sum (x_i-\\bar x)^4 f_i}{s^4}-3\\]\n\n\n\n\n\n\n\nTip\n\n\n\nEl coeficiente de apuntamiento mide la concentración de valores en torno a la media y la longitud de las colas de la distribución. Se toma como referencia la distribución normal (campana de Gauss).\n\n\\(g_2=0\\) indica que la distribución tienen un apuntamiento normal, es decir, la concentración de valores en torno a la media es similar al de una campana de Gauss (mesocúrtica).\n\n\n\n\nDistribución mesocúrtica.\n\n\n\n\\(g_2&lt;0\\) indica que la distribución tiene menos apuntamiento de lo normal, es decir, la concentración de valores en torno a la media es menor que en una campana de Gauss (platicúrtica).\n\n\n\n\nDistribución platicúrtica.\n\n\n\n\\(g_2&gt;0\\) indica que la distribución tiene más apuntamiento de lo normal, es decir, la concentración de valores en torno a la media es menor que en una campana de Gauss (leptocúrtica).\n\n\n\n\nDistribución leptocúrtica.\n\n\n\n\n:::{#exm-coeficiente-apuntamiento} ## Datos agrupados Utilizando la tabla de frecuencias de la muestra de estaturas y añadiendo una nueva columna con las desviaciones de la media \\(\\bar x = 174.67\\) cm a la cuarta, se tiene\n\\[\n\\begin{array}{rrrrr}\n\\hline\nX & x_i & n_i & x_i-\\bar x & (x_i-\\bar x)^4 n_i\\\\\n\\hline\n(150,160] & 155 & 2 & -19.67 & 299396.99\\\\\n(160,170] & 165 & 8 & -9.67 & 69951.31\\\\\n(170,180] & 175 & 11 & 0.33 & 0.13\\\\\n(180,190] & 185 & 7 & 10.33 & 79707.53\\\\\n(190,200] & 195 & 2 & 20.33 & 341648.49\\\\\n\\hline\n\\sum &  & 30 & & 790704.45 \\\\\n\\hline\n\\end{array}\n\\]\n\\[g_2 = \\frac{\\sum (x_i-\\bar x)^4n_i/n}{s^4} - 3 = \\frac{790704.45/30}{10.1^4}-3 = -0.47.\\]\nComo se trata de un valor negativo, aunque cercano a 0, podemos decir que la distribución es ligeramente platicúrtica.\nComo se verá más adelante en la parte de inferencia, muchas de las pruebas estadísticas solo pueden aplicarse a poblaciones normales.\nLas poblaciones normales se caracterizan por ser simétricas y mesocúrticas, de manera que, tanto el coeficiente de asimetría como el de apuntamiento pueden utilizarse para contrastar si los datos de la muestra provienen de una población normal.\n\n\n\n\n\n\nTip\n\n\n\nEn general, se suele rechazar la hipótesis de normalidad de la población cuando \\(g_1\\) o \\(g_2\\) estén fuera del intervalo \\([-2,2]\\).\n\n\nEn tal caso, lo habitual es aplicar alguna transformación a la variable para corregir la anormalidad.\n\n\n2.8.3 Distribuciones no normales\n\n2.8.3.1 Distribución asimétrica a la derecha no normal\nUn ejemplo de distribución asimétrica a la derecha es el ingreso de las familias.\n\n\n\nDistribucion de los ingresos familiares de EEUU.\n\n\n\n\n2.8.3.2 Distribución asimétrica a la izquierda no normal\nUn ejemplo de distribución asimétrica a la izquierda es la edad de fallecimiento.\n\n\n\nDistribucion de la edad de fallecimiento.\n\n\n\n\n2.8.3.3 Distribución bimodal no normal\nUn ejemplo de distribución bimodal es la hora de llegada de los clientes de un restaurante.\n\n\n\nDistribucion de la hora de llegada de los clientes de un restaurante.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estadística Descriptiva</span>"
    ]
  },
  {
    "objectID": "02-estadistica-descriptiva.html#transformaciones-de-variables",
    "href": "02-estadistica-descriptiva.html#transformaciones-de-variables",
    "title": "2  Estadística Descriptiva",
    "section": "2.9 Transformaciones de variables",
    "text": "2.9 Transformaciones de variables\nEn muchas ocasiones se suelen transformar los datos brutos para corregir alguna anormalidad de la distribución o simplemente para trabajar con unas unidades más cómodas.\nPor ejemplo, si estamos trabajando con estaturas medidas en metros y tenemos los siguientes valores:\n\\[\n1.75 \\mbox{ m}, 1.65 \\mbox{ m}, 1.80 \\mbox{ m},\n\\]\npodemos evitar los decimales multiplicando por 100, es decir, pasando de metros a centímetros:\n\\[\n175 \\mbox{ cm}, 165 \\mbox{ cm}, 180 \\mbox{ cm},\n\\]\nY si queremos reducir la magnitud de los datos podemos restarles a todos el menor de ellos, en este caso, 165cm:\n\\[10\\mbox{cm}, 0\\mbox{cm}, 15\\mbox{cm},\\]\nEstá claro que este conjunto de datos es mucho más sencillo que el original. En el fondo lo que se ha hecho es aplicar a los datos la transformación:\n\\[Y= 100X-165\\]\n\n2.9.1 Transformaciones lineales\nUna de las transformaciones más habituales es la transformación lineal:\n\\[Y=a+bX.\\]\n\nTeorema 2.1 Dada una variable muestral \\(X\\), si \\(Y\\) es la variable muestral que resulta de aplicar a \\(X\\) la transformación lineal \\(Y=a+bX\\), entonces\n\\[\\begin{align*}\n\\bar y &= a+ b\\bar x,\\\\\ns_{y} &= |b|s_{x}\n\\end{align*}\\]\nAdemás, el coeficiente de curtosis no se altera y el de asimetría sólo cambia de signo si \\(b\\) es negativo.\n\n\n\n\n\n\n\nDemostración\n\n\n\n\n\nSe deja como ejercicio. \n\n\n\n\n\n2.9.2 Transformación de tipificación y puntuaciones típicas\nUna de las transformaciones lineales más habituales es la tipificación:\n\nDefinición 2.13 (Variable tipificada) La variable tipificada de una variable estadística \\(X\\) es la variable que resulta de restarle su media y dividir por su desviación típica.\n\\[Z=\\frac{X-\\bar x}{s_{x}}\\]\nPara cada valor \\(x_i\\) de la muestra, la puntuación típica es el valor que resulta de aplicarle la transformación de tipificación\n\\[z_i=\\frac{x_i-\\bar x}{s_{x}}.\\]\n\n\n\n\n\n\n\nTip\n\n\n\nLa puntuación típica es el número de desviaciones típicas que un valor está por encima o por debajo de la media, y es útil para evitar la dependencia de una variable respecto de las unidades de medida empleadas. Esto es útil, por ejemplo, para comparar valores de variables o muestras distintas.\n\n\n\nDada una variable muetral \\(X\\), si \\(Z\\) es la variable tipificada de \\(X\\), entonces\n\\[\\bar z = 0 \\qquad s_{z} = 1.\\]\n\n\n\n\n\n\n\nDemostración\n\n\n\n\n\nSe deja como ejercicio. \n\n\n\n\nEjemplo 2.33 Las notas de 5 alumnos en dos asignaturas \\(X\\) e \\(Y\\) son\n\\[\n\\begin{array}{rccccccccc}\n\\mbox{Alumno:} & 1 & 2 & 3 & 4 & 5\\\\\n\\hline\nX: & 2 & 5 & 4 & \\color{red} 8 & 6 & \\qquad & \\bar x = 5 & \\quad s_x = 2\\\\\nY: & 1 & 9 & \\color{red} 8 & 5 & 2 & \\qquad & \\bar y = 5 & \\quad s_y = 3.16\\\\\n\\hline\n\\end{array}\n\\]\n¿Ha tenido el mismo rendimiento el cuarto alumno en la asignatura \\(X\\) que el tercero en la asignatura \\(Y\\)?\nPodría parecer que ambos alumnos han tenido el mismo rendimiento puesto que tienen la misma nota, pero si queremos ver el rendimiento relativo al resto del grupo, tendríamos que tener en cuenta la dispersión de cada muestra y medir sus puntuaciones típicas:\n\\[\n\\begin{array}{cccccc}\n\\mbox{Alumno:} & 1 & 2 & 3 & 4 & 5\\\\\n\\hline\nX: & -1.50 & 0.00 & -0.50 & \\color{red}{1.50} & 0.50 \\\\\nY: & -1.26 & 1.26 & \\color{red}{0.95} & 0.00 & -0.95\\\\\n\\hline\n\\end{array}\n\\]\nEs decir, el alumno que tiene un 8 en \\(X\\) está \\(1.5\\) veces la desviación típica por encima de la media de \\(X\\), mientras que el alumno que tiene un 8 en \\(Y\\) sólo está \\(0.95\\) desviaciones típicas por encima de la media de \\(Y\\). Así pues, el primer alumno tuvo un rendimiento superior al segundo.\nSiguiendo con el ejemplo anterior y considerando ambas asignaturas, ¿cuál es el mejor alumno?\nSi simplemente se suman las puntuaciones de cada asignatura se tiene:\n\\[\\begin{array}{rccccc}\n\\mbox{Alumno:} & 1 & 2 & 3 & 4 & 5\\\\\n\\hline\nX: & 2 & 5 & 4 & 8 & 6 \\\\\nY: & 1 & 9 & 8 & 5 & 2 \\\\\n\\hline\n\\sum & 3 & \\color{red}{14} & 12 & 13 & 8\n\\end{array}\n\\]\nEl mejor alumno sería el segundo.\nPero si se considera el rendimiento relativo tomando las puntuaciones típicas se tiene\n\\[\n\\begin{array}{rccccc}\n\\mbox{Alumno:} & 1 & 2 & 3 & 4 & 5\\\\\n\\hline\nX: & -1.50 & 0.00 & -0.50 & 1.50 & 0.50 \\\\\nY: & -1.26 & 1.26 & 0.95 & 0.00 & -0.95\\\\\n\\hline\n\\sum & -2.76 & 1.26 & 0.45 & \\color{red}{1.5} & -0.45\n\\end{array}\n\\]\nY el mejor alumno sería el cuarto.\n\n\n2.9.2.1 Transformaciones no lineales\nLas transformaciones no lineales son también habituales para corregir la anormalidad de las distribuciones.\nLa transformación \\(Y=X^2\\) comprime la escala para valores pequeños y la expande para valores altos, de manera que es muy útil para corregir asimetrías hacia la izquierda.\n\n\n\nTransformación cuadrática.\n\n\nLas transformaciones \\(Y=\\sqrt x\\), \\(Y= \\log X\\) y \\(Y=1/X\\) comprimen la escala para valores altos y la expanden para valores pequeños, de manera que son útiles para corregir asimetrías hacia la derecha.\n\n\n\nTransformación logarítmica.\n\n\n\n\n\n2.9.3 Variables clasificadoras o factores\nEn ocasiones interesa describir el comportamiento de una variable, no para toda la muestra, sino para distintos grupos de individuos correspondientes a las categorías de otra variable conocida como variable clasificadora o factor.\n\nEjemplo 2.34 Dividiendo la muestra de estaturas según el sexo se obtienen dos submuestras:\n\\[\n\\begin{array}{lll}\n\\hline\n\\mbox{Mujeres} & & 173, 158, 174, 166, 162, 177, 165, 154, 166, 182, 169, 172, 170, 168. \\\\\n\\mbox{Hombres} & & 179, 181, 172, 194, 185, 187, 198, 178, 188, 171, 175, 167, 186, 172, 176, 187. \\\\\n\\hline\n\\end{array}\n\\]\n\nHabitualmente los factores se usan para comparar la distribución de la variable principal para cada categoría del factor.\n\nEjemplo 2.35 Los siguientes diagramas permiten comparar la distribución de estaturas según el sexo.\n\n\n\nHistograma de estaturas por sexo.\n\n\n\n\n\nDiagramas de cajas de estaturas por sexo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estadística Descriptiva</span>"
    ]
  },
  {
    "objectID": "03-regresion.html",
    "href": "03-regresion.html",
    "title": "3  Regresión",
    "section": "",
    "text": "3.1 Distribución de frecuencias conjunta",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "03-regresion.html#distribución-de-frecuencias-conjunta",
    "href": "03-regresion.html#distribución-de-frecuencias-conjunta",
    "title": "3  Regresión",
    "section": "",
    "text": "3.1.1 Frecuencias conjuntas\nAl estudiar la dependencia simple entre dos variables \\(X\\) e \\(Y\\), no se pueden estudiar sus distribuciones por separado, sino que hay que estudiar la distribución conjunta de la variable bidimensional \\((X,Y)\\), cuyos valores son los pares \\((x_i,y_j)\\) donde el primer elemento es un valor \\(X\\) y el segundo uno de \\(Y\\).\n\nDefinición 3.1 (Frecuencias muestrales conjuntas) Dada una muestra de tamaño \\(n\\) de una variable bidimensional \\((X,Y)\\), para cada valor de la variable \\((x_i,y_j)\\) observado en la muestra se define\n\nFrecuencia absoluta \\(n_{ij}\\): Es el número de veces que el par \\((x_i,y_j)\\) aparece en la muestra.\nFrecuencia relativa \\(f_{ij}\\): Es la proporción de veces que el par \\((x_i,y_j)\\) aparece en la muestra.\n\n\\[f_{ij}=\\frac{n_{ij}}{n}\\]\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nPara las variables bidimensionales no tienen sentido las frecuencias acumuladas.\n\n\n\n\n3.1.2 Distribución de frecuencias bidimensional\nAl conjunto de valores de la variable bidimensional y sus respectivas frecuencias muestrales se le denomina distribución de frecuencias bidimensional, y se representa mediante una tabla de frecuencias bidimensional.\n\\[\\begin{array}{|c|ccccc|}\n\\hline\nX\\backslash Y & y_1 & \\cdots & y_j & \\cdots & y_q\\\\\n\\hline\nx_1 & n_{11} & \\cdots & n_{1j} & \\cdots & n_{1q}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\nx_i & n_{i1} & \\cdots & n_{ij} & \\cdots & n_{iq}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\nx_p & n_{p1} & \\cdots & n_{pj} & \\cdots & n_{pq}\\\\\n\\hline\n\\end{array}\\]\n\nEjemplo 3.1 La estatura (en cm) y el peso (en Kg) de una muestra de 30 estudiantes es:\n\n(179,85), (173,65), (181,71), (170,65), (158,51), (174,66), (172,62), (166,60), (194,90), (185,75), (162,55), (187,78), (198,109), (177,61), (178,70), (165,58), (154,50), (183,93), (166,51), (171,65), (175,70), (182,60), (167,59), (169,62), (172,70), (186,71), (172,54), (176,68),(168,67), (187,80).\n\nLa tabla de frecuencias bidimensional es\n\\[\\begin{array}{|c||c|c|c|c|c|c|}\n\\hline\n  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) \\\\\n  \\hline\\hline\n  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 \\\\\n  \\hline\n  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 \\\\\n  \\hline\n  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 \\\\\n  \\hline\n  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 \\\\\n  \\hline\n  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 \\\\\n  \\hline\n\\end{array}\\]\n\n\n\n3.1.3 Diagrama de dispersión\nLa distribución de frecuencias conjunta de una variable bidimensional puede representarse gráficamente mediante un diagrama de dispersión, donde los datos se representan como una colección de puntos en un plano cartesiano.\nHabitualmente la variable independiente se representa en el eje \\(X\\) y la variable dependiente en el eje \\(Y\\). Por cada par de valores \\((x_i,y_j)\\) en la muestra se dibuja un punto en el plano con esas coordenadas.\n\n\n\nDiagrama de dispersión.\n\n\nEl resultado es un conjunto de puntos que se conoce como nube de puntos.\n\nEjemplo 3.2 El siguiente diagrama de dispersión representa la distribución conjunta de estaturas y pesos de la muestra anterior.\n\n\n\nDiagrama de dispersión de estaturas y pesos.\n\n\n\n\n\n\n\n\n\nInterpretación\n\n\n\nEl diagrama de dispersión da información visual sobre el tipo de relación entre las variables.\n\n\n\nDiagramas de dispersión de diferentes tipos de relaciones.\n\n\n\n\n\n\n3.1.4 Distribuciones marginales\nA cada una de las distribuciones de las variables que conforman la variable bidimensional se les llama .\nLas distribuciones marginales se pueden obtener a partir de la tabla de frecuencias bidimensional, sumando las frecuencias por filas y columnas.\n\\[\n\\begin{array}{|c|ccccc|c|}\n\\hline\nX\\backslash Y & y_1 & \\cdots & y_j & \\cdots & y_q & \\color{red}{n_x}\\\\\n\\hline\nx_1 & n_{11} & \\cdots & n_{1j} & \\cdots & n_{1q} & \\color{red}{n_{x_1}}\\\\\n\\vdots & \\vdots & \\vdots & \\downarrow + & \\vdots & \\vdots & \\color{red}{\\vdots} \\\\\nx_i & n_{i1} & \\stackrel{+}{\\rightarrow} & n_{ij} & \\stackrel{+}{\\rightarrow} & n_{iq} & \\color{red}{n_{x_i}}\\\\\n\\vdots & \\vdots & \\vdots & \\downarrow +  & \\vdots & \\vdots & \\color{red}{\\vdots}\\\\\nx_p & n_{p1} & \\cdots & n_{pj} & \\cdots & n_{pq} & \\color{red}{n_{x_p}} \\\\\n\\hline\n\\color{red}{n_y} & \\color{red}{n_{y_1}} & \\color{red}{\\cdots} & \\color{red}{n_{y_j}} & \\color{red}{\\cdots} & \\color{red}{n_{y_q}} & n\\\\\n\\hline\n\\end{array}\n\\]\n\nEjemplo 3.3 En el ejemplo anterior de las estaturas y los pesos, las distribuciones marginales son\n\\[\n\\begin{array}{|c||c|c|c|c|c|c|c|}\n\\hline\n  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) & \\color{red}{n_x}\\\\\n  \\hline\\hline\n  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 & \\color{red}{2}\\\\\n  \\hline\n  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 & \\color{red}{8}\\\\\n  \\hline\n  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 & \\color{red}{11} \\\\\n  \\hline\n  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 & \\color{red}{7} \\\\\n  \\hline\n  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 & \\color{red}{2}\\\\\n  \\hline\n  \\color{red}{n_y} & \\color{red}{7} & \\color{red}{11} & \\color{red}{7} & \\color{red}{2} & \\color{red}{2} & \\color{red}{1} & 30\\\\\n  \\hline\n\\end{array}\n\\]\ny los estadísticos correspondientes son\n\\[\n\\begin{array}{lllll}\n\\bar x = 174.67 \\mbox{ cm} & \\quad & s^2_x = 102.06 \\mbox{ cm}^2 & \\quad & s_x = 10.1 \\mbox{ cm}\\\\\n\\bar y = 69.67 \\mbox{ Kg} & & s^2_y = 164.42 \\mbox{ Kg}^2 & & s_y = 12.82 \\mbox{ Kg}\n\\end{array}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "03-regresion.html#covarianza",
    "href": "03-regresion.html#covarianza",
    "title": "3  Regresión",
    "section": "3.2 Covarianza",
    "text": "3.2 Covarianza\nPara analizar la relación entre dos variables cuantitativas es importante hacer un estudio conjunto de las desviaciones respecto de la media de cada variable.\n\n\n\nDesviaciones de las medias en un diagrama de dispersión.\n\n\nSi dividimos la nube de puntos del diagrama de dispersión en 4 cuadrantes centrados en el punto de medias \\((\\bar x, \\bar y)\\), el signo de las desviaciones será:\n\n\n\nCuadrante\n\\((x_i-\\bar x)\\)\n\\((y_j-\\bar y)\\)\n\\((x_i-\\bar x)(y_j-\\bar y)\\)\n\n\n\n\n1\n\\(+\\)\n\\(+\\)\n\\(+\\)\n\n\n2\n\\(-\\)\n\\(+\\)\n\\(-\\)\n\n\n3\n\\(-\\)\n\\(-\\)\n\\(+\\)\n\n\n4\n\\(+\\)\n\\(-\\)\n\\(-\\)\n\n\n\n\n\n\nCuadrantes de un diagrama de dispersión.\n\n\nSi la relación entre las variables es lineal y creciente, entonces la mayor parte de los puntos estarán en los cuadrantes 1 y 3 y la suma de los productos de desviaciones será positiva.\n\\[\\sum(x_i-\\bar x)(y_j-\\bar y) &gt; 0\\]\n\n\n\nDiagrama de dispersión de una relación lineal creciente.\n\n\nSi la relación entre las variables es lineal y decreciente, entonces la mayor parte de los puntos estarán en los cuadrantes 2 y 4 y la suma de los productos de desviaciones será negativa.\n\\[\\sum(x_i-\\bar x)(y_j-\\bar y) = -\\]\n\n\n\nDiagrama de dispersión de una relación lineal decreciente.\n\n\nUsando el producto de las desviaciones respecto de las medias surge el siguiente estadístico.\n\nDefinición 3.2 (Covarianza muestral) La covarianza muestral de una variable aleatoria bidimensional \\((X,Y)\\) se define como el promedio de los productos de las respectivas desviaciones respecto de las medias de \\(X\\) e \\(Y\\).\n\\[s_{xy}=\\frac{\\sum (x_i-\\bar x)(y_j-\\bar y)n_{ij}}{n}\\]\n\nTambién puede calcularse de manera más sencilla mediante la fórmula\n\\[s_{xy}=\\frac{\\sum x_iy_jn_{ij}}{n}-\\bar x\\bar y.\\]\n\n\n\n\n\n\nInterpretación\n\n\n\nLa covarianza sirve para estudiar la relación lineal entre dos variables:\n\nSi \\(s_{xy}&gt;0\\) existe una relación lineal creciente.\nSi \\(s_{xy}&lt;0\\) existe una relación lineal decreciente.\nSi \\(s_{xy}=0\\) no existe relación lineal.\n\n\n\n\nEjemplo 3.4 Utilizando la tabla de frecuencias bidimensional de la muestra de estaturas y pesos\n\\[\n\\begin{array}{|c||c|c|c|c|c|c|c|}\n\\hline\n  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) & n_x\\\\\n  \\hline\\hline\n  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 & 2\\\\\n  \\hline\n  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 & 8\\\\\n  \\hline\n  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 & 11 \\\\\n  \\hline\n  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 & 7 \\\\\n  \\hline\n  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 & 2\\\\\n  \\hline\n  n_y & 7 & 11 & 7 & 2 & 2 & 1 & 30\\\\\n  \\hline\n\\end{array}\n\\]\n\\[\\bar x = 174.67 \\mbox{ cm} \\qquad \\bar y = 69.67 \\mbox{ Kg}\\]\nla covarianza vale\n\\[\\begin{align*}\ns_{xy} &=\\frac{\\sum x_iy_jn_{ij}}{n}-\\bar x\\bar y =  \\frac{155\\cdot 55\\cdot 2 + 165\\cdot 55\\cdot 4 + \\cdots + 195\\cdot 105\\cdot 1}{30}-174.67\\cdot 69.67 =\\\\\n& = \\frac{368200}{30}-12169.26 = 104.07 \\mbox{ cm$\\cdot$ Kg}.\n\\end{align*}\\]\nEsto indica que existe una relación lineal creciente entre la estatura y el peso.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "03-regresion.html#regresión",
    "href": "03-regresion.html#regresión",
    "title": "3  Regresión",
    "section": "3.3 Regresión",
    "text": "3.3 Regresión\nEn muchos casos el objetivo de un estudio no es solo detectar una relación entre dos variables, sino explicarla mediante alguna función matemática \\[y=f(x)\\] que permita predecir la variable dependiente para cada valor de la independiente.\nLa regresión es la parte de la Estadística encargada de construir esta función, que se conoce como función de regresión o modelo de regresión.\n\n3.3.1 Modelos de regresión simple\nDependiendo de la forma de función de regresión, existen muchos tipos de regresión simple. Los más habituales son los que aparecen en la siguiente tabla:\n\n\n\nModelo\nEcuación\n\n\n\n\nLineal\n\\(y=a+bx\\)\n\n\nCuadrático\n\\(y=a+bx+cx^2\\)\n\n\nCúbico\n\\(y=a+bx+cx^2+dx^3\\)\n\n\nPotencial\n\\(y=a\\cdot x^b\\)\n\n\nExponencial\n\\(y=e^{a+bx}\\)\n\n\nLogarítmico\n\\(y=a+b\\log x\\)\n\n\nInverso\n\\(y=a+\\frac{b}{x}\\)\n\n\nSigmoidal\n\\(y=e^{a+\\frac{b}{x}}\\)\n\n\n\nLa elección de un tipo u otro depende de la forma que tenga la nube de puntos del diagrama de dispersión.\n\n\n3.3.2 Residuos o errores predictivos\nUna vez elegida la familia de curvas que mejor se adapta a la nube de puntos, se determina, dentro de dicha familia, la curva que mejor se ajusta a la distribución, es decir, la función que mejor predice la variable dependiente.\nEl objetivo es encontrar la función de regresión que haga mínimas las distancias entre los valores de la variable dependiente observados en la muestra, y los predichos por la función de regresión. Estas distancias se conocen como residuos o errores predictivos.\n\nDefinición 3.3 (Residuos o errores predictivos) Dado el modelo de regresión \\(y=f(x)\\) para una variable bidimensional \\((X,Y)\\), el residuo o error predictivo de un valor \\((x_i,y_j)\\) observado en la muestra, es la diferencia entre el valor observado de la variable dependiente \\(y_j\\) y el predicho por la función de regresión para \\(x_i\\),\n\\[e_{ij} = y_j-f(x_i).\\]\n\n\n\n\nResiduos de un modelo de regresión.\n\n\n\n\n3.3.3 Ajuste de mínimos cuadrados\nUna forma posible de obtener la función de regresión es mediante el método de mínimos cuadrados que consiste en calcular la función que haga mínima la suma de los cuadrados de los residuos\n\\[\\sum e_{ij}^2.\\]\nEn el caso de un modelo de regresión lineal \\(f(x) = a + bx\\), como la recta depende de dos parámetros (el término independiente \\(a\\) y la pendiente \\(b\\)), la suma también dependerá de estos parámetros\n\\[\\theta(a,b) = \\sum e_{ij}^2 =\\sum (y_j - f(x_i))^2 =\\sum (y_j-a-bx_i)^2.\\]\nAsí pues, todo se reduce a buscar los valores \\(a\\) y \\(b\\) que hacen mínima esta suma.\nConsiderando la suma de los cuadrados de los residuos como una función de dos variables \\(\\theta(a,b)\\), se pueden calcular los valores de los parámetros del modelo que hacen mínima esta suma derivando e igualando a 0 las derivadas con respecto a \\(a\\) y \\(b\\).\n\\[\\begin{align*}\n\\frac{\\partial \\theta(a,b)}{\\partial a} &=  \\frac{\\partial \\sum (y_j-a-bx_i)^2 }{\\partial a} =0\\\\\n\\frac{\\partial \\theta(a,b)}{\\partial b} &=  \\frac{\\partial \\sum (y_j-a-bx_i)^2 }{\\partial b} =0\n\\end{align*}\\]\nTras resolver el sistema se obtienen los valores\n\\[\na= \\bar y - \\frac{s_{xy}}{s_x^2}\\bar x \\qquad b=\\frac{s_{xy}}{s_x^2}\n\\]\nEstos valores hacen mínimos los residuos en \\(Y\\) y por tanto dan la recta de regresión óptima.\n\n\n3.3.4 Coeficiente de determinación\nA partir de la varianza residual se puede definir otro estadístico más sencillo de interpretar.\n\nDefinición 3.4 (Coeficiente de determinación muestral \\(r^2\\)) Dado un modelo de regresión simple \\(y=f(x)\\) de una variable bidimensional \\((X,Y)\\), su coeficiente de determinación muestral es\n\\[r^2 = 1- \\frac{s_{ry}^2}{s_y^2}\\]\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nComo la varianza residual puede tomar valores entre 0 y \\(s_y^2\\), se tiene que\n\\[0\\leq r^2\\leq 1\\]\n\n\n\n\n\n\n\n\nInterpretación\n\n\n\nCuanto mayor sea \\(r^2\\), mejor explicará el modelo de regresión la relación entre las variables, en particular:\n\nSi \\(r^2 =0\\) entonces no existe relación del tipo planteado por el modelo.\nSi \\(r^2=1\\) entonces la relación que plantea el modelo es perfecta.\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEn el caso de las rectas de regresión, el coeficiente de determinación puede calcularse con esta fórmula\n\\[ r^2 =  \\frac{s_{xy}^2}{s_x^2s_y^2}.\\]\n\n\n\n\n\n\n\n\nDemostración\n\n\n\n\n\n\nPrueba. Cuando el modelo ajustado es la recta de regresión la varianza residual vale\n\\[\\begin{align*}\ns_{ry}^2 & = \\sum e_{ij}^2f_{ij} = \\sum (y_j - f(x_i))^2f_{ij} = \\sum \\left(y_j - \\bar y -\\frac{s_{xy}}{s_x^2}(x_i-\\bar x) \\right)^2f_{ij}=\\\\\n& = \\sum \\left((y_j - \\bar y)^2 +\\frac{s_{xy}^2}{s_x^4}(x_i-\\bar x)^2 - 2\\frac{s_{xy}}{s_x^2}(x_i-\\bar x)(y_j -\\bar y)\\right)f_{ij} =\\\\\n& = \\sum (y_j - \\bar y)^2f_{ij} +\\frac{s_{xy}^2}{s_x^4}\\sum (x_i-\\bar x)^2f_{ij}- 2\\frac{s_{xy}}{s_x^2}\\sum (x_i-\\bar x)(y_j -\\bar y)f_{ij}=\\\\\n& = s_y^2 + \\frac{s_{xy}^2}{s_x^4}s_x^2 - 2 \\frac{s_{xy}}{s_x^2}s_{xy} = s_y^2 - \\frac{s_{xy}^2}{s_x^2}.\n\\end{align*}\\]\ny, por tanto, el coeficiente de determinación lineal vale\n\\[\\begin{align*}\nr^2 &= 1- \\frac{s_{ry}^2}{s_y^2} = 1- \\frac{s_y^2 - \\frac{s_{xy}^2}{s_x^2}}{s_y^2} = 1 - 1 + \\frac{s_{xy}^2}{s_x^2s_y^2} = \\frac{s_{xy}^2}{s_x^2s_y^2}.\n\\end{align*}\\]\n\n\n\n\n\nEjemplo 3.5 En el ejemplo de las estaturas y pesos se tenía\n\\[\n\\begin{array}{lll}\n\\bar x = 174.67 \\mbox{ cm} & \\quad & s^2_x = 102.06 \\mbox{ cm}^2\\\\\n\\bar y = 69.67 \\mbox{ Kg} & & s^2_y = 164.42 \\mbox{ Kg}^2\\\\\ns_{xy} = 104.07 \\mbox{ cm$\\cdot$ Kg}\n\\end{array}\n\\]\nDe modo que el coeficiente de determinación lineal vale\n\\[\nr^2\n= \\frac{s_{xy}^2}{s_x^2s_y^2}\n= \\frac{(104.07 \\mbox{ cm$ \\cdot$ Kg})^2}{102.06 \\mbox{ cm}^2 \\cdot 164.42 \\mbox{ Kg}^2}\n= 0.65.\n\\]\nEsto indica que la recta de regresión del peso sobre la estatura explica el 65% de la variabilidad del peso, y de igual modo, la recta de regresión de la estatura sobre el peso explica el 65% de la variabilidad de la estatura.\n\n\n\n3.3.5 Coeficiente de correlación lineal\n\nDefinición 3.5 (Coeficiente de correlación lineal muestral) Dada una variable bidimensional \\((X,Y)\\), el coeficiente de correlación lineal muestral es la raíz cuadrada de su coeficiente de determinación lineal, con signo el de la covarianza\n\\[\nr = \\sqrt{r^2} = \\dfrac{s_{xy}}{s_xs_y}.\n\\]\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nComo \\(r^2\\) toma valores entre 0 y 1, \\(r\\) tomará valores entre -1 y 1,\n\\[-1\\leq r\\leq 1\\]\n\n\n\n\n\n\n\n\nInterpretación\n\n\n\nEl coeficiente de correlación lineal no sólo mide mide el grado de dependencia lineal sino también su dirección (creciente o decreciente):\n\nSi \\(r =0\\) entonces no existe relación lineal.\nSi \\(r=1\\) entonces existe una relación lineal creciente perfecta.\nSi \\(r=-1\\) entonces existe una relación lineal decreciente perfecta.\n\n\n\n:::{#exm-coeficiente-correlacion} En el ejemplo de las estaturas y los pesos se tenía\n\\[\n\\begin{array}{lll}\n\\bar x = 174.67 \\mbox{ cm} & \\quad & s^2_x = 102.06 \\mbox{ cm}^2\\\\\n\\bar y = 69.67 \\mbox{ Kg} & & s^2_y = 164.42 \\mbox{ Kg}^2\\\\\ns_{xy} = 104.07 \\mbox{ cm$\\cdot$ Kg}\n\\end{array}\n\\]\nDe manera que el coeficiente de correlación lineal es\n\\[\nr\n= \\frac{s_{xy}}{s_xs_y}\n= \\frac{104.07 \\mbox{ cm $\\cdot$ Kg}}{10.1 \\mbox{ cm} \\cdot 12.82 \\mbox{ Kg}}\n= +0.8.\n\\]\nEsto indica que la relación lineal entre el peso y la estatura es fuerte, y además creciente.\n\n\n3.3.6 Distintos grados de correlación\nLos siguientes diagramas de dispersión muestran modelos de regresión lineales con diferentes grados de correlación.\n\n\n\nModelos de regresión lineales con diferentes grados de correlación.\n\n\n\n\n3.3.7 Fiabilidad de las predicciones de un modelo de regresión\nAunque el coeficiente de determinación o el de correlación determinan la bondad de ajuste de un modelo de regresión, existen otros factores que influyen en la fiabilidad de las predicciones de un modelo de regresión:\n\nEl coeficiente de determinación: Cuanto mayor sea, menores serán los errores predictivos y mayor la fiabilidad de las predicciones.\nLa variabilidad de la población: Cuanto más variable es una población, más difícil es predecir y por tanto menos fiables serán las predicciones.\nEl tamaño muestral: Cuanto mayor sea, más información tendremos y, en consecuencia, más fiables serán las predicciones.\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nAdemás, hay que tener en cuenta que un modelo de regresión es válido únicamente para el rango de valores observados en la muestra. Fuera de ese rango no hay información del tipo de relación entre las variables, por lo que no deben hacerse predicciones para valores lejos de los observados en la muestra.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "03-regresion.html#regresión-no-lineal",
    "href": "03-regresion.html#regresión-no-lineal",
    "title": "3  Regresión",
    "section": "3.4 Regresión no lineal",
    "text": "3.4 Regresión no lineal\nEl ajuste de un modelo de regresión no lineal es similar al del modelo lineal y también puede realizarse mediante la técnica de mínimos cuadrados.\nNo obstante, en determinados casos un ajuste no lineal puede convertirse en un ajuste lineal mediante una sencilla transformación de alguna de las variables del modelo.\n\n3.4.1 Transformación de modelos de regresión no lineales\n\nLogarítmico: Un modelo logarítmico \\(y = a+b \\log x\\) se convierte en un modelo lineal haciendo el cambio \\(t=\\log x\\):\n\\[y=a+b\\log x = a+bt.\\]\nExponencial: Un modelo exponencial \\(y = ae^{bx}\\) se convierte en un modelo lineal haciendo el cambio \\(z = \\log y\\):\n\\[z = \\log y = \\log(ae^{bx}) =  \\log a + \\log e^{bx} = a^\\prime +bx.\\]\nPotencial: Un modelo potencial \\(y = ax^b\\) se convierte en un modelo lineal haciendo los cambios \\(t=\\log x\\) y \\(z=\\log y\\):\n\\[z = \\log y = \\log(ax^b) = \\log a + b \\log x = a^\\prime+bt.\\]\nInverso: Un modelo inverso \\(y = a+b/x\\) se convierte en un modelo lineal haciendo el cambio \\(t=1/x\\):\n\\[y = a + b(1/x) = a+bt.\\]\nSigmoidal: Un modelo curva S \\(y = e^{a+b/x}\\) se convierte en un modelo lineal haciendo los cambios \\(t=1/x\\) y \\(z=\\log y\\):\n\\[z = \\log y = \\log (e^{a+b/x}) = a+b(1/x) = a+bt.\\]\n\n\n\n3.4.2 Relación exponencial\n:::{#exm-regresion-exponencial} El número de bacterias de un cultivo evoluciona con el tiempo según la siguiente tabla:\n\\[\\begin{array}{c|c}\n\\mbox{Horas} & \\mbox{Bacterias}\\\\\n\\hline\n0 &  25 \\\\\n1 & 28 \\\\\n2 &  47\\\\\n3 & 65 \\\\\n4 & 86\\\\\n5 & 121\\\\\n6 & 190\\\\\n7 & 290\\\\\n8 & 362\n\\end{array}\n\\]\nEl diagrama de dispersión asociado es\n\n\n\nDiagrama de dispersión de la evolución de bacterias.\n\n\nSi realizamos un ajuste lineal, obtenemos la siguiente recta de regresión\n\\[\\mbox{Bacterias} = -30.18+41,27\\,\\mbox{Horas, with } r^2=0.85.\\]\n\n\n\nRegresión lineal de la evolución de un cultivo de bacterias.\n\n\n¿Es un buen modelo?\nAunque el modelo lineal no es malo, de acuerdo al diagrama de dispersión es más lógico construir un modelo exponencial o cuadrático.\nPara construir el modelo exponencial \\(y = ae^{bx}\\) hay que realizar la transformación \\(z=\\log y\\), es decir, aplicar el logaritmo a la variable dependiente.\n\\[\\begin{array}{c|c|c}\n\\mbox{Horas} & \\mbox{Bacterias} & \\mbox{$\\log$(Bacterias)}\\\\\n\\hline\n0 &  25 & 3.22\\\\\n1 & 28 & 3.33\\\\\n2 &  47 & 3.85\\\\\n3 & 65  & 4.17\\\\\n4 & 86 & 4.45\\\\\n5 & 121 & 4.80\\\\\n6 & 190 & 5.25\\\\\n7 & 290 & 5.67\\\\\n8 & 362 & 5.89\n\\end{array}\n\\]\n\n\n\nDiagrama de dispersión de la evolución del logarítmo de las bacterias de un cultivo.\n\n\nAhora sólo queda calcular la recta de regresión del logaritmo de Bacterias sobre Horas\n\\[\\mbox{Log Bacterias} = 3.107 + 0.352\\, \\mbox{Horas}.\\]\nY, deshaciendo el cambio de variable, se obtiene el modelo exponencial\n\\[\\mbox{Bacterias} = e^{3.107+0.352\\,\\mbox{Horas}}, \\mbox{ con } r^2=0.99.\\]\n\n\n\nRegresión exponencial de la evolución de las bacterias de un cultivo.\n\n\nComo se puede apreciar, el modelo exponencial se ajusta mucho mejor que el modelo lineal.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "03-regresion.html#riesgos-de-la-regresión",
    "href": "03-regresion.html#riesgos-de-la-regresión",
    "title": "3  Regresión",
    "section": "3.5 Riesgos de la regresión",
    "text": "3.5 Riesgos de la regresión\n\n3.5.1 La falta de ajuste no significa independencia\nEs importante señalar que cada modelo de regresión tiene su propio coeficiente de determinación.\n\n\n\n\n\n\nAdvertencia\n\n\n\nAsí, un coeficiente de determinación cercano a cero significa que no existe relación entre las variables del tipo planteado por el modelo, pero eso no quiere decir que las variables sean independientes, ya que puede existir relación de otro tipo.\n\n\n\n\n\n\n\n\n\n\n\nModelo de regresión lineal en una relación cuadrática.\n\n\n\n\n\n\n\nModelo de regresión cuadrático en una relación cuadrática.\n\n\n\n\n\n\n\n3.5.2 Datos atípicos en regresión\nLos datos atípicos en un estudio de regresión son los puntos que claramente no siguen la tendencia del resto de los puntos en el diagrama de dispersión, incluso si los valores del par no se pueden considerar atípicos para cada variable por separado.\n\n\n\nDiagrama de dispersión con un dato atípico.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nLos datos atípicos en regresión suelen provocar cambios drásticos en el ajuste de los modelos de regresión, y por tanto, habrá que tener mucho cuidado con ellos.\n\n\n\n\n\n\n\n\n\n\n\nModelo de regresión lineal con datos atípicos.\n\n\n\n\n\n\n\nModelo de regresión lineal sin datos atípicos.\n\n\n\n\n\n\n\n3.5.3 La paradoja de Simpson\nA veces, una tendencia desaparece o incluso se revierte cuando se divide la muestra en grupos de acuerdo a una variable cualitativa que está relacionada con la variable dependiente. Esto se conoce como la paradoja de Simpson.\n:::{#exm-paradoja-simpson} El siguiente diagrama de dispersión muestra una relación inversa entre entre las horas de estudio preparando un examen y la nota del examen.\n\n\n\nParadoja de Simpson. Relación inversa entre las horas de estudio para un examen y la nota obtenida.\n\n\nPero si se divide la muestra en dos grupos (buenos y malos estudiantes) se obtienen diferentes tendencias y ahora la relación es directa, lo que tiene más lógica.\n\n\n\nParadoja de Simpson. Relación directa entre las horas de estudio para un examen y la nota obtenida.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "04-relaciones-cualitativas.html",
    "href": "04-relaciones-cualitativas.html",
    "title": "4  Relaciones entre variables cualitativas",
    "section": "",
    "text": "4.1 Relación entre atributos ordinales\nCuando se quiere estudiar la relación entre dos atributos ordinales, o entre un atributo ordinal y una variable cuantitativa, es importante tener en cuenta el orden de las categorías. En estos casos se puede utilizar el siguiente coeficiente.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Relaciones entre variables cualitativas</span>"
    ]
  },
  {
    "objectID": "04-relaciones-cualitativas.html#relación-entre-atributos-ordinales",
    "href": "04-relaciones-cualitativas.html#relación-entre-atributos-ordinales",
    "title": "4  Relaciones entre variables cualitativas",
    "section": "",
    "text": "4.1.1 Coeficiente de correlación de Spearman\nCuando se tengan atributos ordinales es posible ordenar sus categorías y asignarles valores ordinales, de manera que se puede calcular el coeficiente de correlación lineal entre estos valores ordinales.\nEsta medida de relación entre el orden que ocupan las categorías de dos atributos ordinales se conoce como coeficiente de correlación de Spearman.\n\nDefinición 4.1 (Coeficiente de correlación de Spearman) Dada una muestra de \\(n\\) individuos en los que se han medido dos atributos ordinales \\(X\\) e \\(Y\\), el coeficiente de correlación de Spearman se define como\n\\[r_s = 1-\\frac{6\\sum d_i^2}{n(n^2-1)}\\]\ndonde \\(d_i\\) es la diferencia entre el valor ordinal de \\(X\\) y el valor ordinal de \\(Y\\) del individuo \\(i\\).\n\n\n\n\n\n\n\nImportante\n\n\n\nComo el coeficiente de correlación de Spearman es en el fondo el coeficiente de correlación lineal aplicado a los órdenes, se tiene que\n\\[-1\\leq r_s\\leq 1,\\]\n\n\n\n\n\n\n\n\nInterpretación\n\n\n\n\nSi \\(r_s=0\\) entonces no existe relación entre los atributos ordinales.\nSi \\(r_s=1\\) entonces los órdenes de los atributos coinciden y existe una relación directa perfecta.\nSi \\(r_s=-1\\) entonces los órdenes de los atributos están invertidos y existe una relación inversa perfecta.\n\nEn general, cuanto más cerca de \\(1\\) o \\(-1\\) esté \\(r_s\\), mayor será la relación entre los atributos, y cuanto más cerca de \\(0\\), menor será la relación.\n\n\n\nEjemplo 4.1 Una muestra de 5 alumnos realizaron dos tareas diferentes \\(X\\) e \\(Y\\), y se ordenaron de acuerdo a la destreza que manifestaron en cada tarea:\n\\[\n\\begin{array}{lrrrr}\n\\hline\n\\mbox{Alumnos} & X & Y & d_i & d_i^2\\\\\n\\hline\n\\mbox{Alumno 1} & 2 & 3 & -1 & 1\\\\\n\\mbox{Alumno 2} & 5 & 4 & 1 & 1 \\\\\n\\mbox{Alumno 3} & 1 & 2 & -1 & 1\\\\\n\\mbox{Alumno 4} & 3 & 1 & 2 & 4\\\\\n\\mbox{Alumno 5} & 4 & 5 & -1 & 1\\\\\n\\hline\n\\sum &  &  & 0 & 8 \\\\\n\\hline\n\\end{array}\n\\]\nEl coeficiente de correlación de Spearman para esta muestra es\n\\[\nr_s = 1-\\frac{6\\sum d_i^2}{n(n^2-1)} = 1- \\frac{6\\cdot 8}{5(5^2-1)} = 0.6.\n\\]\nEsto indica que existe bastante relación directa entre las destrezas manifestadas en ambas tareas.\n\n\nEjemplo 4.2 (Empates) Cuando hay empates en el orden de las categorías se atribuye a cada valor empatado la media aritmética de los valores ordinales que hubieran ocupado esos individuos en caso de no haber estado empatados.\nSi en el ejemplo anterior los alumnos 4 y 5 se hubiesen comportado igual en la primera tarea y los alumnos 3 y 4 se hubiesen comportado igual en la segunda tarea, entonces se tendría\n\\[\n\\begin{array}{lrrrr}\n\\hline\n\\mbox{Alumnos} & X & Y & d_i & d_i^2\\\\\n\\hline\n\\mbox{Alumno 1} & 2 & 3 & -1 & 1\\\\\n\\mbox{Alumno 2} & 5 & 4 & 1 & 1 \\\\\n\\mbox{Alumno 3} & 1 & 1.5 & -0.5 & 0.25\\\\\n\\mbox{Alumno 4} & 3.5 & 1.5 & 2 & 4\\\\\n\\mbox{Alumno 5} & 3.5 & 5 & -1.5 & 2.25\\\\\n\\hline\n\\sum &  &  & 0 & 8.5 \\\\\n\\hline\n\\end{array}\n\\]\nEl coeficiente de correlación de Spearman para esta muestra es\n\\[\nr_s = 1-\\frac{6\\sum d_i^2}{n(n^2-1)} = 1- \\frac{6\\cdot 8.5}{5(5^2-1)} = 0.58.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Relaciones entre variables cualitativas</span>"
    ]
  },
  {
    "objectID": "04-relaciones-cualitativas.html#relación-entre-atributos-nominales",
    "href": "04-relaciones-cualitativas.html#relación-entre-atributos-nominales",
    "title": "4  Relaciones entre variables cualitativas",
    "section": "4.2 Relación entre atributos nominales",
    "text": "4.2 Relación entre atributos nominales\nCuando se quiere estudiar la relación entre atributos nominales no tiene sentido calcular el coeficiente de correlación de Spearman ya que las categorías no pueden ordenarse.\nPara estudiar la relación entre atributos nominales se utilizan medidas basadas en las frecuencias de la tabla de frecuencias bidimensional, que para atributos se suele llamar tabla de contingencia.\n\nEjemplo 4.3 En un estudio para ver si existe relación entre el sexo y el hábito de fumar se ha tomado una muestra de 100 personas. La tabla de contingencia resultante es\n\\[\n\\begin{array}{|l|rr|r|}\n\\hline\n\\mbox{Sexo}\\backslash\\mbox{Fuma} & \\mbox{Si} & \\mbox{No} & n_i\\\\\n\\hline\n\\mbox{Mujer} & 12 & 28 & 40 \\\\\n\\mbox{Hombre} & 26 & 34 & 60 \\\\\n\\hline\nn_j & 38 & 62 & 100\\\\\n\\hline\n\\end{array}\n\\]\nSi el hábito de fumar fuese independiente del sexo, la proporción de fumadores en mujeres y hombres sería la misma.\n\n\n4.2.1 Frecuencias teóricas o esperadas\nEn general, dada una tabla de contingencia para dos atributos \\(X\\) e \\(Y\\),\n\\[\n\\begin{array}{|c|ccccc|c|}\n\\hline\nX\\backslash Y & y_1 & \\cdots & y_j & \\cdots & y_q & n_x\\\\\n\\hline\nx_1 & n_{11} & \\cdots & n_{1j} & \\cdots & n_{1q} & n_{x_1}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nx_i & n_{i1} & \\cdots & n_{ij} & \\cdots & n_{iq} & n_{x_i}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\nx_p & n_{p1} & \\cdots & n_{pj} & \\cdots & n_{pq} & n_{x_p} \\\\\n\\hline\nn_y & n_{y_1} & \\cdots & n_{y_j} & \\cdots & n_{y_q} & n\\\\\n\\hline\n\\end{array}\n\\]\nsi \\(X\\) e \\(Y\\) fuesen independientes, para cualquier valor \\(y_j\\) se tendría\n\\[\n\\frac{n_{1j}}{n_{x_1}} = \\frac{n_{2j}}{n_{x_2}} = \\cdots = \\frac{n_{pj}}{n_{x_p}} = \\frac{n_{1j}+\\cdots\n+n_{pj}}{n_{x_1}+\\cdots+n_{x_p}} = \\frac{n_{y_j}}{n},\\] de donde se deduce que \\[n_{ij} = \\frac{n_{x_i}n_{y_j}}{n}.\n\\]\nA esta última expresión se le llama frecuencia teórica o frecuencia esperada del par \\((x_i,y_j)\\).\n\n\n4.2.2 Coeficiente chi-cuadrado \\(\\chi^2\\)\nEs posible estudiar la relación entre dos atributos \\(X\\) e \\(Y\\) comparando las frecuencias reales con las esperadas.\n\nDefinición 4.2 (Coeficiente Chi-cuadrado \\(\\chi^2\\)) Dada una muestra de tamaño \\(n\\) en la que se han medido dos atributos \\(X\\) e \\(Y\\), se define el coeficiente \\(\\chi^2\\) como\n\\[\n\\chi^2 = \\sum_{i=1}^p\\sum_{j=1}^q \\frac{\\left(n_{ij}-\\frac{n_{x_i}n_{y_j}}{n}\\right)^2}{\\frac{n_{x_i}n_{y_j}}{n}},\n\\]\ndonde \\(p\\) es el número de categorías de \\(X\\) y \\(q\\) el número de categorías de \\(Y\\).\n\n\n\n\n\n\n\nImportante\n\n\n\nPor ser suma de cuadrados, se cumple que\n\\[\\chi^2 \\geq 0.\\]\n\n\n\n\n\n\n\n\nInterpretación\n\n\n\n\\(\\chi^2=0\\) cuando los atributos son independientes, y crece a medida que aumenta la dependencia entre las variables.\n\n\n\nEjemplo 4.4 Siguiendo con el ejemplo anterior, a partir de la tabla de contingencia\n\\[\n\\begin{array}{|l|rr|r|}\n\\hline\n\\mbox{Sexo}\\backslash\\mbox{Fuma} & \\mbox{Si} & \\mbox{No} & n_i\\\\\n\\hline\n\\mbox{Mujer} & 12 & 28 & 40 \\\\\n\\mbox{Hombre} & 26 & 34 & 60 \\\\\n\\hline\nn_j & 38 & 62 & 100\\\\\n\\hline\n\\end{array}\n\\]\nse obtienen las siguientes frecuencias esperadas\n\\[\n\\begin{array}{|l|rr|r|}\n\\hline\n\\mbox{Sexo}\\backslash\\mbox{Fuma} & \\mbox{Si} & \\mbox{No} & n_i\\\\\n\\hline\n\\mbox{Mujer} & \\frac{40\\cdot 38}{100}=15.2 & \\frac{40\\cdot 62}{100}=24.8 & 40 \\\\\n\\mbox{Hombre} & \\frac{60\\cdot 38}{100}=22.8 & \\frac{60\\cdot 62}{100}=37.2 & 60 \\\\\n\\hline\nn_j & 38 & 62 & 100\\\\\n\\hline\n\\end{array}\n\\]\ny el coeficiente \\(\\chi^2\\) vale\n\\[\\chi^2 = \\frac{(12-15.2)^2}{15.2}+\\frac{(28-24.8)^2}{24.8}+\\frac{(26-22.8)^2}{22.8}+\\frac{(34-37.2)^2}{37.2} = 1.81.\\]\nEsto indica que no existe gran relación entre el sexo y el hábito de fumar.\n\n\n\n4.2.3 Coeficiente de contingencia\nEl coeficiente \\(\\chi^2\\) depende del tamaño muestral, ya que al multiplicar por una constante las frecuencias de todas las casillas, su valor queda multiplicado por dicha constante, lo que podría llevarnos al equívoco de pensar que ha aumentado la relación, incluso cuando las proporciones se mantienen. En consecuencia el valor de \\(\\chi^2\\) no está acotado superiormente y resulta difícil de interpretar.\nPara evitar estos problemas se suele utilizar el siguiente estadístico.\n\nDefinición 4.3 (Coeficiente de contingencia) Dada una muestra de tamaño \\(n\\) en la que se han medido dos atributos \\(X\\) e \\(Y\\), se define el coeficiente de contingencia como\n\\[C = \\sqrt{\\frac{\\chi^2}{\\chi^2+n}}\\]\n\n\n\n\n\n\n\nImportante\n\n\n\nDe la definición anterior se deduce que\n\\[0\\leq C\\leq 1,\\]\n\n\n\n\n\n\n\n\nInterpretación\n\n\n\n\\(C=0\\) cuando las variables son independientes, y crece a medida que aumenta la relación.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nAunque \\(C\\) nunca puede llegar a valer 1, se puede demostrar que para tablas de contingencia con \\(k\\) filas y \\(k\\) columnas, el valor máximo que puede alcanzar \\(C\\) es \\(\\sqrt{(k-1)/k}\\).\n\n\n\nEjemplo 4.5 En el ejemplo anterior el coeficiente de contingencia vale\n\\[\nC = \\sqrt{\\frac{1.81}{1.81+100}} = 0.13.\n\\]\nComo se trata de una tabla de contingencia de \\(2\\times 2\\), el valor máximo que podría tomar el coeficiente de contingencia es \\(\\sqrt{(2-1)/2}=\\sqrt{1/2}=0.707\\), y como \\(0.13\\) está bastante lejos de este valor, se puede concluir que no existe demasiada relación entre el hábito de fumar y el sexo.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Relaciones entre variables cualitativas</span>"
    ]
  },
  {
    "objectID": "05-probabilidad.html",
    "href": "05-probabilidad.html",
    "title": "5  Probabilidad",
    "section": "",
    "text": "5.1 Experimentos y sucesos aleatorios\nEl estudio de una característica en una población se realiza a través de experimentos aleatorios.\nEn experimentos donde se mide más de una variable, la determinación del espacio muestral puede resultar compleja. En tales casos es recomendable utilizar un para construir el espacio muestral.\nEn un diagrama de árbol cada variable se representa en un nivel del árbol y cada posible valor de la variable como una rama.\nExisten distintos tipos de sucesos:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "05-probabilidad.html#experimentos-y-sucesos-aleatorios",
    "href": "05-probabilidad.html#experimentos-y-sucesos-aleatorios",
    "title": "5  Probabilidad",
    "section": "",
    "text": "Definición 5.1 (Experimento aleatorio) Un experimento aleatorio es un experimento que cumple dos condiciones:\n\nEl conjunto de posibles resultados es conocido.\nNo se puede predecir con absoluta certeza el resultado del experimento.\n\n\n\nEjemplo 5.1 Un ejemplo típico de experimentos aleatorios son los juegos de azar. El lanzamiento de un dado, por ejemplo, es un experimento aleatorio ya que:\n\nSe conoce el conjunto posibles de resultados \\(\\{1,2,3,4,5,6\\}\\).\nAntes de lanzar el dado, es imposible predecir con absoluta certeza el valor que saldrá.\n\nOtro ejemplo de experimento aleatorio sería la selección de un individuo de una población al azar y la determinación de su grupo sanguíneo.\nEn general, la obtención de cualquier muestra mediante procedimientos aleatorios será un experimento aleatorio.\n\n\nDefinición 5.2 (Espacio muestral) Al conjunto \\(\\Omega\\) de todos los posibles resultados de un experimento aleatorio se le llama espacio muestral.\n\n\nEjemplo 5.2 Algunos ejemplos de espacios muestrales son:\n\nLanzamiento de una moneda: \\(\\Omega=\\{c,x\\}\\).\nLanzamiento de un dado: \\(\\Omega=\\{1,2,3,4,5,6\\}\\).\nGrupo sanguíneo de un individuo seleccionado al azar: \\(\\Omega=\\{\\mbox{A},\\mbox{B},\\mbox{AB},\\mbox{0}\\}\\).\nEstatura de un individuo seleccionado al azar: \\(\\Omega=\\mathbb{R}^+\\).\n\n\n\n\n\nEjemplo 5.3 El siguiente diagrama de árbol representa el espacio muestral de un experimento aleatorio en el que se mide el sexo y el grupo sanguíneo de un individuo al azar.\n\n\n\nDiagrama de árbol del espacio muestral del sexo y el grupo sanguineo.\n\n\n\n\nDefinición 5.3 (Suceso aleatorio) Un suceso aleatorio es cualquier subconjunto del espacio muestral \\(\\Omega\\) de un experimento aleatorio.\n\n\n\nSuceso imposible: Es el suceso vacío \\(\\emptyset\\). Este suceso nunca ocurre.\nSucesos elementales: Son los sucesos formados por un solo elemento.\nSucesos compuestos: Son los sucesos formados por dos o más elementos.\nSuceso seguro: Es el suceso que contiene el propio espacio muestral \\(\\Omega\\). Este suceso siempre ocurre.\n\n\nEjemplo 5.4 En el experimento aleatorio del lanzamiento de un dado, con espacio muestral \\(\\Omega=\\{1, 2, 3, 4, 5, 6\\}\\), el subconjunto \\(\\{2, 4, 6\\}\\) es un suceso aleatorio que se cumple cuando sale un número par, y el subconjunto \\(\\{1, 2, 3, 4\\}\\) es un suceso aleatorio que se cumple cuando sale un número menor que 5.\n\n\n5.1.1 Espacio de sucesos\n\nDefinición 5.4 (Espacio de sucesos) Dado un espacio muestral \\(\\Omega\\) de un experimento aleatorio, el conjunto formado por todos los posibles sucesos de \\(\\Omega\\) se llama espacio de sucesos de \\(\\Omega\\) y se denota \\(\\mathcal{P}(\\Omega)\\).\n\n\nEjemplo 5.5 Dado el espacio muestral \\(\\Omega=\\{a,b,c\\}\\), su espacio de sucesos es\n\\[\\mathcal{P}(\\Omega)=\\left\\{\\emptyset, \\{a\\},\\{b\\},\\{c\\},\\{a,b\\},\\{a,c\\},\\{b,c\\},\\{a,b,c\\}\\right\\}\\]\n\nPuesto que los sucesos son conjuntos, por medio de la teoría de conjuntos se pueden definir las siguientes operaciones entre sucesos:\n\nUnión.\nIntersección.\nComplementario.\nDiferencia.\n\n\n\n5.1.2 Unión de suscesos\n\nDefinición 5.5 (Suceso unión) Dados dos sucesos \\(A,B\\subseteq \\Omega\\), se llama suceso unión de \\(A\\) y \\(B\\), y se denota \\(A\\cup B\\), al suceso formado por los elementos de \\(A\\) junto a los elementos de \\(B\\), es decir,\n\\[A\\cup B = \\{x\\,|\\, x\\in A\\mbox{ o }x\\in B\\}.\\]\n\n\n\n\nUnion de dos sucesos.\n\n\nEl suceso unión \\(A\\cup B\\) ocurre siempre que ocurre \\(A\\) o \\(B\\).\n\nEjemplo 5.6 Dado el espacio muestral correspondiente al lanzamiento de un dado \\(\\Omega=\\{1,2,3,4,5,6\\}\\) y los sucesos \\(A=\\{2,4,6\\}\\) y \\(B=\\{1,2,3,4\\}\\), la unión de \\(A\\) y \\(B\\) es \\(A\\cup B=\\{1,2,3,4,6\\}\\).\n\n\n\n5.1.3 Intersección de sucesos\n\nDefinición 5.6 (Suceso intersección) Dados dos sucesos \\(A,B\\subseteq \\Omega\\), se llama suceso intersección de \\(A\\) y \\(B\\), y se denota \\(A\\cap B\\), al suceso formado por los elementos comunes de \\(A\\) y \\(B\\), es decir,\n\\[A\\cap B = \\{x\\,|\\, x\\in A\\mbox{ y }x\\in B\\}.\\]\n\n\n\n\nIntersección de dos sucesos.\n\n\nEl suceso intersección \\(A\\cap B\\) ocurre siempre que ocurren \\(A\\) y \\(B\\).\nDiremos que dos sucesos son incompatibles si su intersección es vacía.\n\nEjemplo 5.7 Dado el espacio muestral correspondiente al lanzamiento de un dado \\(\\Omega=\\{1,2,3,4,5,6\\}\\) y los sucesos \\(A=\\{2,4,6\\}\\) y \\(B=\\{1,2,3,4\\}\\), la intersección de \\(A\\) y \\(B\\) es \\(A\\cap B=\\{2,4\\}\\), y por tanto, se trata de sucesos compatibles. Sin embargo, el suceso \\(C=\\{1, 3\\}\\) es incompatible con \\(A\\) ya que \\(A\\cap C=\\emptyset\\).\n\n\n\n5.1.4 Contrario de un suceso\n\nDefinición 5.7 (Suceso contrario) Dado suceso \\(A\\subseteq \\Omega\\), se llama suceso contrario o complementario de \\(A\\), y se denota \\(\\overline A\\), al suceso formado por los elementos de \\(\\Omega\\) que no pertenecen a \\(A\\), es decir,\n\\[\\overline A = \\{x\\,|\\, x\\not\\in A\\}.\\]\n\n\n\n\nContrario de un suceso.\n\n\nEl suceso contrario \\(\\overline A\\) ocurre siempre que no ocurre \\(A\\).\n\nEjemplo 5.8 Dado el espacio muestral correspondiente al lanzamiento de un dado \\(\\Omega=\\{1,2,3,4,5,6\\}\\) y los sucesos \\(A=\\{2,4,6\\}\\) y \\(B=\\{1,2,3,4\\}\\), el contrario de \\(A\\) es \\(\\overline A=\\{1,3,5\\}\\).\n\n\n\n5.1.5 Diferencia de sucesos\n\nDefinición 5.8 (Suceso diferencia) Dados dos sucesos \\(A,B\\subseteq \\Omega\\), se llama suceso diferencia de \\(A\\) y \\(B\\), y se denota \\(A-B\\), al suceso formado por los elementos de \\(A\\) que no pertenecen a \\(B\\), es decir,\n\\[A-B = \\{x\\,|\\, x\\in A\\mbox{ y }x\\not\\in B\\} = A \\cap \\overline B.\\]\n\n\n\n\nDiferencia de sucesos.\n\n\nEl suceso diferencia \\(A-B\\) ocurre siempre que ocurre \\(A\\) pero no ocurre \\(B\\), y también puede expresarse como \\(A\\cap \\bar B\\).\n\nEjemplo 5.9 Dado el espacio muestral correspondiente al lanzamiento de un dado \\(\\Omega=\\{1,2,3,4,5,6\\}\\) y los sucesos \\(A=\\{2,4,6\\}\\) y \\(B=\\{1,2,3,4\\}\\), la diferencia de \\(A\\) y \\(B\\) es \\(A-B=\\{6\\}\\), y la diferencia de \\(B\\) y \\(A\\) es \\(B-A=\\{1,3\\}\\).\n\n\n\n5.1.6 Álgebra de sucesos\nDados los sucesos \\(A,B,C\\in  \\mathcal{P}(\\Omega)\\), se cumplen las siguientes propiedades:\n\n\\(A\\cup A=A\\), \\(A\\cap A=A\\) (idempotencia).\n\\(A\\cup B=B\\cup A\\), \\(A\\cap B = B\\cap A\\) (conmutativa).\n\\((A\\cup B)\\cup C = A\\cup (B\\cup C)\\), \\((A\\cap B)\\cap C = A\\cap (B\\cap C)\\) (asociativa).\n\\((A\\cup B)\\cap C = (A\\cap C)\\cup (B\\cap C)\\), \\((A\\cap B)\\cup C = (A\\cup C)\\cap (B\\cup C)\\) (distributiva).\n\\(A\\cup \\emptyset=A\\), \\(A\\cap E=A\\) (elemento neutro).\n\\(A\\cup E=E\\), \\(A\\cap \\emptyset=\\emptyset\\) (elemento absorbente).\n\\(A\\cup \\overline A = E\\), \\(A\\cap \\overline A= \\emptyset\\) (elemento simétrico complementario).\n\\(\\overline{\\overline A} = A\\) (doble contrario).\n\\(\\overline{A\\cup B} = \\overline A\\cap \\overline B\\), \\(\\overline{A\\cap B} = \\overline A\\cup \\overline B\\) (leyes de Morgan).\n\\(A\\cap B\\subseteq A\\cup B\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "05-probabilidad.html#definición-de-probabilidad",
    "href": "05-probabilidad.html#definición-de-probabilidad",
    "title": "5  Probabilidad",
    "section": "5.2 Definición de probabilidad",
    "text": "5.2 Definición de probabilidad\n\n5.2.1 Definición clásica de probabilidad\n\nDefinición 5.9 (Probabilidad - Laplace) Dado un espacio muestral \\(\\Omega\\) de un experimento aleatorio donde todos los elementos de \\(\\Omega\\) son equiprobables, la probabilidad de un suceso \\(A\\subseteq \\Omega\\) es el cociente entre el número de elementos de \\(A\\) y el número de elementos de \\(\\Omega\\)\n\\[P(A) = \\frac{|A|}{|\\Omega|} = \\frac{\\mbox{nº casos favorables a A}}{\\mbox{nº casos posibles}}\\]\n\nEsta definición es ampliamente utilizada, aunque tiene importantes restricciones:\n\nEs necesario que todos los elementos del espacio muestral tengan la misma probabilidad de ocurrir (equiprobabilidad).\nNo puede utilizarse con espacios muestrales infinitos, o de los que no se conoce el número de casos posibles.\n\n\n\n\n\n\n\nPrecaución\n\n\n\nEsto no se cumple en muchos experimentos aleatorios reales.\n\n\n\nEjemplo 5.10 Dado el espacio muestral correspondiente al lanzamiento de un dado \\(\\Omega=\\{1,2,3,4,5,6\\}\\) y el suceso \\(A=\\{2,4,6\\}\\), la probabilidad de \\(A\\) es\n\\[P(A) = \\frac{|A|}{|\\Omega|} = \\frac{3}{6} = 0.5.\\]\nSin embargo, si se considera el espacio muestral correspondiente a observar el grupo sanguíneo de un individuo al azar, \\(\\Omega=\\{O,A,B,AB\\}\\), no se puede usar la definición clásica de probabilidad para calcular la probabilidad de que tenga grupo sanguíneo \\(A\\),\n\\[P(A) \\neq \\frac{|A|}{|\\Omega|} = \\frac{1}{4} = 0.25,\\]\nya que los grupos sanguíneos no son igualmente probables en las poblaciones humanas.\n\n\n\n5.2.2 Definición frecuentista de probabilidad\n\nTeorema 5.1 (Ley de los grandes números) Cuando un experimento aleatorio se repite un gran número de veces, las frecuencias relativas de los sucesos del experimento tienden a estabilizarse en torno a cierto número, que es precisamente su probabilidad.\n\nDe acuerdo al teorema anterior, podemos dar la siguiente definición\n\nDefinición 5.10 (Probabilidad frecuentista) Dado un espacio muestral \\(\\Omega\\) de un experimento aleatorio reproducible, la probabilidad de un suceso \\(A\\subseteq \\Omega\\) es la frecuencia relativa del suceso \\(A\\) en infinitas repeticiones del experimento\n\\[P(A) = lim_{n\\rightarrow \\infty}\\frac{n_{A}}{n}\\]\n\nAunque esta definición es muy útil en experimentos científicos reproducibles, también tiene serios inconvenientes, ya que\n\nSólo se calcula una aproximación de la probabilidad real.\nLa repetición del experimento debe ser en las mismas condiciones.\n\n\nEjemplo 5.11 Dado el espacio muestral correspondiente al lanzamiento de una moneda \\(\\Omega=\\{C,X\\}\\), si después de lanzar la moneda 100 veces obtenemos 54 caras, entonces la probabilidad de \\(C\\) es aproximadamente\n\\[P(C) = \\frac{n_C}{n} = \\frac{54}{100} = 0.54.\\]\nSi se considera el espacio muestral correspondiente a observar el grupo sanguíneo de un individuo al azar, \\(\\Omega=\\{O,A,B,AB\\}\\), si se toma una muestra aleatoria de 1000 personas y se observa que 412 tienen grupo sanguíneo \\(A\\), entonces la probabilidad del grupo sanguíneo \\(A\\) es aproximadamente\n\\[P(A) = \\frac{n_A}{n} = \\frac{412}{1000} = 0.412.\\]\n\n\n\n5.2.3 Definición axiomática de probabilidad\n\nDefinición 5.11 (Probabilidad - Kolmogórov) Dado un espacio muestral \\(\\Omega\\) de un experimento aleatorio, una función de probabilidad es una aplicación que asocia a cada suceso \\(A\\subseteq \\Omega\\) un número real \\(P(A)\\), conocido como probabilidad de \\(A\\), que cumple los siguientes axiomas:\n\nLa probabilidad de un suceso cualquiera es positiva o nula, \\[P(A)\\geq 0.\\]\nLa probabilidad del suceso seguro es igual a la unidad, \\[P(\\Omega)=1.\\]\nLa probabilidad de la unión de dos sucesos incompatibles (\\(A\\cap B=\\emptyset\\)) es igual a la suma de las probabilidades de cada uno de ellos, \\[P(A\\cup B) = P(A)+P(B).\\]\n\n\n\nTeorema 5.2 Si \\(P\\) es una función de de probabilidad de un espacio muestral \\(\\Omega\\), entonces para cualesquiera sucesos \\(A, B\\in \\Omega\\), se cumple\n\n\\(P(\\overline A) = 1-P(A)\\).\n\\(P(\\emptyset)= 0\\).\nSi \\(A\\subseteq B\\) entonces \\(P(A)\\leq P(B)\\).\n\\(P(A) \\leq 1\\).\n\\(P(A-B) = P(A)-P(A\\cap B)\\).\nSi \\(A\\) y \\(B\\) son sucesos compatibles, es decir, su intersección no es vacía, entonces\n\\[P(A\\cup B)= P(A) + P(B) - P(A\\cap B).\\]\nSi el suceso \\(A\\) está compuesto por los sucesos elementales \\(e_1,e_2,...,e_n\\), entonces\n\\[P(A)=\\sum_{i=1}^n P(e_i).\\]\n\n\n\n\n\n\n\n\nDemostración\n\n\n\n\n\n\nPrueba. \n\n\\(\\overline A = \\Omega \\Rightarrow P(A\\cup \\overline A) = P(\\Omega) \\Rightarrow P(A)+P(\\overline A) = 1 \\Rightarrow P(\\overline A)=1-P(A)\\).\n\\(\\emptyset = \\overline \\Omega \\Rightarrow P(\\emptyset) = P(\\overline \\Omega) = 1-P(\\Omega) = 1-1 = 0.\\)\n\\(B = A\\cup (B-A)\\). Como \\(A\\) y \\(B-A\\) son incompatibles, \\(P(B) = P(A\\cup (B-A)) = P(A)+P(B-A) \\geq P(A).\\)\nSi pensamos en probabilidades como áreas, es fácil de ver gráficamente,\n\n\n\nProbabilidad de un suceso incluido en otro.\n\n\n\\(A\\subseteq \\Omega \\Rightarrow P(A)\\leq P(\\Omega)=1.\\)\n\\(A=(A-B)\\cup (A\\cap B)\\). Como \\(A-B\\) y \\(A\\cap B\\) son incompatibles, \\(P(A)=P(A-B)+P(A\\cap B) \\Rightarrow P(A-B)=P(A)-P(A\\cap B)\\).\nSi pensamos en probabilidades como áreas, es fácil de ver gráficamente,\n\n\n\nProbabilidad de la diferencia de dos sucesos.\n\n\n\\(A\\cup B= (A-B) \\cup (B-A) \\cup (A\\cap B)\\). Como \\(A-B\\), \\(B-A\\) y \\(A\\cap B\\) son incompatibles, \\(P(A\\cup\nB)=P(A-B)+P(B-A)+P(A\\cap B) = P(A)-P(A\\cap B)+P(B)-P(A\\cap B)+P(A\\cap B)= P(A)+P(B)-P(A\\cup B)\\).\nSi pensamos en probabilidades como áreas, es fácil de ver gráficamente,\n\n\n\nProbabilidad de la unión de dos sucesos.\n\n\n\\(A=\\{e_1,\\cdots,e_n\\} = \\{e_1\\}\\cup \\cdots \\cup \\{e_n\\} \\Rightarrow\\) \\(P(A)=P(\\{e_1\\}\\cup \\cdots \\cup \\{e_n\\}) = P(\\{e_1\\})+ \\cdots P(\\{e_n\\}).\\)\n\n\n\n\n\n\n\n5.2.4 Interpretación de la probabilidad\nComo ha quedado claro en los axiomas anteriores, la probabilidad de un evento \\(A\\) es un número real \\(P(A)\\) que está siempre entre 0 y 1.\nEn cierto modo, este número expresa la verosimilitud del evento, es decir, la confianza que hay en que ocurra \\(A\\) en el experimento. Por tanto, también nos da una medida de la incertidumbre sobre el suceso.\n\nLa mayor incertidumbre corresponde a \\(P(A)=0.5\\) (Es tan probable que ocurra \\(A\\) como que no ocurra).\nLa menor incertidumbre corresponde a \\(P(A)=1\\) (\\(A\\) sucederá con absoluta certeza) y \\(P(A)=0\\) (\\(A\\) no sucederá con absoluta certeza).\n\nCuando \\(P(A)\\) está más próximo a 0 que a 1, la confianza en que no ocurra \\(A\\) es mayor que la de que ocurra \\(A\\). Por el contrario, cuando \\(P(A)\\) está más próximo a 1 que a 0, la confianza en que ocurra \\(A\\) es mayor que la de que no ocurra \\(A\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "05-probabilidad.html#probabilidad-condicionada",
    "href": "05-probabilidad.html#probabilidad-condicionada",
    "title": "5  Probabilidad",
    "section": "5.3 Probabilidad condicionada",
    "text": "5.3 Probabilidad condicionada\n\n5.3.1 Experimentos condicionados\nEn algunas ocasiones, es posible que tengamos alguna información sobre el experimento antes de su realización. Habitualmente esa información se da en forma de un suceso \\(B\\) del mismo espacio muestral que sabemos que es cierto antes de realizar el experimento.\nEn tal caso se dice que el suceso \\(B\\) es un suceso condicionante, y la probabilidad de otro suceso \\(A\\) se conoce como y se expresa\n\\[P(A|B).\\]\nEsto debe leerse como probabilidad de \\(A\\) dado \\(B\\) o probabilidad de \\(A\\) bajo la condición de \\(B\\).\nLos condicionantes suelen cambiar el espacio muestral del experimento y por tanto las probabilidades de sus sucesos.\n\nEjemplo 5.12 Supongamos que tenemos una muestra de 100 hombres y 100 mujeres con las siguientes frecuencias\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n& \\mbox{No fumadores} & \\mbox{Fumadores} \\\\\n\\hline\n\\mbox{Mujeres} & 80 & 20 \\\\\n\\hline\n\\mbox{Hombres} & 60 & 40 \\\\\n\\hline\n\\end{array}\n\\]\nEntonces, usando la definición frecuentista de probabilidad, la probabilidad de que una persona elegida al azar sea fumadora es\n\\[P(\\mbox{Fumadora})= \\frac{60}{200}=0.3.\\]\nSin embargo, si se sabe que la persona elegida es mujer, entonces la muestra se reduce a la primera fila, y la probabilidad de ser fumadora es\n\\[P(\\mbox{Fumadora}|\\mbox{Mujer})=\\frac{20}{100}=0.2.\\]\n\n\n\n5.3.2 Probabilidad condicionada\n\nDefinición 5.12 (Probabilidad condicionada) Dado un espacio muestral \\(\\Omega\\) de un experimento aleatorio, y dos dos sucesos \\(A,B\\subseteq \\Omega\\), la probabilidad de \\(A\\) condicionada por \\(B\\) es\n\\[P(A|B) = \\frac{P(A\\cap B)}{P(B)},\\]\nsiempre y cuando, \\(P(B)\\neq 0\\).\n\nEsta definición permite calcular probabilidades sin tener que alterar el espacio muestral original del experimento.\n\nEjemplo 5.13 En el ejemplo anterior\n\\[P(\\mbox{Fumadora}|\\mbox{Mujer})= \\frac{P(\\mbox{Fumadora}\\cap \\mbox{Mujer})}{P(\\mbox{Mujer})} =  \\frac{20/200}{100/200}=\\frac{20}{100}=0.2.\\]\n\n\n\n5.3.3 Probabilidad del suceso intersección\nA partir de la definición de probabilidad condicionada es posible obtener la fórmula para calcular la probabilidad de la intersección de dos sucesos.\n\\[P(A\\cap B) = P(A)P(B|A) = P(B)P(A|B).\\]\n\nEjemplo 5.14 En una población hay un 30% de fumadores y se sabe que el 40% de los fumadores tiene cáncer de pulmón. La probabilidad de que una persona elegida al azar sea fumadora y tenga cáncer de pulmón es\n\\[P(\\mbox{Fumadora}\\cap \\mbox{Cáncer})= P(\\mbox{Fumadora})P(\\mbox{Cáncer}|\\mbox{Fumadora}) = 0.3\\times 0.4 = 0.12.\\]\n\n\n\n5.3.4 Independencia de sucesos\nEn ocasiones, la ocurrencia del suceso condicionante no cambia la probabilidad original del suceso principal.\n\nDefinición 5.13 (Sucesos independientes) Dado un espacio muestral \\(\\Omega\\) de un experimento aleatorio, dos sucesos \\(A,B\\subseteq \\Omega\\) son independientes si la probabilidad de \\(A\\) no se ve alterada al condicionar por \\(B\\), y viceversa, es decir,\n\\[P(A|B) = P(A) \\quad \\mbox{and} \\quad P(B|A)=P(B),\\]\nsi \\(P(A)\\neq 0\\) y \\(P(B)\\neq 0\\).\n\nEsto significa que la ocurrencia de uno evento no aporta información relevante para cambiar la incertidumbre sobre el otro.\nCuando dos eventos son independientes, la probabilidad de su intersección es igual al producto de sus probabilidades,\n\\[P(A\\cap B) = P(A)P(B).\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "05-probabilidad.html#espacio-probabilístico",
    "href": "05-probabilidad.html#espacio-probabilístico",
    "title": "5  Probabilidad",
    "section": "5.4 Espacio probabilístico",
    "text": "5.4 Espacio probabilístico\n\nDefinición 5.14 (Espacio probabilístico) Un espacio probabilístico de un experimento aleatorio es una terna \\((\\Omega,\\mathcal{F},P)\\) donde\n\n\\(\\Omega\\) es el espacio muestral del experimento.\n\\(\\mathcal{F}\\) es un un conjunto de sucesos del experimento.\n\\(P\\) es una función de probabilidad.\n\n\nSi conocemos la probabilidad de todos los elementos de \\(\\Omega\\), entonces podemos calcular la probabilidad de cualquier suceso en \\(\\mathcal{F}\\) y se puede construir fácilmente el espacio probabilístico.\nPara determinar la probabilidad de cada suceso elemental se puede utilizar un diagrama de árbol, mediante las siguientes reglas:\n\nPara cada nodo del árbol, etiquetar la rama que conduce hasta él con la probabilidad de que la variable en ese nivel tome el valor del nodo, condicionada por los sucesos correspondientes a sus nodos antecesores en el árbol.\nLa probabilidad de cada suceso elemental en las hojas del árbol es el producto de las probabilidades de las ramas que van desde la raíz a la hoja del árbol.\n\n\n\n\nDiagrama de árbol de un espacio probabilístico.\n\n\n\n5.4.1 Árboles de probabilidad con variables dependientes\n\nEjemplo 5.15 Sea una población en la que el 30% de las personas fuman, y que la incidencia del cáncer de pulmón en fumadores es del 40% mientras que en los no fumadores es del 10%.\nEl espacio probabilístico del experimento aleatorio que consiste en elegir una persona al azar y medir las variables Fumar y Cáncer de pulmón se muestra a continuación.\n\n\n\nDiagrama de árbol del espacio probabilístico de fumar y tener cáncer de pulmón.\n\n\n\n\n\n5.4.2 Árboles de probabilidad con variables independientes\n\nEjemplo 5.16 El árbol de probabilidad asociado al experimento aleatorio que consiste en el lanzamiento de dos monedas se muestra a continuación.\n\n\n\nDiágrama de árbol del espacio probabilístico del lanzamiento de dos monedas.\n\n\n\n\nEjemplo 5.17 Dada una población en la que hay un 40% de hombres y un 60% de mujeres, el experimento aleatorio que consiste en tomar una muestra aleatoria de tres personas tiene el árbol de probabilidad que se muestra a continuación.\n\n\n\nDiagrama de árbol del espacio probabilístico del sexo de tres individuos elegidos al azar.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "05-probabilidad.html#teorema-de-la-probabilidad-total",
    "href": "05-probabilidad.html#teorema-de-la-probabilidad-total",
    "title": "5  Probabilidad",
    "section": "5.5 Teorema de la probabilidad total",
    "text": "5.5 Teorema de la probabilidad total\n\nDefinición 5.15 (Sistema completo de sucesos) Una colección de sucesos \\(A_1,A_2,\\ldots,A_n\\) de un mismo espacio muestral \\(\\Omega\\) es un sistema completo si cumple las siguientes condiciones:\n\nLa unión de todos es el espacio muestral: \\(A_1\\cup \\cdots\\cup A_n =\\Omega\\).\nSon incompatibles dos a dos: \\(A_i\\cap A_j = \\emptyset\\) \\(\\forall i\\neq j\\).\n\n\n\n\n\nPartición del espacio muestral en un sistema completo de sucesos.\n\n\nEn realidad un sistema completo de sucesos es una partición del espacio muestral de acuerdo a algún atributo, como por ejemplo el sexo o el grupo sanguíneo.\n\n5.5.1 Teorema de la probabilidad total\nConocer las probabilidades de un determinado suceso en cada una de las partes de un sistema completo puede ser útil para calcular su probabilidad.\n\nTeorema 5.3 (Probabilidad total) Dado un sistema completo de sucesos \\(A_1,\\ldots,A_n\\) y un suceso \\(B\\) de un espacio muestral \\(\\Omega\\), la probabilidad de cualquier suceso \\(B\\) del espacio muestral se puede calcular mediante la fórmula\n\\[P(B) = \\sum_{i=1}^n P(A_i\\cap B) = \\sum_{i=1}^n P(A_i)P(B|A_i).\\]\n\n\n\n\n\n\n\nDemostración\n\n\n\n\n\n\nPrueba. La demostración del teorema es sencilla, ya que al ser \\(A_1,\\ldots,A_n\\) un sistema completo tenemos\n\\[B = B\\cap E = B\\cap (A_1\\cup \\cdots \\cup A_n) = (B\\cap A_1)\\cup \\cdots \\cup (B\\cap A_n)\\]\ny como estos sucesos son incompatibles entre sí, se tiene\n\\[\\begin{align*}\nP(B) &= P((B\\cap A_1)\\cup \\cdots \\cup (B\\cap A_n)) = P(B\\cap A_1)+\\cdots + P(B\\cap A_n) =\\\\\n&= P(A_1)P(B/A_1)+\\cdots + P(A_n)P(B/A_n) = \\sum_{i=1}^n P(A_i)P(B/A_i).\n\\end{align*}\\]\n\n\n\nTeorema de la probabilidad total.\n\n\n\n\n\n\n\nEjemplo 5.18 Un determinado síntoma \\(S\\) puede ser originado por una enfermedad \\(E\\) pero también lo pueden presentar las personas sin la enfermedad. Sabemos que la prevalencia de la enfermedad \\(E\\) es \\(0.2\\). Además, se sabe que el \\(90\\%\\) de las personas con la enfermedad presentan el síntoma, mientras que sólo el \\(40\\%\\) de las personas sin la enfermedad lo presentan. Si se toma una persona al azar de la población, ¿qué probabilidad hay de que tenga el síntoma?\nPara responder a la pregunta se puede aplicar el teorema de la probabilidad total usando el sistema completo \\(\\{E,\\overline{E}\\}\\):\n\\[P(S) = P(E)P(S|E)+P(\\overline E)P(S|\\overline E) = 0.2\\cdot 0.9 + 0.8\\cdot 0.4 = 0.5.\\]\nEs decir, la mitad de la población tendrá el síntoma.\n¡En el fondo se trata de una media ponderada de probabilidades!\nLa respuesta a la pregunta anterior es evidente a la luz del árbol de probabilidad del espacio probabilístico del experimento.\n\n\n\nAplicación del teorema de la probabilidad total en un espacio probabilístico.\n\n\n\\[\\begin{align*}\nP(S) &= P(E,S) + P(\\overline E,S) = P(E)P(S|E)+P(\\overline E)P(S|\\overline E)\\\\\n& = 0.2\\cdot 0.9+ 0.8\\cdot 0.4 = 0.18 + 0.32 = 0.5.\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "05-probabilidad.html#teorema-de-bayes",
    "href": "05-probabilidad.html#teorema-de-bayes",
    "title": "5  Probabilidad",
    "section": "5.6 Teorema de Bayes",
    "text": "5.6 Teorema de Bayes\nLos sucesos de un sistema completo de sucesos \\(A_1,\\cdots,A_n\\) también pueden verse como las distintas hipótesis ante un determinado hecho \\(B\\).\nEn estas condiciones resulta útil poder calcular las probabilidades a posteriori \\(P(A_i|B)\\) de cada una de las hipótesis.\n\nTeorema 5.4 (Bayes) Dado un sistema completo de sucesos \\(A_1,\\ldots,A_n\\) y un suceso \\(B\\) de un espacio muestral \\(\\Omega\\) y otro suceso \\(B\\) del mismo espacio muestral, la probabilidad de cada suceso \\(A_i\\) \\(i=1,\\ldots,n\\) condicionada por \\(B\\) puede calcularse con la siguiente fórmula\n\\[P(A_i|B) = \\frac{P(A_i\\cap B)}{P(B)} = \\frac{P(A_i)P(B|A_i)}{\\sum_{i=1}^n P(A_i)P(B|A_i)}.\\]\n\n\nEjemplo 5.19 En el ejemplo anterior, una pregunta más interesante es qué diagnosticar a una persona que presenta el síntoma.\nEn este caso se puede interpretar \\(E\\) y \\(\\overline{E}\\) como las dos posibles hipótesis para el síntoma \\(S\\). Las probabilidades a priori para ellas son \\(P(E)=0.2\\) y \\(P(\\overline E)=0.8\\). Esto quiere decir que si no se dispone de información sobre el síntoma, el diagnóstico será que la persona no tiene la enfermedad.\nSin embargo, si al reconocer a la persona se observa que presenta el síntoma, dicha información condiciona a las hipótesis, y para decidir entre ellas es necesario calcular sus probabilidades a posteriori, es decir, \\(P(E|S)\\) y \\(P(\\overline{E}|S)\\).\nPara calcular las probabilidades a posteriori se puede utilizar el teorema de Bayes:\n\\[\\begin{align*}\nP(E|S) &= \\frac{P(E)P(S|E)}{P(E)P(S|E)+P(\\overline{E})P(S|\\overline{E})} = \\frac{0.2\\cdot 0.9}{0.2\\cdot 0.9 + 0.8\\cdot 0.4} = \\frac{0.18}{0.5}=0.36,\\\\\nP(\\overline{E}|S) &= \\frac{P(\\overline{E})P(S|\\overline{E})}{P(E)P(S|E)+P(\\overline{E})P(S|\\overline{E})} = \\frac{0.8\\cdot 0.4}{0.2\\cdot 0.9 + 0.8\\cdot 0.4} = \\frac{0.32}{0.5}=0.64.\n\\end{align*}\\]\nComo se puede ver la probabilidad de tener la enfermedad ha aumentado. No obstante, la probabilidad de no tener la enfermedad sigue siendo mayor que la de tenerla, y por esta razón el diagnóstico seguirá siendo que no tiene la enfermedad.\nEn este caso se dice que el síntoma \\(S\\) no es determinante a la hora de diagnosticar la enfermedad.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "05-probabilidad.html#epidemiología",
    "href": "05-probabilidad.html#epidemiología",
    "title": "5  Probabilidad",
    "section": "5.7 Epidemiología",
    "text": "5.7 Epidemiología\nUna de las ramas de la Medicina que hace un mayor uso de la probabilidad es la , que estudia la distribución y las causas de las enfermedades en las poblaciones, identificando factores de riesgos para las enfermedades de cara a la atención médica preventiva.\nEn Epidemiología interesa la frecuencia de un suceso médico \\(E\\) (típicamente una enfermedad como la gripe, un factor de riesgo como fumar o un factor de protección como vacunarse) que se mide mediante una variable nominal con dos categorías (ocurrencia o no del suceso).\nHay diferentes medidas relativas a la frecuencia de un suceso médico. Las más importantes son:\n\nPrevalencia\nIncidencia\nRiesgo relativo\nOdds ratio\n\n\n5.7.1 Prevalencia\n\nDefinición 5.16 (Prevalencia) La prevalencia de un suceso médico \\(E\\) es la proporción de una población que está afectada por el suceso.\n\\[\\mbox{Prevalencia}(E) = \\frac{\\mbox{Nº individuos afectados por $E$}}{\\mbox{Tamaño poblacional}}\\]\n\nA menudo, la prevalencia se estima mediante una muestra como la frecuencia relativa de los individuos afectados por el suceso en la muestra. Es también común expresarla esta frecuencia como un porcentaje.\n\nEjemplo 5.20 Para estimar la prevalencia de la gripe se estudió una muestra de \\(1000\\) personas de las que \\(150\\) presentaron gripe. Así, la prevalencia de la gripe es aproximadamente \\(150/1000=0.15\\), es decir, un 15%.\n\n\n\n5.7.2 Incidencia\nLa mide la probabilidad de ocurrencia de un suceso médico en una población durante un periodo de tiempo específico. La incidencia puede medirse como una proporción acumulada o como una tasa.\n\nDefinición 5.17 (Incidencia acumulada) La incidencia acumulada de un suceso médico \\(E\\) es la proporción de individuos que experimentaron el evento en un periodo de tiempo, es decir, el número de nuevos casos afectados por el evento en el periodo de tiempo, divido por el tamaño de la población inicialmente en riesgo de verse afectada.\n\\[R(E)=\\frac{\\mbox{Nº de nuevos casos con $E$}}{\\mbox{Tamaño de la población en riesgo}}.\\]\n\n\nEjemplo 5.21 Una población contenía inicialmente \\(1000\\) personas sin gripe y después de dos años se observó que \\(160\\) de ellas sufrieron gripe. La incidencia acumulada de la gripe es 160 casos pro 1000 personas por dos años, es decir, 16% en dos años.\n\n\n\n5.7.3 Tasa de incidencia o Riesgo absoluto\n\nDefinición 5.18 (Riesgo absoluto) La tasa de incidencia o riesgo absoluto de un suceso médico \\(E\\) es el número de nuevos casos afectados por el evento divido por la población en riesgo y por el número de unidades temporales del periodo considerado.\n\\[R(E)=\\frac{\\mbox{Nº nuevos casos con $E$}}{\\mbox{Tamaño población en riesgo}\\times \\mbox{Nº unidades de tiempo}}\\]\n\n\nEjemplo 5.22 Una población contenía inicialmente \\(1000\\) personas sin gripe y después de dos años se observó que \\(160\\) de ellas sufrieron gripe. Si se considera el año como intervalo de tiempo, la tasa de incidencia de la gripe es \\(160\\) casos dividida por \\(1000\\) personas y por dos años, es decir, \\(80\\) casos por \\(1000\\) personas-año o 8% de personas al año.\n\n\n\n5.7.4 Prevalencia vs Incidencia\nLa prevalencia no debe confundirse con la incidencia. La prevalencia indica cómo de extendido está el suceso médico en una población, sin preocuparse por cuándo los sujetos se han expuesto al riesgo o durante cuánto tiempo, mientras que la incidencia se fija en el riesgo de verse afectado por el suceso en un periodo concreto de tiempo.\nAsí, la prevalencia se calcula en estudios transversales en un momento temporal puntual, mientras que para medir la incidencia se necesita un estudio longitudinal que permita observar a los individuos durante un periodo de tiempo.\nLa incidencia es más útil cuando se pretende entender la causalidad del suceso: por ejemplo, si la incidencia de una enfermedad en una población aumenta, seguramente hay un factor de riesgo que lo está promoviendo.\nCuando la tasa de incidencia es aproximadamente constante en la duración del suceso, la prevalencia es aproximadamente el producto de la incidencia por la duración media del suceso, es decir,\n\\[ \\mbox{Prevalencia} = \\mbox{Incidencia} \\times \\mbox{duración}\\]\n\n\n5.7.5 Comparación de riesgos\nPara determinar si un factor o característica está asociada con el suceso médico es necesario comparar el riesgo del suceso en dos poblaciones, una expuesta al factor y la otra no. El grupo expuesto al factor se conoce como grupo tratamiento o grupo experimental \\(T\\) y el grupo no expuesto como grupo control \\(C\\).\nHabitualmente los casos observados para cada grupo se representan en una tabla de \\(2\\times2\\) como la siguiente:\n\n\n\n\n\n\nSuceso \\(E\\)\n\n\nNo suceso \\(\\overline E\\)\n\n\n\n\n\n\nGrupo tratamiento \\(T\\)\n\n\n\\(a\\)\n\n\n\\(b\\)\n\n\n\n\nGrupo control \\(C\\)\n\n\n\\(c\\)\n\n\n\\(d\\)\n\n\n\n\n\n\n5.7.6 Riesgo atribuible o diferencia de riesgos \\(RA\\)\n\nDefinición 5.19 (Riesgo atribuible) El riesgo atribuible o diferencia de riesgo de un suceso médico \\(E\\) para los individuos expuestos a un factor es la diferencia entre los riesgos absolutos de los grupos tratamiento y control.\n\\[RA(E)=R_T(E)-R_C(E)=\\frac{a}{a+b}-\\frac{c}{c+d}.\\]\n\nEl riesgo atribuible es el riesgo de un suceso que es debido específicamente al factor de interés.\nObsérvese que el riesgo atribuible puede ser positivo, cuando el riesgo del grupo tratamiento es mayor que el del grupo control, o negativo, de lo contrario.\n\nEjemplo 5.23 Para determinar la efectividad de una vacuna contra la gripe, una muestra de \\(1000\\) personas sin gripe fueron seleccionadas al comienzo del año. La mitad de ellas fueron vacunadas (grupo tratamiento) y la otra mitad recibieron un placebo (grupo control). La tabla siguiente resume los resultados al final del año.\n\n\n\n\n\n\nGripe \\(E\\)\n\n\nNo gripe \\(\\overline E\\)\n\n\n\n\n\n\nGrupo tratamiento (vacunados)\n\n\n20\n\n\n480\n\n\n\n\nGrupo control (No vacunados)\n\n\n80\n\n\n420\n\n\n\n\nEl riesgo atribuible de contraer la gripe cuando se es vacunado es\n\\[AR(D) = \\frac{20}{20+480}-\\frac{80}{80+420} = -0.12.\\]\nEsto quiere decir que el riesgo de contraer la gripe es un 12% menor en vacunados que en no vacunados.\n\n\n\n5.7.7 Riesgo relativo \\(RR\\)\n\nDefinición 5.20 (Riesgo relativo) El riesgo relativo de un suceso médico \\(E\\) para los individuos expuestos a un factor es el cociente entre las proporciones de individuos afectados por el suceso en un periodo de tiempo de los grupos tratamiento y control. Es decir, el cociente entre las incidencias de grupo tratamiento y el grupo control.\n\\[RR(D)=\\frac{\\mbox{Riesgo grupo tratamiento}}{\\mbox{Riesgo grupo control}}=\\frac{R_T(E)}{R_C(E)}=\\frac{a/(a+b)}{c/(c+d)}\\]\n\n\n\n\n\n\n\nInterpretación\n\n\n\nEl riesgo relativo compara el riesgo de desarrollar un suceso médico entre el grupo tratamiento y el grupo control.\n\n\\(RR=1\\) \\(\\Rightarrow\\) No hay asociación entre el suceso y la exposición al factor.\n\\(RR&lt;1\\) \\(\\Rightarrow\\) La exposición al factor disminuye el riesgo del suceso.\n\\(RR&gt;1\\) \\(\\Rightarrow\\) La exposición al factor aumenta el riesgo del suceso.\n\nCuanto más lejos de 1, más fuerte es la asociación.\n\n\n\nEjemplo 5.24 Para determinar la efectividad de una vacuna contra la gripe, una muestra de \\(1000\\) personas sin gripe fueron seleccionadas al comienzo del año. La mitad de ellas fueron vacunadas (grupo tratamiento) y la otra mitad recibieron un placebo (grupo control). La tabla siguiente resume los resultados al final del año.\n\n\n\n\n\n\nGripe \\(E\\)\n\n\nNo gripe \\(\\overline E\\)\n\n\n\n\n\n\nGrupo tratamiento (vacunados)\n\n\n20\n\n\n480\n\n\n\n\nGrupo control (No vacunados)\n\n\n80\n\n\n420\n\n\n\n\nEl riesgo relativo de contraer la gripe cuando se es vacunado es\n\\[RR(D) = \\frac{20/(20+480)}{80/(80+420)} = 0.25.\\]\nAsí, la probabilidad de contraer la gripe en los individuos vacunados fue la cuarta parte de la de contraerla en el caso de no haberse vacunado, es decir, la vacuna reduce el riesgo de gripe un 75%.\n\n\n\n5.7.8 Odds\nUna forma alternativa de medir el riesgo de un suceso médico es el odds.\n\nDefinición 5.21 El odds de un suceso médico \\(E\\) en una población es el cociente entre el número de individuos que adquirieron el suceso y los que no en un periodo de tiempo.\n\\[ODDS(E)=\\frac{\\mbox{Nº nuevos casos con $E$}}{\\mbox{Nº casos sin $E$}}=\\frac{P(E)}{P(\\overline E)}.\\]\n\nA diferencia de la incidencia, que es una proporción menor o igual que 1, el odds puede ser mayor que 1. No obstante es posible convertir el odds en una probabilidad con al fórmula\n\\[P(E) = \\frac{ODDS(E)}{ODDS(E)+1}.\\]\n\nEjemplo 5.25 Una población contenía inicialmente \\(1000\\) personas sin gripe. Después de un año \\(160\\) de ellas tuvieron gripe. Entonces el odds de la gripe es \\(160/840\\).\nObsérvese que la incidencia es \\(160/1000\\).\n\n\n\n5.7.9 Odds ratio \\(OR\\)\n\nDefinición 5.22 (Odds ratio) El odds ratio o la oportunidad relativa de un suceso médico \\(E\\) para los individuos expuestos a un factor es el cociente entre los odds del sucesos de los grupos tratamiento y control.\n\\[OR(E)=\\frac{\\mbox{Odds en grupo tratamiento}}{\\mbox{Odds en grupo control}}=\\frac{a/b}{c/d}=\\frac{ad}{bc}.\\]\n\n\n\n\n\n\n\nInterpretación\n\n\n\nEl odds ratio compara los odds de un suceso médico entre el grupo tratamiento y control. La interpretación es similar a la del riesgo relativo:\n\n\\(OR=1\\) \\(\\Rightarrow\\) No existe asociación entre el suceso y la exposición al factor.\n\\(OR&lt;1\\) \\(\\Rightarrow\\) La exposición al factor disminuye el riesgo del suceso.\n\\(OR&gt;1\\) \\(\\Rightarrow\\) La exposición al factor aumenta el riesgo del suceso.\n\nCuanto más lejos de 1, más fuerte es la asociación.\n\n\n\nEjemplo 5.26 Para determinar la efectividad de una vacuna contra la gripe, una muestra de 1000 personas sin gripe fueron seleccionadas al comienzo del año. La mitad de ellas fueron vacunadas (grupo tratamiento) y la otra mitad recibieron un placebo (grupo control). La tabla siguiente resume los resultados al final del año.\n\n\n\n\n\n\nGripe \\(E\\)\n\n\nNo gripe \\(\\overline E\\)\n\n\n\n\n\n\nGrupo tratamiento (vacunados)\n\n\n20\n\n\n480\n\n\n\n\nGrupo control (No vacunados)\n\n\n80\n\n\n420\n\n\n\n\nEl odds ratio de sufrir la gripe para los individuos vacunados es\n\\[OR(D) = \\frac{20/480}{80/420} = 0.21875.\\]\nEsto quiere decir que el odds de sufrir la gripe frente a no sufrirla en los vacunados es casi un quinto del de los no vacunados, es decir, que aproximadamente por cada 22 personas vacunadas con gripe habrá 100 personas no vacunadas con gripe.\n\n\n\n5.7.10 Riesgo relativo vs Odds ratio\nEl riesgo relativo y el odds ratio son dos medidas de asociación pero su interpretación es ligeramente diferente. Mientras que el riesgo relativo expresa una comparación de riesgos entre los grupos tratamiento y control, el odds ratio expresa una comparación de odds, que no es lo mismo que el riesgo. Así, un odds ratio de 2 no significa que el grupo tratamiento tiene el doble de riesgo de adquirir el suceso.\nLa interpretación del odds ratio es un poco más enrevesada porque es contrafactual, y nos da cuántas veces es más frecuente el suceso en el grupo tratamiento en comparación con el control, asumiendo que en el grupo control es tan frecuente que ocurra el suceso como que no.\nLa ventaja del odds ratio es que no depende de la prevalencia o la incidencia del suceso, y debe usarse siempre que el número de individuos que presenta el suceso se selecciona arbitrariamente en ambos grupos, como ocurre en los estudios casos-control.\n\nEjemplo 5.27 Para determinar la asociación entre el cáncer de pulmón y fumar se tomaron dos muestras (la segunda con el doble de individuos sin cáncer) obteniendo los siguientes resultados:\nMuestra 1\n\n\n\n\n\n\nCáncer\n\n\nNo cáncer\n\n\n\n\n\n\nFumadores\n\n\n60\n\n\n80\n\n\n\n\nNo fumadores\n\n\n40\n\n\n320\n\n\n\n\n\\[\\begin{align*}\nRR(D) &= \\frac{60/(60+80)}{40/(40+320)} = 3.86.\\\\\nOR(D) &= \\frac{60/80}{40/320} = 6.\n\\end{align*}\\]\nMuestra 2\n\n\n\n\n\n\nCáncer\n\n\nNo cáncer\n\n\n\n\n\n\nFumadores\n\n\n60\n\n\n160\n\n\n\n\nNo fumadores\n\n\n40\n\n\n640\n\n\n\n\n\\[\\begin{align*}\nRR(D) &= \\frac{60/(60+160)}{40/(40+640)} = 4.64.\\\\\nOR(D) &= \\frac{60/160}{40/640} = 6.\n\\end{align*}\\]\nAsí, cuando cambia la incidencia o prevalencia de un suceso (cáncer de pulmón) el riesgo relativo cambia, mientras que el odds ratio no.\n\nLa relación entre el riesgo relativo y el odds ratio viene dada por la siguiente fórmula\n\\[RR = \\frac{OR}{1-R_0+R_0\\cdot OR} = OR \\frac{1-R_1}{1-R_0},\\]\ndonde \\(R_C\\) and \\(R_T\\) son la prevalencia o la incidencia en los grupos control y tratamiento respectivamente.\nEl odds ratio siempre sobrestima el riesgo relativo cuando este es mayor que 1 y lo subestima cuando es menor que 1. No obstante, con sucesos médicos raros (con una prevalencia o incidencia baja) el riesgo relativo y el odds ratio son casi iguales.\n\n\n\nOdss ratio versus riesgo relativo.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "05-probabilidad.html#tests-diagnósticos",
    "href": "05-probabilidad.html#tests-diagnósticos",
    "title": "5  Probabilidad",
    "section": "5.8 Tests diagnósticos",
    "text": "5.8 Tests diagnósticos\nEn Epidemiología es común el uso de test para diagnosticar enfermedades.\nGeneralmente estos test no son totalmente fiables, sino que hay cierta probabilidad de acierto o fallo en el diagnóstico, que suele representarse en la siguiente tabla:\n\n\n\n\n\n\nPresencia enfermedad \\(E\\)\n\n\nAusencia enfermedad \\(\\overline E\\)\n\n\n\n\n\n\nTest positivo \\(+\\)\n\n\nVerdadero positivo \\(VP\\)\n\n\nFalso positivo \\(FP\\)\n\n\n\n\nTest negativo \\(−\\)\n\n\nFalso negativo \\(FN\\)\n\n\nVerdadero Negativo \\(VN\\)\n\n\n\n\n\n5.8.1 Sensibilidad y especificidad de un test diagnóstico\nLa fiabilidad de un test diagnóstico depende de las siguientes probabilidades.\n\nDefinición 5.23 (Sensibilidad) La sensibilidad de un test diagnóstico es la proporción de resultados positivos del test en personas con la enfermedad,\n\\[P(+|E)=\\frac{VP}{VP+FN}.\\]\n\n\nDefinición 5.24 (Especificidad) La especificidad de un test diagnóstico es la proporción de resultados negativos del test en personas sin la enfermedad,\n\\[P(-|\\overline{E})=\\frac{VN}{VN+FP}.\\]\n\nNormalmente existe un balance entre la sensibilidad y la especificidad.\nUn test con una alta sensibilidad detectará la enfermedad en la mayoría de las personas enfermas, pero también dará más falsos positivos que un test menos sensible. De este modo, un resultado positivo en un test con una gran sensibilidad no es muy útil para confirmar la enfermedad, pero un resultado negativo es útil para descartar la enfermedad, ya que raramente da resultados negativos en personas con la enfermedad.\nPor otro lado, un test con una alta especificidad descartará la enfermedad en la mayoría de las personas sin la enfermedad, pero también producirá más falsos negativos que un test menos específico. Así, un resultado negativo en un test con una gran especificidad no es útil para descartar la enfermedad, pero un resultado positivo es muy útil para confirmar la enfermedad, ya que raramente da resultados positivos en personas sin la enfermedad.\n\nEjemplo 5.28 Un test diagnóstico para la gripe se ha aplicado a una muestra aleatoria de \\(1000\\) personas. Los resultados aparecen resumidos en la siguiente tabla.\n\n\n\n\n\n\nPresencia de gripe \\(E\\)\n\n\nAusencia de gripe \\(\\overline E\\)\n\n\n\n\n\n\nTest \\(+\\)\n\n\n95\n\n\n90\n\n\n\n\nTest \\(−\\)\n\n\n5\n\n\n810\n\n\n\n\nSegún esta muestra, la prevalencia de la gripe puede estimarse como\n\\[P(E) = \\frac{95+5}{1000} = 0.1.\\]\nLa sensibilidad del test diagnóstico es\n\\[P(+|E) = \\frac{95}{95+5}= 0.95.\\]\nY la especificidad es\n\\[P(-|\\overline{E}) = \\frac{810}{90+810}=0.9.\\]\nAsí pues, se trata de un buen test tanto para descartar la enfermedad como para confirmarla, pero es un poco mejor para confirmarla que para descartarla porque la especificidad es mayor que la sensibilidad.\n\nDecidir entre un test con una gran sensibilidad o un test con una gran especificidad depende del tipo de enfermedad y el objetivo del test. En general, utilizaremos un test sensible cuando:\n\nLa enfermedad es grave y es importante detectarla.\nLa enfermedad es curable.\nLos falsos positivos no provocan traumas serios.\n\nY utilizaremos un test específico cuando:\n\nLa enfermedad es importante pero difícil o imposible de curar.\nLos falsos positivos pueden provocar traumas serios.\nEl tratamiento de los falsos positivos puede tener graves consecuencias.\n\n\n\n5.8.2 Valores predictivos de un test diagnóstico\nPero el aspecto más importante de un test diagnóstico es su poder predictivo, que se mide con las siguientes probabilidades a posteriori.\n\nDefinición 5.25 (Valor predictivo positivo) El valor predictivo positivo de un test diagnóstico es la proporción de personas con la enfermedad entre las personas con resultado positivo en el test,\n\\[P(E|+) = \\frac{VP}{VP+FP}.\\]\n\n\nDefinición 5.26 (Valor predictivo negativo) El valor predictivo negativo de un test diagnóstico es la proporción de personas sin la enfermedad entre las personas con resultado negativo en el test,\n\\[P(\\overline{E}|-) = \\frac{VN}{VN+FN}.\\]\n\n\n\n\n\n\n\nInterpretación\n\n\n\nLos valores predictivos positivo y negativo permiten confirmar o descartar la enfermedad, respectivamente, si alcanzan al menos el umbral de \\(0.5\\).\n\\[\n\\begin{array}{rcl}\nVPP&gt;0.5 & \\Rightarrow & \\mbox{Diagnosticar la enfermedad}\\\\\nVPN&gt;0.5 & \\Rightarrow & \\mbox{Diagnosticar la no enfermedad}\n\\end{array}\n\\]\n\n\nNo obstante, estas probabilidades dependen de la prevalencia de la enfermedad \\(P(E)\\). Pueden calcularse a partir de la sensibilidad y la especificidad del test diagnóstico usando el teorema de Bayes.\n\\[\\begin{align*}\nVPP=P(E|+) &= \\frac{P(E)P(+|E)}{P(E)P(+|E)+P(\\overline{E})P(+|\\overline{E})}\\\\\nVPN=P(\\overline{E}|-) &= \\frac{P(\\overline{E})P(-|\\overline{E})}{P(E)P(-|E)+P(\\overline{E})P(-|\\overline{E})}\n\\end{align*}\\]\nAsí, con enfermedades frecuentes, el valor predictivo positivo aumenta, y con enfermedades raras, el valor predictivo negativo aumenta.\n\nEjemplo 5.29 Siguiendo con el ejemplo anterior de la gripe, se tiene que el valor predictivo positivo del test es\n\\[VPP = P(E|+) = \\frac{95}{95+90} = 0.5135.\\]\nComo este valor es mayor que \\(0.5\\), eso significa que se diagnosticará la gripe si el resultado del test es positivo. No obstante, la confianza en el diagnóstico será baja, ya que el valor es poco mayor que \\(0.5\\).\nPor otro lado, el valor predictivo negativo es\n\\[VPN = P(\\overline{E}|-) = \\frac{810}{5+810} = 0.9939.\\]\nComo este valor es casi 1, eso significa que es casi seguro que no se tiene la gripe cuando el resultado del test es negativo.\nAsí, se puede concluir que este test es muy potente para descartar la gripe, pero no lo est tanto para confirmarla.\n\n\n\n5.8.3 Razón de verosimilitud de un test diagnóstico\nLa siguientes medidas también se derivan de la sensibilidad y la especificidad de un test diagnóstico.\n\nDefinición 5.27 (Razón de verosimilitud positiva) La razón de verosimilitud positiva de un test diagnóstico es el cociente entre la probabilidad de un resultado positivo en personas con la enfermedad y personas sin la enfermedad, respectivamente.\n\\[RV+=\\frac{P(+|E)}{P(+|\\overline{E})} = \\frac{\\mbox{Sensibilidad}}{1-\\mbox{Especificidad}}.\\]\n\n\nDefinición 5.28 (Razón de verosimilitud negativa) La razón de verosimilitud negativa de un test diagnóstico es el cociente entre la probabilidad de un resultado negativo en personas con la enfermedad y personas sin la enfermedad, respectivamente.\n\\[RV-=\\frac{P(-|E)}{P(-|\\overline{E})} = \\frac{1-\\mbox{Sensibilidad}}{\\mbox{Especificidad}}.\\]\n\n\n\n\n\n\n\nInterpretación\n\n\n\nLa razón de verosimilitud positiva puede interpretarse como el número de veces que un resultado positivo es más probable en personas con la enfermedad que en personas sin la enfermedad.\nPor otro lado, la razón de verosimilitud negativa puede interpretarse como el número de veces que un resultado negativo es más probable en personas con la enfermedad que en personas sin la enfermedad.\nLas probabilidades a posteriori pueden calculares a partir de las probabilidades a priori usando las razones de verosimilitud\n\\[P(E|+) = \\frac{P(E)P(+|E)}{P(E)P(+|E)+P(\\overline{E})P(+|\\overline{E})} = \\frac{P(E)RV+}{1-P(E)+P(E)RV+}\\]\nAsí,\n\nUna razón de verosimilitud positiva mayor que 1 aumenta la probabilidad de la enfermedad.\nUna razón de verosimilitud positiva menor que 1 disminuye la probabilidad de la enfermedad.\nUna razón de verosimilitud 1 no cambia la probabilidad a priori de la de tener la enfermedad.\n\n\n\n\n\n\nRazón de verosimilitud.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "08-estimacion.html",
    "href": "08-estimacion.html",
    "title": "6  Estimación de parámetros poblacionales",
    "section": "",
    "text": "6.1 Distribuciones muestrales\nLos valores de una variable \\(X\\) en una muestra de tamaño \\(n\\) de una población pueden verse como el valor de una variable aleatoria \\(n\\)-dimensional.\nLos valores que puede tomar esta variable \\(n\\) dimensional, serán todas las posibles muestras de tamaño \\(n\\) que pueden extraerse de la población.\nLas tres características fundamentales de la variable aleatoria muestral son:\nLas dos primeras cuestiones pueden resolverse si se utiliza muestreo aleatorio simple para obtener la muestra. En cuanto a la última, hay que responder, a su vez, a dos cuestiones:\nEn este tema se abordará la segunda cuestión, es decir, suponiendo que se conoce el modelo de distribución de una población, se intentará estimar los principales parámetros que la definen. Por ejemplo, los principales parámetros que definen las distribuciones vistas en el tema anterior son:\nLa distribución de probabilidad de los valores de la variable muestral depende claramente de la distribución de probabilidad de los valores de la población.\nPor ser función de una variable aleatoria, un estadístico en el muestreo es también una variable aleatoria. Por tanto, su distribución de probabilidad también depende de la distribución de la población y de los parámetros que la determinan (\\(\\mu\\), \\(\\sigma\\), \\(p\\), …).\n¿Cuál es la probabilidad de obtener una media muestral que aproxime la media poblacional con un error máximo de 0.5?\nComo hemos visto, para conocer la distribución de un estadístico muestral, es necesario conocer la distribución de la población, lo cual no siempre es posible. Afortunadamente, para muestras grandes es posible aproximar la distribución de algunos estadísticos como la media, gracias al siguiente teorema:\nEste teorema además es la explicación de que la mayoría de las variables biológicas presenten una distribución normal, ya que suelen ser causa de múltiples factores que suman sus efectos de manera independiente.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimación de parámetros poblacionales</span>"
    ]
  },
  {
    "objectID": "08-estimacion.html#distribuciones-muestrales",
    "href": "08-estimacion.html#distribuciones-muestrales",
    "title": "6  Estimación de parámetros poblacionales",
    "section": "",
    "text": "Definición 6.1 (Variable aleatoria muestral) Una variable aleatoria muestral de una variable \\(X\\) estudiada en una población es una colección de \\(n\\) variables aleatorias \\(X_1,\\ldots,X_n\\) tales que:\n\nCada una de las variables \\(X_i\\) sigue la misma distribución de probabilidad que la variable \\(X\\) en la población.\nTodas las variables \\(X_i\\) son mutuamente independientes.\n\n\n\n\n\n\nProceso de obtención de la muestra.\n\n\n\n\nHomogeneidad: Las \\(n\\) variables que componen la variable aleatoria muestral siguen la misma distribución.\nIndependencia: Las variables son independientes entre sí.\nModelo de distribución: El modelo de distribución que siguen las \\(n\\) variables.\n\n\n\n¿Qué modelo de distribución se ajusta mejor a nuestro conjunto de datos? Esto se resolverá, en parte, mediante la utilización de técnicas no paramétricas.\nUna vez seleccionado el modelo de distribución más apropiado, ¿qué estadístico del modelo nos interesa y cómo determinar su valor? De esto último se encarga la parte de la inferencia estadística conocida como Estimación de Parámetros.\n\n\n\n\n\nDistribución\nParámetro\n\n\n\n\nBinomial\n\\(n,p\\)\n\n\nPoisson\n\\(\\lambda\\)\n\n\nUniforme\n\\(a,b\\)\n\n\nNormal\n\\(\\mu,\\sigma\\)\n\n\nChi-cuadrado\n\\(n\\)\n\n\nT-Student\n\\(n\\)\n\n\nF-Fisher\n\\(m,n\\)\n\n\n\n\n\nEjemplo 6.1 Sea una población en la que la cuarta parte de las familias no tienen hijos, la mitad de las familias tiene 1 hijo, y el resto tiene 2 hijos.\n\n\n\n\n\n\n\n\nEjemplo 6.2 Si se toma la media muestral \\(\\bar X\\) de las muestras de tamaño 2 del ejemplo anterior, su distribución de probabilidad es\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeorema 6.1 (Teorema central del límite) Si \\(X_1,\\ldots, X_n\\) son variables aleatorias independientes (\\(n\\geq 30\\)) con medias y varianzas \\(\\mu_i=E(X_i)\\), \\(\\sigma^2_i=Var(X_i)\\), \\(i=1,\\ldots,n\\) respectivamente, entonces la variable aleatoria \\(X=X_1+\\cdots+X_n\\) sigue una distribución aproximadamente normal de media la suma de las medias y varianza la suma de las varianzas\n\\[\nX=X_1+\\cdots+X_n\\stackrel{n\\geq 30} \\sim N\\left(\\sum_{i=1}^n \\mu_i, \\sqrt{\\sum_{i=1}^n \\sigma^2_i}\\right)\n\\]\n\n\n\n6.1.1 Distribución de la media muestral para muestras grandes (\\(n\\geq 30\\))\nLa media muestral de una muestra aleatoria de tamaño \\(n\\) es la suma de \\(n\\) variables aleatorias independientes, idénticamente distribuidas:\n\\[\n\\bar X = \\frac{X_1+\\cdots+X_n}{n} = \\frac{X_1}{n}+\\cdots+\\frac{X_n}{n}\n\\]\nDe acuerdo a las propiedades de las transformaciones lineales, la media y la varianza de cada una de estas variables son\n\\[\nE\\left(\\frac{X_i}{n}\\right) =\\frac{\\mu}{n} \\quad  \\mbox{y} \\quad Var\\left(\\frac{X_i}{n}\\right) = \\frac{\\sigma^2}{n^2}\n\\]\ncon \\(\\mu\\) y \\(\\sigma^2\\) la media y la varianza de la población de partida.\nEntonces, si el tamaño de la muestra es grande (\\(n\\geq 30\\)), de acuerdo al teorema central del límite, la distribución de la media muestral será normal:\n\\[\n\\bar X \\sim N\\left(\\sum_{i=1}^n \\frac{\\mu}{n},\\sqrt{\\sum_{i=1}^n \\frac{\\sigma^2}{n^2}} \\right) = N\\left(\\mu,\\frac{\\sigma}{\\sqrt{n}} \\right).\n\\]\n\nEjemplo 6.3 (Ejemplo para muestras grandes (\\(n\\geq 30\\))) Supóngase que se desea estimar el número medio de hijos de una población con media \\(\\mu=2\\) hijos y desviación típica \\(\\sigma=1\\) hijo.\n¿Qué probabilidad hay de estimar \\(\\mu\\) a partir de \\(\\bar x\\) con un error menor de \\(0.2\\)?\n\n\nDe acuerdo al teorema central del límite se tiene:\n\nPara \\(n=30\\), \\(\\bar x\\sim N(2,1/\\sqrt{30})\\) y\n\n\\[\nP(1.8&lt;\\bar x&lt;2.2) = 0.7267.\n\\]\n\nPara \\(n=100\\), \\(\\bar x\\sim N(2,1/\\sqrt{100})\\) y\n\n\\[\nP(1.8&lt;\\bar x&lt;2.2) = 0.9545.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n6.1.2 Distribución de una proporción muestral para muestras grandes (\\(n\\geq 30\\))\nUna proporción \\(p\\) poblacional puede calcularse como la media de una variable dicotómica (0,1). Esta variable se conoce como variable de Bernouilli \\(B(p)\\), que es un caso particular de la binomial para \\(n=1\\). Por tanto, para una muestra aleatoria de tamaño \\(n\\), una proporción muestral \\(\\hat p\\) también puede expresarse como la suma de \\(n\\) variables aleatorias independientes, idénticamente distribuidas:\n\\[\n\\hat p = \\bar X = \\frac{X_1+\\cdots+X_n}{n} = \\frac{X_1}{n}+\\cdots+\\frac{X_n}{n}, \\mbox{ con } X_i\\sim B(p)\n\\]\ny con media y varianza\n\\[\nE\\left(\\frac{X_i}{n}\\right) =\\frac{p}{n} \\quad  \\mbox{y} \\quad Var\\left(\\frac{X_i}{n}\\right) = \\frac{p(1-p)}{n^2}\n\\]\nEntonces, si el tamaño de la muestra es grande (\\(n\\geq 30\\)), de acuerdo al teorema central del límite, la distribución de la proporción muestral también será normal:\n\\[\n\\hat p \\sim N\\left(\\sum_{i=1}^n \\frac{p}{n},\\sqrt{\\sum_{i=1}^n \\frac{p(1-p)}{n^2}} \\right) = N\\left(p,\\sqrt{\\frac{p(1-p)}{n}} \\right).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimación de parámetros poblacionales</span>"
    ]
  },
  {
    "objectID": "08-estimacion.html#estimadores",
    "href": "08-estimacion.html#estimadores",
    "title": "6  Estimación de parámetros poblacionales",
    "section": "6.2 Estimadores",
    "text": "6.2 Estimadores\nLos estadísticos muestrales pueden utilizarse para aproximar los parámetros de la población, y cuando un estadístico se utiliza con este fin se le llama estimador del parámetro.\n\nDefinición 6.2 (Estimador y estimación) Un estimador es una función de la variable aleatoria muestral\n\\[\n\\hat \\theta = F(X_1,\\ldots,X_n).\n\\]\nDada una muestra concreta \\((x_1,\\ldots,x_n)\\), el valor del estimador aplicado a ella se conoce como estimación\n\\[\n\\hat \\theta_0 = F(x_1,\\ldots,x_n).\n\\]\n\nPor ser una función de la variable aleatoria muestral, un estimador es, a su vez, una variable aleatoria cuya distribución depende de la población de partida.\nMientras que el estimador es una función que es única, la estimación no es única, sino que depende de la muestra tomada.\n\n\n\n\n\n\nEjemplo 6.4 Supóngase que se quiere saber la proporción \\(p\\) de fumadores en una ciudad. En ese caso, la variable dicotómica que mide si una persona fuma (1) o no (0), sigue una distribución de Bernouilli \\(B(p)\\).\nSi se toma una muestra aleatoria de tamaño 5, \\((X_1,X_2,X_3,X_4,X_5)\\), de esta población, se puede utilizar la proporción de fumadores en la muestra como estimador para la proporción de fumadores en la población:\n\\[\n\\hat p = \\frac{\\sum_{i=1}^5 X_i}{5}\n\\]\nEste estimador es una variable que se distribuye \\(\\hat p\\sim \\frac{1}{n}B\\left(p,\\sqrt{\\frac{p(1-p)}{n}}\\right)\\).\nSi se toman distintas muestras, se obtienen diferentes estimaciones:\n\\[\n\\begin{array}{|c|c|}\n\\hline\n\\mbox{Muestra} & \\mbox{Estimación}\\\\\n\\hline\\hline\n(1, 0, 0, 1, 1) & 3/5\\\\\n\\hline\n(1, 0, 0, 0, 0) & 1/5\\\\\n\\hline\n(0, 1, 0, 0, 1) & 2/5\\\\\n\\hline\n\\cdots & \\cdots\\\\\n\\hline\n\\end{array}\n\\]\n\nLa estimación de parámetros puede realizar de de dos formas:\n\nEstimación puntual: Se utiliza un único estimador que proporciona un valor o estimación aproximada del parámetro. El principal inconveniente de este tipo de estimación es que no se especifica la bondad de la estimación.\nEstimación por intervalos: Se utilizan dos estimadores que proporcionan los extremos de un intervalo dentro del cual se cree que está el verdadero valor del parámetro con un cierto grado de seguridad. Esta forma de estimar sí permite controlar el error cometido en la estimación.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimación de parámetros poblacionales</span>"
    ]
  },
  {
    "objectID": "08-estimacion.html#estimación-puntual",
    "href": "08-estimacion.html#estimación-puntual",
    "title": "6  Estimación de parámetros poblacionales",
    "section": "6.3 Estimación puntual",
    "text": "6.3 Estimación puntual\nLa estimación puntual utiliza un único estimador para estimar el valor del parámetro desconocido de la población.\nEn teoría pueden utilizarse distintos estimadores para estimar un mismo parámetro. Por ejemplo, en el caso de estimar la proporción de fumadores en una ciudad, podrían haberse utilizado otros posibles estimadores además de la proporción muestral, como pueden ser: \\[\\begin{align*}\n\\hat \\theta_1 &= \\sqrt[5]{X_1X_2X_3X_4X_5}\\\\\n\\hat \\theta_2 &= \\frac{X_1+X_5}{2}\\\\\n\\hat \\theta_3 &= X_1 \\cdots\n\\end{align*}\\]\n¿Cuál es el mejor estimador?\nLa respuesta a esta cuestión depende de las propiedades de cada estimador.\nAunque la estimación puntual no proporciona ninguna medida del grado de bondad de la estimación, existen varias propiedades que garantizan dicha bondad.\nLas propiedades más deseables en un estimador son:\n\nInsesgadez\nEficiencia\nConsistencia\nNormalidad asintótica\nSuficiencia\n\n\nDefinición 6.3 (Estimador insesgado) Un estimador \\(\\hat \\theta\\) es insesgado para un parámetro \\(\\theta\\) si su esperanza es precisamente \\(\\theta\\), es decir,\n\\[\nE(\\hat \\theta)=\\theta.\n\\]\n\n\n\n\nDistribución de estimadores sesgados e insesgados.\n\n\nCuando un estimador no es insesgado, a la diferencia entre su esperanza y el valor del parámetro \\(\\theta\\) se le llama sesgo:\n\\[\nSesgo(\\hat \\theta) = E(\\hat \\theta)-\\theta.\n\\]\nCuanto menor sea el sesgo de un estimador, mejor se aproximarán sus estimaciones al verdadero valor del parámetro.\n\nDefinición 6.4 (Estimador consistente) Un estimador \\(\\hat \\theta_n\\) para muestras de tamaño \\(n\\) es consistente para un parámetro \\(\\theta\\) si para cualquier valor \\(\\epsilon&gt;0\\) se cumple\n\\[\n\\lim_{n\\rightarrow \\infty} P(|\\hat \\theta_n-\\theta|&lt;\\epsilon)=1.\n\\]\n\n\n\n\n\n\n\n\n\n\nDistribución de estimadores consistentes.\n\n\n\n\n\n\n\nDistribución de estimadores consistentes segados.\n\n\n\n\n\nLas condiciones suficientes para que un estimador sea consistente son:\n\n\\(Sesgo(\\hat \\theta_n)=0\\) o \\(\\lim_{n\\rightarrow \\infty}Sesgo(\\hat \\theta_n)=0\\).\n\\(\\lim_{n\\rightarrow \\infty}Var(\\hat \\theta_n)=0\\).\n\nAsí pues, si la varianza y el sesgo disminuyen a medida que aumenta el tamaño de la muestra, el estimador será consistente.\n\nDefinición 6.5 (Estimador eficiente) Un estimador \\(\\hat \\theta\\) de un parámetro \\(\\theta\\) es eficiente si tiene el menor error cuadrático medio\n\\[\nECM(\\hat \\theta) = Sesgo(\\hat \\theta)^2+Var(\\theta).\n\\]\n\n\n\n\n\nDistribución de estimadores insesgados y eficientes sesgados.\n\n\n\nDefinición 6.6 (Estimador asintóticamente normal) Un estimador \\(\\hat \\theta\\) es asintóticamente normal si, independientemente de la distribución de la variable aleatoria muestral, su distribución es normal si el tamaño de la muestra es suficientemente grande.:::\n\nComo veremos más adelante esta propiedad es muy interesante para hacer estimaciones de parámetros mediante intervalos.\n\n\n\nDistribución de estimadores asintóticamente normales.\n\n\n\nDefinición 6.7 (Estimador suficiente) Un estimador \\(\\hat \\theta\\) es suficiente para un parámetro \\(\\theta\\), si la distribución condicionada de la variable aleatoria muestral, una vez dada la estimación \\(\\hat \\theta = \\hat \\theta_0\\), no depende de \\(\\theta\\).\n\nEsto significa que cuando se obtiene una estimación, cualquier otra información es irrelevante para \\(\\theta\\).\nEl estimador que se suele utilizar para estimar la media poblacional es la media muestral.\nPara muestras de tamaño \\(n\\) resulta la siguiente variable aleatoria:\n\\[\n\\bar X = \\frac{X_1+\\cdots+X_n}{n}\n\\]\nSi la población de partida tiene media \\(\\mu\\) y varianza \\(\\sigma^2\\) se cumple\n\\[\nE(\\bar X) = \\mu \\quad \\mbox{y} \\quad Var(\\bar X)=\\frac{\\sigma^2}{n}\n\\]\nAsí pues, la media muestral es un estimador insesgado, y como su varianza disminuye a medida que aumenta el tamaño muestral, también es consistente y eficiente.\nSin embargo, la varianza muestral\n\\[\nS^2 = \\frac{\\sum_{i=1}^n (X_i-\\bar X)^2}{n}\n\\]\nes un estimador sesgado para la varianza poblacional, ya que\n\\[\nE(S^2)= \\frac{n-1}{n}\\sigma^2.\n\\]\nNo obstante, resulta sencillo corregir este sesgo para llegar a un estimador insesgado:\n\nDefinición 6.8 (Cuasivarianza muestral) Dada una muestra de tamaño \\(n\\) de una variable aleatoria \\(X\\), se define la cuasivarianza muestral como\n\\[\n\\hat{S}^2 = \\frac{\\sum_{i=1}^n (X_i-\\bar X)^2}{n-1} = \\frac{n}{n-1}S^2.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimación de parámetros poblacionales</span>"
    ]
  },
  {
    "objectID": "08-estimacion.html#estimación-por-intervalos",
    "href": "08-estimacion.html#estimación-por-intervalos",
    "title": "6  Estimación de parámetros poblacionales",
    "section": "6.4 Estimación por intervalos",
    "text": "6.4 Estimación por intervalos\nEl principal problema de la estimación puntual es que, una vez seleccionada la muestra y hecha la estimación, resulta imposible saber el error cometido.\n\n\n\n\n\nPara controlar el error de la estimación es mejor utilizar la estimación por intervalos\n\n\n\n\n\nLa estimación por intervalos trata de construir a partir de la muestra un intervalo dentro del cual se supone que se encuentra el parámetro a estimar con un cierto grado de confianza. Para ello se utilizan dos estimadores, uno para el límite inferior del intervalo y otro para el superior.\n\nDefinición 6.9 (Intervalo de confianza) Dados dos estimadores \\(\\hat l_i(X_1,\\ldots,X_n)\\) y \\(\\hat l_s(X_1,\\ldots,X_n)\\), y sus respectivas estimaciones \\(l_1\\) y \\(l_2\\) para una muestra concreta, se dice que el intervalo \\(I=[l_1,l_2]\\) es un intervalo de confianza para un parámetro poblacional \\(\\theta\\), con un nivel de confianza \\(1-\\alpha\\) (o nivel de significación \\(\\alpha\\)), si se cumple\n\\[\nP(\\hat l_i(X_1,\\ldots,X_n)\\leq \\theta \\leq \\hat l_s(X_1,\\ldots,X_n))= 1-\\alpha.\n\\]\n\nUn intervalo de confianza nunca garantiza con absoluta certeza que el parámetro se encuentra dentro él.\nTampoco se puede decir que la probabilidad de que el parámetro esté dentro del intervalo es \\(1-\\alpha\\), ya que una vez calculado el intervalo, las variables aleatorias que determinan sus extremos han tomado un valor concreto y ya no tiene sentido hablar de probabilidad, es decir, o el parámetro está dentro, o está fuera, pero con absoluta certeza.\nLo que si se deduce de la definición es que el \\((1-\\alpha)\\%\\) de los intervalos correspondientes a las todas las posibles muestras aleatorias, contendrán al parámetro. Es por eso que se habla de confianza y no de probabilidad.\nPara que un intervalo sea útil su nivel de confianza debe ser alto:\n\\[\\begin{align*}\n1-\\alpha &= 0.90 \\mbox{ o } \\alpha=0.10\\\\\n1-\\alpha &= {0.95} \\mbox{ o } \\alpha=0.05\\\\\n1-\\alpha &= 0.99 \\mbox{ o } \\alpha=0.01\\\\\n\\end{align*}\\]\nsiendo \\(0.95\\) el nivel de confianza más habitual y \\(0.99\\) en casos críticos.\nTeóricamente, de cada 100 intervalos para estimar un parámetro \\(\\theta\\) con nivel de confianza \\(1-\\alpha=0.95\\), 95 contendrían a \\(\\theta\\) y sólo 5 lo dejarían fuera.\n\n\n\n\n\n\n\n\n\n\n6.4.1 Error de estimación\nOtro de los aspectos más importantes de un intervalo de confianza es su error.\n\nDefinición 6.10 (Error o imprecisión de un intervalo) El error o la imprecisión de un intervalo de confianza \\([l_i,l_s]\\) es su amplitud\n\\[\nA=l_s-l_i.\n\\]\n\n\n\n\n\n\nPara que un intervalo sea útil no debe ser demasiado impreciso.\nEn general, la precisión de un intervalo depende de tres factores:\n\nLa dispersión de la población. Cuanto más dispersa sea, menos preciso será el intervalo.\nEl nivel de confianza. Cuanto mayor sea el nivel de confianza, menos preciso será el intervalo.\nEl tamaño muestral. Cuanto mayor sea el tamaño muestral, más preciso será el intervalo.\n\n\nSi la confianza y la precisión están reñidas, ¿cómo se puede ganar precisión sin perder confianza?\n\nHabitualmente, para calcular un intervalo de confianza se suele partir de un estimador puntual del que se conoce su distribución muestral.\nA partir de este estimador se calculan los extremos del intervalo sobre su distribución, buscando los valores que dejan encerrada una probabilidad \\(1-\\alpha\\). Estos valores suelen tomarse de manera simétrica, de manera que el extremo inferior deje una probabilidad acumulada inferior \\(\\alpha/2\\) y el extremo superior deje una probabilidad acumulada superior también de \\(\\alpha/2\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimación de parámetros poblacionales</span>"
    ]
  },
  {
    "objectID": "08-estimacion.html#intervalos-de-confianza-para-una-población",
    "href": "08-estimacion.html#intervalos-de-confianza-para-una-población",
    "title": "6  Estimación de parámetros poblacionales",
    "section": "6.5 Intervalos de confianza para una población",
    "text": "6.5 Intervalos de confianza para una población\nA continuación se presentan los intervalos de confianza para estimar un parámetro de una poblacion:\n\nIntervalo para la media de una población normal con varianza conocida.\nIntervalo para la media de una población normal con varianza desconocida.\nIntervalo para la media de una población con varianza desconocida a partir de muestras grandes.\nIntervalo para la varianza de una población normal.\nIntervalo para un proporción de una población.\n\n\n6.5.1 Intervalo de confianza para la media de una población normal con varianza conocida\nSea \\(X\\) una variable aleatoria que cumple las siguientes hipótesis:\n\nSu distribución es normal \\(X\\sim N(\\mu,\\sigma)\\).\nLa media \\(\\mu\\) es desconocida, pero su varianza \\(\\sigma^2\\) es conocida.\n\nBajo estas hipótesis, la media muestral, para muestras de tamaño \\(n\\), sigue también una distribución normal\n\\[\n\\bar X \\sim N\\left(\\mu,\\frac{\\sigma}{\\sqrt n}\\right)\n\\]\nTipificando la variable se tiene\n\\[\nZ=\\frac{\\bar X-\\mu}{\\sigma/\\sqrt n} \\sim N(0,1)\n\\]\nSobre esta distribución resulta sencillo calcular los valores \\(z_i\\) y \\(z_s\\) de manera que\n\\[\nP(z_i\\leq Z \\leq z_s) = 1-\\alpha.\n\\]\nComo la distribución normal estándar es simétrica respecto al 0, lo mejor es tomar valores opuestos \\(-z_{\\alpha/2}\\) y \\(z_{\\alpha/2}\\) que dejen sendas colas de probabilidad acumulada \\(\\alpha/2\\).\n\n\n\n\n\nA partir de aquí, deshaciendo la tipificación, resulta sencillo llegar a los estimadores que darán los extremos del intervalo de confianza:\n\\[\\begin{align*}\n1-\\alpha &= P(-z_{\\alpha/2}\\leq Z \\leq z_{\\alpha/2}) = P\\left(-z_{\\alpha/2}\\leq \\frac{\\bar X -\\mu}{\\sigma/\\sqrt{n}} \\leq z_{\\alpha/2}\\right) =\\\\\n&= P\\left(-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\leq \\bar X -\\mu \\leq z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)=\\\\\n&= P\\left(-\\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\leq -\\mu \\leq -\\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)= \\\\\n&= P\\left(\\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right).\n\\end{align*}\\]\nAsí pues, el intervalo de confianza para la media de una población normal con varianza conocida es:\n\nTeorema 6.2 (Intervalo de confianza para la media de una población normal con varianza conocida) Si \\(X\\sim N(\\mu, \\sigma)\\) con \\(\\sigma\\) conocida, el intervalo de confianza para la media \\(\\mu\\) con nivel de confianza \\(1-\\alpha\\) es\n\\[\n\\left[\\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}},\\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right]\n\\] o bien \\[\n\\bar{X}\\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\]\n\nDe la fórmula del intervalo de confianza\n\\[\n\\bar{X}\\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\]\nse deducen varias características:\n\nEl intervalo está centrado en la media muestral \\(\\bar X\\) que era el mejor estimador de la media poblacional.\nLa amplitud o imprecisión del intervalo es\n\n\\[\nA= 2 z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\]\nde manera que depende de:\n\n\\(\\sigma\\): cuanto mayor sea la varianza poblacional, mayor será la imprecisión.\n\\(z_{\\alpha/2}\\): que a su vez depende del nivel de confianza, y cuanto mayor sea \\(1-\\alpha\\), mayor será la imprecisión.\n\\(n\\): cuanto mayor sea el tamaño de la muestra, menor será la imprecisión.\n\nPor tanto, la única forma de reducir la imprecisión del intervalo, manteniendo la confianza, es aumentando el tamaño muestral.\n\n6.5.1.1 Cálculo del tamaño muestra para estimar la media de una población normal con varianza conocida\nTeniendo en cuenta que la amplitud o imprecisión del intervalo para la media de una población normal con varianza conocida es\n\\[\nA= 2 z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\]\nse puede calcular fácilmente el tamaño muestral necesario para conseguir un intervalo de amplitud \\(A\\) con confianza \\(1-\\alpha\\):\n\\[\nA= 2 z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\Leftrightarrow \\sqrt{n}= 2 z_{\\alpha/2}\\frac{\\sigma}{A},\n\\]\nde donde se deduce\n\\[\n{n = 4 z_{\\alpha/2}^2\\frac{\\sigma^2}{A^2}}\n\\]\n\nEjemplo 6.5 Sea una población de estudiantes en la que la puntuación obtenida en un examen sigue una distribución normal \\(X\\sim N(\\mu,\\sigma=1.5)\\).\nPara estimar la nota media \\(\\mu\\), se toma una muestra de 10 estudiantes:\n\\[\n4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3\n\\]\nA partir de esta muestra, podemos calcular el intervalo de confianza para \\(\\mu\\) con un nivel de confianza \\(1-\\alpha=0.95\\) (nivel de significación \\(\\alpha=0.05\\)):\n\n\\(\\bar X = \\frac{4+\\cdots+3}{10}= \\frac{53}{10} = 5.3\\) puntos.\n\\(z_{\\alpha/2}=z_{0.025}\\) es el valor de la normal estándar que deja una probabilidad acumulada superior de \\(0.025\\), que vale aproximadamente \\(1.96\\).\n\nSustituyendo estos valores en la fórmula del intervalo, se tiene\n\\[\n\\bar{X}\\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} = 5.3\\pm 1.96\\frac{1.5}{\\sqrt{10}} = 5.3\\pm 0.93 = \\left[4.37,\\,6.23\\right].\n\\]\nEs decir, \\(\\mu\\) estaría entre \\(4.37\\) y \\(6.23\\) puntos con un 95% de confianza.\n\n\nEjemplo 6.6 La imprecisión del intervalo anterior es de \\(\\pm 0.93\\) puntos.\nSi se desea reducir esta imprecisión a \\(\\pm 0.5\\) puntos, ¿qué tamaño muestral sería necesario?\n\\[\nn = 4 z_{\\alpha/2}^2\\frac{\\sigma^2}{A^2} = 4\\cdot 1.96^2\\frac{1.5^2}{(2\\cdot 0.5)^2} = 34.57.\n\\]\nPor tanto, se necesitaría una muestra de al menos 35 estudiantes para conseguir un intervalo del 95% de confianza y una precisión de \\(\\pm\n0.5\\) puntos.\n\n\n\n\n6.5.2 Intervalo de confianza para la media de una población normal con varianza desconocida\nSea \\(X\\) una variable aleatoria que cumple las siguientes hipótesis:\n\nSu distribución es normal \\(X\\sim N(\\mu,\\sigma)\\).\nTanto su media \\(\\mu\\) como su varianza \\(\\sigma^2\\) son desconocidas.\n\nCuando se desconoce la varianza poblacional se suele estimar mediante la cuasivarianza \\(\\hat{S}^2\\). Como consecuencia, el estimador de referencia ya no sigue una distribución normal como en el caso de conocer la varianza, sino un T de Student de \\(n-1\\) grados de libertad:\n\\[\n\\left.\n\\begin{array}{l}\n\\bar X \\sim N\\left(\\mu,\\frac{\\sigma}{\\sqrt{n}}\\right)\\\\\n\\displaystyle\\frac{(n-1)\\hat{S}^2}{\\sigma^2}\\sim \\chi^2(n-1)\n\\end{array}\n\\right\\}\n\\Rightarrow\n\\frac{\\bar X -\\mu}{\\hat{S}/\\sqrt{n}}\\sim T(n-1),\n\\]\nComo la distribución T de Student, al igual que la normal, también es simétrica respecto al 0, se pueden tomar dos valores opuestos \\(-t^{n-1}_{\\alpha/2}\\) y \\(t^{n-1}_{\\alpha/2}\\) de manera que\n\\[\\begin{align*}\n1-\\alpha\n&= P\\left(-t^{n-1}_{\\alpha/2}\\leq \\frac{\\bar X -\\mu}{\\hat{S}/\\sqrt{n}} \\leq t^{n-1}_{\\alpha/2}\\right)\\\\\n&= P\\left(-t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}}\\leq \\bar X -\\mu \\leq t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}}\\right)\\\\\n&= P\\left(\\bar X-t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}}\\leq \\mu \\leq \\bar X t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}}\\right)\n\\end{align*}\\]\n\nTeorema 6.3 (Intervalo de confianza para la media de una población normal con varianza desconocida) Si \\(X\\sim N(\\mu, \\sigma)\\) con \\(\\sigma\\) desconocida, el intervalo de confianza para la media \\(\\mu\\) con nivel de confianza \\(1-\\alpha\\) es\n\\[\n\\left[\\bar{X}-t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}},\\bar{X}+t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}}\\right]\n\\]\no bien\n\\[\n\\bar{X}\\pm t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}}\n\\]\n\n\n6.5.2.1 Calculo del tamaño muestral para estimar la media de una población normal con varianza desconocida\nAl igual que antes, teniendo en cuenta que la amplitud o imprecisión del intervalo para la media de una población con varianza desconocida es\n\\[\nA= 2 t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}}\n\\]\nse puede calcular fácilmente el tamaño muestral necesario para conseguir un intervalo de amplitud \\(A\\) con confianza \\(1-\\alpha\\):\n\\[\nA= 2 t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}} \\Leftrightarrow \\sqrt{n}= 2 t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{A},\n\\]\nde donde se deduce\n\\[\n{n = 4 (t^{n-1}_{\\alpha/2})^2\\frac{\\hat{S}^2}{A^2}}\n\\]\nEl único problema, a diferencia del caso anterior en que \\(\\sigma\\) era conocida, es que se necesita \\(\\hat{S}\\), por lo que se suele tomar una muestra pequeña previa para calcularla. Por otro lado, el valor de la T de student suele aproximarse asintóticamente por el de la normal estándar \\(t^{n-1}_{\\alpha/2}\\approx z_{\\alpha/2}\\).\n\nEjemplo 6.7 Supóngase que en el ejemplo anterior no se conoce la varianza poblacional de las puntuaciones.\nTrabajando con la misma muestra de las puntuaciones de 10 estudiantes\n\\[\n4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3\n\\]\nse puede calcular el intervalo de confianza para \\(\\mu\\) con un nivel de confianza \\(1-\\alpha=0.95\\) (nivel de significación \\(\\alpha=0.05\\)):\n\n\\(\\bar X = \\frac{4+\\cdots+3}{10}= \\frac{53}{10} = 5.3\\) puntos.\n\\(\\hat{S}^2= \\frac{(4-5.3)^2+\\cdots+(3-5.3)^2}{9} = 3.5667\\) y \\(\\hat{S}=\\sqrt{3.5667}=1.8886\\) puntos.\n\\(t^{n-1}_{\\alpha/2}=t^9_{0.025}\\) es el valor de la T de Student de 9 grados de libertad, que deja una probabilidad acumulada superior de \\(0.025\\), que vale \\(2.2622\\).\n\nSustituyendo estos valores en la fórmula del intervalo, se tiene\n\\[\n\\bar{X}\\pm t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}} = 5.3\\pm 2.2622\\frac{1.8886}{\\sqrt{10}} = 5.3\\pm 1.351 = \\left[3.949,\\,6.651\\right].\n\\]\n\n\nEjemplo 6.8 Como se puede apreciar, la imprecisión del intervalo anterior es de \\(\\pm 1.8886\\) puntos, que es significativamente mayor que en el caso de conocer la varianza de la población. Esto es lógico pues al tener que estimar la varianza de la población, el error de la estimación se agrega al error del intervalo.\nAhora, el tamaño muestral necesario para reducir la imprecisión a \\(\\pm 0.5\\) puntos es\n\\[\nn = 4 (z_{\\alpha/2})^2\\frac{\\hat{S}^2}{A^2} = 4\\cdot 1.96^2\\frac{3.5667}{(2\\cdot 0.5)^2} = 54.81.\n\\]\nPor tanto, si se desconoce la varianza de la población se necesita una muestra de al menos 55 estudiantes para conseguir un intervalo del 95% de confianza y una precisión de \\(\\pm 0.5\\) puntos.\n\n\n\n\n6.5.3 Intervalo de confianza para la media de una población no normal\nSea \\(X\\) una variable aleatoria que cumple las siguientes hipótesis:\n\nSu distribución no es normal.\nTanto su media \\(\\mu\\) como su varianza \\(\\sigma^2\\) son desconocidas.\n\nSi la población no es normal las distribuciones de los estimadores de referencia cambian, de manera que los intervalos anteriores no son válidos.\nNo obstante, si la muestras es grande (\\(n\\geq 30\\)), de acuerdo al teorema central del límite, la distribución de la media muestral se aproximará a una normal, de modo que sigue siendo cierto\n\\[\n\\bar X \\sim N\\left(\\mu,\\frac{\\sigma}{\\sqrt{n}}\\right)\n\\]\nEn consecuencia, sigue siendo válido el intervalo anterior.\n\nTeorema 6.4 (Intervalo de confianza para la media de una población no normal con muestras grandes) Si \\(X\\) es una variable con distribución no normal y \\(n\\geq 30\\), el intervalo de confianza para la media \\(\\mu\\) con nivel de confianza \\(1-\\alpha\\) es\n\\[\n\\bar{X}\\pm t^{n-1}_{\\alpha/2}\\frac{\\hat{S}}{\\sqrt{n}}\n\\]\n\n\n\n6.5.4 Intervalo de confianza para la varianza de una población normal\nSea \\(X\\) una variable aleatoria que cumple las siguientes hipótesis:\n\nSu distribución es normal \\(X\\sim N(\\mu,\\sigma)\\).\nTanto su media \\(\\mu\\) como su varianza \\(\\sigma^2\\) son desconocidas.\n\nPara estimar la varianza de una población normal, se parte del estimador de referencia\n\\[\n\\frac{nS^2}{\\sigma^2} = \\frac{(n-1)\\hat{S}^2}{\\sigma^2}\\sim \\chi^2(n-1),\n\\]\nque sigue una distribución chi-cuadrado de \\(n-1\\) grados de libertad.\nSobre esta distribución hay que calcular los valores \\(\\chi_i\\) y \\(\\chi_s\\) tales que\n\\[\nP(\\chi_i\\leq \\chi^2(n-1) \\leq \\chi_s) = 1-\\alpha.\n\\]\nComo la distribución chi-cuadrado no es simétrica respecto al 0, se toman dos valores \\(\\chi^{n-1}_{\\alpha/2}\\) y \\(\\chi^{n-1}_{1-\\alpha/2}\\) que dejen sendas colas de probabilidad acumulada inferior de \\(\\alpha/2\\) y \\(1-\\alpha/2\\) respectivamente.\n\n\n\n\n\nAsí pues, se tiene\n\\[\\begin{align*}\n1-\\alpha &= P\\left(\\chi^{n-1}_{\\alpha/2}\\leq \\frac{nS^2}{\\sigma^2}  \\leq \\chi^{n-1}_{1-\\alpha/2}\\right) =\nP\\left(\\frac{1}{\\chi^{n-1}_{\\alpha/2}}\\geq \\frac{\\sigma^2}{nS^2}  \\geq \\frac{1}{\\chi^{n-1}_{1-\\alpha/2}}\\right)=\\\\\n&= P\\left(\\frac{1}{\\chi^{n-1}_{1-\\alpha/2}}\\leq \\frac{\\sigma^2}{nS^2}  \\leq \\frac{1}{\\chi^{n-1}_{\\alpha/2}}\\right)\n= P\\left(\\frac{nS^2}{\\chi^{n-1}_{1-\\alpha/2}}\\leq \\sigma^2  \\leq \\frac{nS^2}{\\chi^{n-1}_{\\alpha/2}}\\right).\n\\end{align*}\\]\nPor tanto, el intervalo de confianza para la varianza de una población normal es:\n\nTeorema 6.5 (Intervalo de confianza para la varianza de una población normal) Si \\(X\\sim N(\\mu, \\sigma)\\) con \\(\\sigma\\) conocida, el intervalo de confianza para la varianza \\(\\sigma^2\\) con nivel de confianza \\(1-\\alpha\\) es\n\\[\n\\left[\\frac{nS^2}{\\chi^{n-1}_{1-\\alpha/2}},\\frac{nS^2}{\\chi^{n-1}_{\\alpha/2}}\\right]\n\\]\n\n\nEjemplo 6.9 Siguiendo con el ejemplo de las puntuaciones en un examen, si se quiere estimar la varianza a partir de la muestra:\n\\[\n4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3\n\\]\npara el intervalo de confianza para \\(\\sigma^2\\) con un nivel de confianza \\(1-\\alpha=0.95\\) (nivel de significación \\(\\alpha=0.05\\)) se tiene:\n\n\\(S^2= \\frac{(4-5.3)^2+\\cdots+(3-5.3)^2}{10} = 3.21\\) puntos\\(^2\\).\n\\(\\chi^{n-1}_{\\alpha/2}=\\chi^9_{0.025}\\) es el valor de la chi-cuadrado de 9 grados de libertad, que deja una probabilidad acumulada inferior de \\(0.025\\), y vale \\(2.7\\).\n\\(\\chi^{n-1}_{1-\\alpha/2}=\\chi^9_{0.975}\\) es el valor de la chi-cuadrado de 9 grados de libertad, que deja una probabilidad acumulada inferior de \\(0.975\\), y vale \\(19\\).\n\nSustituyendo estos valores en la fórmula del intervalo, se llega a\n\\[\n\\left[\\frac{nS^2}{\\chi^{n-1}_{1-\\alpha/2}},\\frac{nS^2}{\\chi^{n-1}_{\\alpha/2}}\\right] =\n\\left[\\frac{10\\cdot 3.21}{19},\\frac{10\\cdot 3.21}{2.7}\\right] = [1.69,\\,11.89] \\text{ puntos}^2.\n\\]\n\n\n\n6.5.5 Intervalo de confianza para una proporción\nPara estimar la proporción \\(p\\) de individuos de una población que presentan una determinada característica, se parte de la variable que mide el número de individuos que la presentan en una muestra de tamaño \\(n\\). Dicha variable sigue una distribución binomial\n\\[\nX\\sim B(n,p)\n\\]\nComo ya se vio, si el tamaño muestral es suficientemente grande (en realidad basta que se cumpla \\(np\\geq 5\\) y \\(n(1-p)\\geq 5\\)), el teorema central de límite asegura que \\(X\\) tendrá una distribución aproximadamente normal\n\\[\nX\\sim N(np,\\sqrt{np(1-p)}).\n\\]\nEn consecuencia, la proporción muestral \\(\\hat p\\) también será normal\n\\[\n\\hat{p}=\\frac{X}{n} \\sim N\\left(p,\\sqrt{\\frac{p(1-p)}{n}}\\right),\n\\]\nque es el estimador de referencia.\nTrabajando con la distribución del estimador de referencia\n\\[\n\\hat p\\sim N\\left(p,\\sqrt{\\frac{p(1-p)}{n}}\\right)\n\\]\ntras tipificar, se pueden encontrar fácilmente, al igual que hicimos antes, valores \\(-z_{\\alpha/2}\\) y \\(z_{\\alpha/2}\\) que cumplan\n\\[\nP\\left(-z_{\\alpha/2}\\leq \\frac{\\hat p-p}{\\sqrt{p(1-p)/n}}\\leq z_{\\alpha/2} \\right) = 1-\\alpha.\n\\]\nAsí pues, deshaciendo la tipificación y razonando como antes, se tiene\n\\[\\begin{align*}\n1-\\alpha\n&= P\\left(-z_{\\alpha/2}\\leq \\frac{\\hat p-p}{\\sqrt{p(1-p)/n}}\\leq z_{\\alpha/2} \\right) \\\\\n&= P\\left(-z_{\\alpha/2}\\frac{\\sqrt{p(1-p)}}{n}\\leq \\hat p-p\\leq z_{\\alpha/2}\\frac{\\sqrt{p(1-p)}}{n} \\right) \\\\\n&= P\\left(\\hat{p}-z_{\\alpha/2}\\frac{\\sqrt{p(1-p)}}{n}\\leq p\\leq \\hat{p}+z_{\\alpha/2}\\frac{\\sqrt{p(1-p)}}{n} \\right)\n\\end{align*}\\]\nPor tanto, el intervalo de confianza para una proporción es\n\nTeorema 6.6 (Intervalo de confianza para una proporción) Si \\(X\\sim B(n,p)\\), y se cumple que \\(np\\geq 5\\) y \\(n(1-p)\\geq 5\\), entonces el intervalo de confianza para la proporción \\(p\\) con nivel de confianza \\(1-\\alpha\\) es\n\\[\n\\left[\\hat{p}-z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}},\\hat{p}+z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right]\n\\]\no bien\n\\[\n\\hat{p}\\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\n\n\n6.5.5.1 Cálculo del tamaño muestra para estimar una proporción\nLa amplitud o imprecisión del intervalo para la proporción de una población es\n\\[\nA= 2 z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nasí que se puede calcular fácilmente el tamaño muestral necesario para conseguir un intervalo de amplitud \\(A\\) con confianza \\(1-\\alpha\\):\n\\[\nA= 2 z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\Leftrightarrow A^2= 4 z_{\\alpha/2}^2\\frac{\\hat{p}(1-\\hat{p})}{n},\n\\]\nde donde se deduce\n\\[\n{n= 4 z_{\\alpha/2}^2\\frac{\\hat{p}(1-\\hat{p})}{A^2}}\n\\]\nPara poder hacer el cálculo se necesita una estimación de la proporción \\(\\hat{p}\\), por lo que suele tomarse una muestra previa pequeña para calcularla. En el peor de los casos, si no se dispone de una muestra previa, puede tomarse \\(\\hat{p}=0.5\\).\n\nEjemplo 6.10 Supóngase que se quiere estimar la proporción de fumadores que hay en una determinada población. Para ello se toma una muestra de 20 personas y se observa si fuman (1) o no (0):\n\\[\n0 - 1 - 1 - 0 - 0 - 0 - 1 - 0 - 0 - 1 - 0 - 0 - 0 - 1 - 1- 0 - 1 - 1 - 0 - 0\n\\]\nEntonces:\n\n\\(\\hat p=\\frac{8}{20}=0.4\\), por tanto, se cumple \\(np=20\\cdot 0.4 = 8\\geq 5\\) y \\(n(1-p)=20\\cdot 0.6= 12\\geq 5\\).\n\\(z_{\\alpha/2}=z_{0.025}\\) es el valor de la normal estándar que deja una probabilidad acumulada superior de \\(0.025\\), que vale aproximadamente \\(1.96\\).\n\nSustituyendo estos valores en la fórmula del intervalo, se tiene\n\\[\n\\hat{p}\\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} = 0.4\\pm 1.96\\sqrt{\\frac{0.4\\cdot 0.6}{10}} = 0.4\\pm  0.3 = \\left[0.1,\\,0.7\\right].\n\\]\nEs decir, \\(p\\) estaría entre \\(0.1\\) y \\(0.7\\) con un 95% de confianza.\n\n\nEjemplo 6.11 Como se puede apreciar la imprecisión del intervalo anterior es \\(\\pm 0.3\\), que es enorme teniendo en cuenta que se trata de un intervalo para una proporción.\nPara conseguir intervalos precisos para estimar proporciones se necesitan tamaños muestrales bastante grandes. Si por ejemplo se quiere una precisión de \\(\\pm 0.05\\), el tamaño muestral necesario sería:\n\\[\nn= 4 z_{\\alpha/2}^2\\frac{\\hat{p}(1-\\hat{p})}{A^2}=4\\cdot 1.96^2\\frac{0.4\\cdot 0.6}{(2\\cdot0.05)^2}= 368.79.\n\\]\nEs decir, se necesitarían al menos 369 individuos para conseguir un intervalo para la proporción con una confianza del \\(95\\%\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimación de parámetros poblacionales</span>"
    ]
  },
  {
    "objectID": "08-estimacion.html#intervalos-de-confianza-para-la-comparación-dos-poblaciones",
    "href": "08-estimacion.html#intervalos-de-confianza-para-la-comparación-dos-poblaciones",
    "title": "6  Estimación de parámetros poblacionales",
    "section": "6.6 Intervalos de confianza para la comparación dos poblaciones",
    "text": "6.6 Intervalos de confianza para la comparación dos poblaciones\nEn muchos estudios el objetivo en sí no es averiguar el valor de un parámetro, sino compararlo con el de otra población. Por ejemplo, comparar si un determinado parámetro vale lo mismo en la población de hombres y en la de mujeres.\nEn estos casos no interesa realmente estimar los dos parámetros por separado, sino hacer una estimación que permita su comparación.\nSe verán tres casos:\n\nComparación de medias: Se estima la diferencia de medias \\(\\mu_1-\\mu_2\\).\nComparación de varianzas: Se estima la razón de varianzas \\(\\displaystyle \\frac{\\sigma^2_1}{\\sigma^2_2}\\).\nComparación de proporciones: Se estima la diferencia de proporciones \\(\\hat p_1-\\hat p_2\\).\n\nA continuación se presentan los siguientes intervalos de confianza para la comparación de dos poblaciones:\n\nIntervalo para la diferencia de medias de dos poblaciones normales con varianzas conocidas.\nIntervalo para la diferencia de medias de dos poblaciones normales con varianzas desconocidas pero iguales.\nIntervalo para la diferencia de medias de dos poblaciones normales con varianzas desconocidas y diferentes.\nIntervalo para el cociente de varianzas de dos poblaciones normales.\nIntervalo para la diferencia de proporciones de dos poblaciones.\n\n\n6.6.1 Intervalo de confianza para la diferencia de medias de poblaciones normales con varianzas conocidas\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias que cumplen las siguientes hipótesis:\n\nSu distribución es normal \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\).\nSus medias \\(\\mu_1\\) y \\(\\mu_2\\) son desconocidas, pero sus varianzas \\(\\sigma^2_1\\) y \\(\\sigma^2_2\\) son conocidas.\n\nBajo estas hipótesis, si se toman dos muestras independientes, una de cada población, de tamaños \\(n_1\\) y \\(n_2\\) respectivamente, la diferencia de las medias muestrales sigue una distribución normal\n\\[\n\\left.\n\\begin{array}{l}\n\\bar{X}_1\\sim N\\left(\\mu_1,\\frac{\\sigma_1}{\\sqrt{n_1}} \\right)\\\\\n\\bar{X}_2\\sim N\\left(\\mu_2,\\frac{\\sigma_2}{\\sqrt{n_2}} \\right)\n\\end{array}\n\\right\\}\n\\Rightarrow\n\\bar{X}_1-\\bar{X}_2 \\sim N\\left(\\mu_1-\\mu_2,\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\right).\n\\]\nA partir de aquí, tipificando, se pueden buscar los valores de la normal estándar \\(-z_{\\alpha/2}\\) y \\(z_{\\alpha/2}\\) que cumplen:\n\\[\nP\\left(-z_{\\alpha/2}\\leq \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}} \\leq z_{\\alpha/2}\\right) = 1-\\alpha.\n\\]\nY deshaciendo la tipificación, se tiene\n\\[\\begin{align*}\n1-\\alpha\n&= P\\left(-z_{\\alpha/2}\\leq \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}} \\leq z_{\\alpha/2}\\right) \\\\\n&= P\\left(-z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\leq (\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)\\leq z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\right) \\\\\n&= P\\left(\\bar{X}_1-\\bar{X}_2 - z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\leq \\mu_1-\\mu_2\\leq \\bar{X}_1-\\bar{X}_2 + z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\right)\n\\end{align*}\\]\nAsí pues, el intervalo de confianza para la diferencia de medias es\n\nTeorema 6.7 (Intervalo de confianza para la diferencia de medias de poblaciones normales con varianzas conocidas) Si \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\), con \\(\\sigma_1\\) y \\(\\sigma_2\\) conocidas, el intervalo de confianza para la diferencia de medias \\(\\mu_1-\\mu_2\\) con nivel de confianza \\(1-\\alpha\\) es\n\\[\n\\left[\\bar{X}_1-\\bar{X}_2-z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}},\\bar{X}_1-\\bar{X}_2+z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\right]\n\\]\no bien\n\\[\n\\bar{X}_1-\\bar{X}_2\\pm z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\n\\]\n\n\n\n6.6.2 Intervalo de confianza para la diferencia de medias de dos poblaciones normales con varianzas desconocidas e iguales\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias que cumplen las siguientes hipótesis:\n\nSu distribución es normal \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\).\nSus medias \\(\\mu_1\\) y \\(\\mu_2\\) son desconocidas y sus varianzas también, pero son iguales \\(\\sigma^2_1=\\sigma^2_2=\\sigma^2\\).\n\nCuando se desconoce la varianza poblacional se puede estimar a partir de las muestras de tamaños \\(n_1\\) y \\(n_2\\) de ambas poblaciones mediante la cuasivarianza ponderada:\n\\[\n\\hat{S}^2_p = \\frac{n_1S^2_1+n_2S^2_2}{n_1+n_2-2}.\n\\]\nEl estimador de referencia en este caso sigue una distribución T de Student:\n\\[\n\\left.\n\\begin{array}{l}\n\\bar{X}_1-\\bar{X}_2\\sim N\\left(\\mu_1-\\mu_2,\\sigma\\sqrt{\\frac{n_1+n_2}{n_1n_2}} \\right)\\\\\n\\displaystyle \\frac{n_1S_1^2+n_2S_2^2}{\\sigma^2} \\sim \\chi^2(n_1+n_2-2)\n\\end{array}\n\\right\\}\n\\Rightarrow\n\\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)}{\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}} \\sim T(n_1+n_2-2).\n\\]\nA partir de aquí, se pueden buscar los valores de la T de Student \\(-t^{n_1+n_2-2}_{\\alpha/2}\\) y \\(t^{n_1+n_2-2}_{\\alpha/2}\\) que cumplen\n\\[\nP\\left(-t^{n_1+n_2-2}_{\\alpha/2}\\leq \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)}{\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}}\n\\leq t^{n_1+n_2-2}_{\\alpha/2}\\right) = 1-\\alpha.\n\\]\nY deshaciendo la transformación se tiene\n\n\\[\\begin{align*}\n1-\\alpha\n&= P\\left(-t^{n_1+n_2-2}_{\\alpha/2}\\leq \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)}{\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}} \\leq t^{n_1+n_2-2}_{\\alpha/2}\\right) \\\\\n&= P\\left(-t^{n_1+n_2-2}_{\\alpha/2}\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}\\leq (\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2) \\leq t^{n_1+n_2-2}_{\\alpha/2}\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}\\right) \\\\\n&= P\\left(\\bar{X}_1-\\bar{X}_2 - t^{n_1+n_2-2}_{\\alpha/2}\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}\\leq \\mu_1-\\mu_2 \\leq \\bar{X}_1-\\bar{X}_2 + t^{n_1+n_2-2}_{\\alpha/2}\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}\\right).\n\\end{align*}\\]\n\nAsí pues, el intervalo de confianza para la diferencia de medias es\n\nTeorema 6.8 (Intervalo de confianza para la diferencia de medias de poblaciones normales con varianzas desconocidas iguales) Si \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\), con \\(\\sigma_1 = \\sigma_2\\) desconocidas, el intervalo de confianza para la diferencia de medias \\(\\mu_1-\\mu_2\\) con nivel de confianza \\(1-\\alpha\\) es\n\\[\n\\left[\\bar{X}_1-\\bar{X}_2-t^{n_1+n_2-2}_{\\alpha/2}\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}},\\bar{X}_1-\\bar{X}_2+t^{n_1+n_2-2}_{\\alpha/2}\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}\\right]\n\\]\no bien\n\\[\n\\bar{X}_1-\\bar{X}_2\\pm t^{n_1+n_2-2}_{\\alpha/2}\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}\n\\]\n\nSi \\([l_i,l_s]\\) es un intervalo de confianza de nivel \\(1-\\alpha\\) para la diferencia de medias \\(\\mu_1-\\mu_2\\), entonces\n\\[\n\\mu_1-\\mu_2 \\in [l_i,l_s]\n\\]\ncon una confianza del \\(1-\\alpha\\%\\).\nPor consiguiente, según los valores del intervalo de confianza se tiene:\n\nSi todos los valores del intervalo son negativos \\((l_s&lt;0)\\), entonces se puede concluir que \\(\\mu_1-\\mu_2&lt;0\\) y por tanto \\(\\mu_1&lt;\\mu_2\\).\nSi todos los valores del intervalo son positivos \\((l_i&gt;0)\\), entonces se puede concluir que \\(\\mu_1-\\mu_2&gt;0\\) y por tanto \\(\\mu_1&gt;\\mu_2\\).\nSi el intervalo tiene tanto valores positivos como negativos, y por tanto contiene al 0 (\\(0\\in [l_i,l_s])\\), entonces no se puede afirmar que una media sea mayor que la otra. En este caso se suele asumir la hipótesis de que las medias son iguales \\(\\mu_1=\\mu_2\\).\n\nTanto en el primer como en el segundo caso se dice que entre las medias hay diferencias estadísticamente significativas.\n\nEjemplo 6.12 Supóngase que se quiere comparar el rendimiento académico de dos grupos de alumnos, uno con 10 alumnos y otro con 12, que han seguido metodologías diferentes. Para ello se les realiza un examen y se obtienen las siguientes puntuaciones:\n\\[\\begin{align*}\nX_1 &: 4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3 \\\\\nX_2 &: 8 - 9 - 5 - 3 - 8 - 7 - 8 - 6 - 8 - 7 - 5 - 7\n\\end{align*}\\]\nSi se supone que ambas variables tienen la misma varianza, se tiene\n\n\\(\\bar{X}_1 = \\frac{4+\\cdots +3}{10}=5.3\\) y \\(\\bar{X}_2=\\frac{8+\\cdots +7}{12}=6.75\\) puntos.\n\\(S_1^2= \\frac{4^2+\\cdots + 3^2}{10}-5.3^2=3.21\\) y \\(S_2^2= \\frac{8^2+\\cdots\n+3^2}{12}-6.75^2=2.6875\\) puntos\\(^2\\).\n\\(\\hat{S}_p^2 = \\frac{10\\cdot 3.21+12\\cdot 2.6875}{10+12-2}= 3.2175\\) puntos\\(^2\\), y \\(\\hat S_p=1.7937\\).\n\\(t^{n_1+n_2-2}_{\\alpha/2}=t^{20}_{0.025}\\) es el valor de la T de Student de 20 grados de libertad que deja una probabilidad acumulada superior de \\(0.025\\), y que vale aproximadamente \\(2.09\\).\n\n\nY sustituyendo en la fórmula del intervalo llegamos a\n\\[\n5.3-6.75 \\pm 2.086\\cdot 1.7937\\sqrt{\\frac{10+12}{10\\cdot 12}} = -1.45\\pm 1.6021 = [-3.0521,\\,0.1521] \\text{ puntos}.\n\\]\nEs decir, la diferencia de puntuaciones medias \\(\\mu_1-\\mu_2\\) está entre \\(-3.0521\\) y \\(0.1521\\) puntos con una confianza del \\(95\\%\\).\nA la vista del intervalo se puede concluir que, puesto que el intervalo contiene tanto valores positivos como negativos, y por tanto contiene al 0, no puede afirmarse que una de las medias se mayor que la otra, de modo que se supone que son iguales y no se puede decir que haya diferencias significativas entre los grupos.\n\n\n6.6.3 Intervalo de confianza para la diferencia de medias de dos poblaciones normales con varianzas desconocidas y distintas\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias que cumplen las siguientes hipótesis:\n\nSu distribución es normal \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\).\nSus medias \\(\\mu_1\\), \\(\\mu_2\\) y varianzas \\(\\sigma_1^2\\), \\(\\sigma_2^2\\), son desconocidas, pero \\(\\sigma^2_1\\neq \\sigma^2_2\\).\n\nEn este caso el estimador de referencia sigue una distribución T de Student\n\\[\n\\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}} \\sim T(g),\n\\]\ndonde el número de grados de libertad es \\(g=n_1+n_2-2-\\Delta\\), siendo\n\\[\n\\Delta =\n\\frac{(\\frac{n_2-1}{n_1}\\hat{S}_1^2-\\frac{n_1-1}{n_2}\\hat{S}_2^2)^2}{\\frac{n_2-1}{n_1^2}\\hat{S}_1^4+\\frac{n_1-1}{n_2^2}\\hat{S}_2^4}.\n\\]\nA partir de aquí, una vez más, se pueden buscar los valores de la T de Student \\(-t^{g}_{\\alpha/2}\\) y \\(t^{g}_{\\alpha/2}\\) que cumplen\n\\[\nP\\left(-t^{g}_{\\alpha/2}\\leq \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}} \\leq t^{g}_{\\alpha/2}\\right) = 1-\\alpha.\n\\]\nY deshaciendo la transformación se llega a\n\n\\[\\begin{align*}\n1-\\alpha\n&= P\\left(-t^{g}_{\\alpha/2}\\leq \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}} \\leq t^{g}_{\\alpha/2}\\right) \\\\\n&= P\\left(-t^{g}_{\\alpha/2}\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}\\leq (\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2) \\leq t^{g}_{\\alpha/2}\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}\\right) \\\\\n&= P\\left(\\bar{X}_1-\\bar{X}_2 - t^{g}_{\\alpha/2}\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}\\leq\\mu_1-\\mu_2 \\leq \\bar{X}_1-\\bar{X}_2 + t^{g}_{\\alpha/2}\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}\\right) \\\\\n\\end{align*}\\]\n\nAsí pues, el intervalo de confianza para la diferencia de medias es\n\nTeorema 6.9 (Intervalo de confianza para la diferencia de medias de poblaciones normales con varianzas desconocidas distintas) Si \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\), con \\(\\sigma_1 \\neq \\sigma_2\\) desconocidas, el intervalo de confianza para la diferencia de medias \\(\\mu_1-\\mu_2\\) con nivel de confianza \\(1-\\alpha\\) es \\[\n\\left[\\bar{X}_1-\\bar{X}_2-t^{g}_{\\alpha/2}\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}},\\bar{X}_1-\\bar{X}_2-t^{g}_{\\alpha/2}\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}\\right]\n\\]\no bien\n\\[\n\\bar{X}_1-\\bar{X}_2\\pm t^{g}_{\\alpha/2}\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}\n\\]\n\nComo se acaba de ver, existen dos intervalos posibles para estimar la diferencia de medias: uno para cuando las varianzas poblacionales son iguales y otro para cuando no lo son.\nAhora bien, si las varianzas poblacionales son desconocidas,\n\n¿cómo saber qué intervalo utilizar?\n\nLa respuesta está en el próximo intervalo que se verá, que permite estimar la razón de varianzas \\(\\frac{\\sigma_2^2}{\\sigma_1^2}\\) y por tanto, su comparación.\nAsí pues, antes de calcular el intervalo de confianza para la comparación de medias, cuando las varianzas poblacionales sean desconocidas, es necesario calcular el intervalo de confianza para la razón de varianzas y elegir el intervalo para la comparación de medias en función del valor de dicho intervalo.\n\n\n6.6.4 Intervalo de confianza para el cociente de varianzas\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias que cumplen las siguientes hipótesis:\n\nSu distribución es normal \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\).\nSus medias \\(\\mu_1\\), \\(\\mu_2\\) y varianzas \\(\\sigma_1^2\\), \\(\\sigma_2^2\\) son desconocidas.\n\nEn este caso, para muestras de ambas poblaciones de tamaños \\(n_1\\) y \\(n_2\\) respectivamente, el estimador de referencia sigue una distribución F de Fisher-Snedecor:\n\\[\n\\left.\n\\begin{array}{l}\n\\displaystyle \\frac{(n_1-1)\\hat{S}_1^2}{\\sigma_1^2}\\sim \\chi^2(n_1-1) \\\\\n\\displaystyle \\frac{(n_2-1)\\hat{S}_2^2}{\\sigma_2^2}\\sim \\chi^2(n_2-1)\n\\end{array}\n\\right\\}\n\\Rightarrow\n\\frac{\\frac{\\frac{(n_2-1)\\hat{S}_2^2}{\\sigma_2^2}}{n_2-1}}{\\frac{\\frac{(n_1-1)\\hat{S}_1^2}{\\sigma_1^2}}{n_1-1}} =\n\\frac{\\sigma_1^2}{\\sigma_2^2}\\frac{\\hat{S}_2^2}{\\hat{S}_1^2}\\sim F(n_2-1,n_1-1).\n\\]\nComo la distribución F de Fisher-Snedecor no es simétrica respecto al 0, se toman dos valores \\(f^{n_2-1,n_1-1}_{\\alpha/2}\\) y \\(f^{n_2-1,n_1-1}_{1-\\alpha/2}\\) que dejen sendas colas de probabilidad acumulada inferior de \\(\\alpha/2\\) y \\(1-\\alpha/2\\) respectivamente.\n\n\n\n\n\nAsí pues, se tiene\n\\[\\begin{align*}\n1-\\alpha &= P\\left(f^{n_2-1,n_1-1}_{\\alpha/2}\\leq \\frac{\\sigma_1^2}{\\sigma_2^2}\\frac{\\hat{S}_2^2}{\\hat{S}_1^2}  \\leq\nf^{n_2-1,n_1-1}_{1-\\alpha/2}\\right) = \\\\ &= P\\left(f^{n_2-1,n_1-1}_{\\alpha/2}\\frac{\\hat{S}_1^2}{\\hat{S}_2^2} \\leq\n\\frac{\\sigma_1^2}{\\sigma_2^2}  \\leq f^{n_2-1,n_1-1}_{1-\\alpha/2}\\frac{\\hat{S}_1^2}{\\hat{S}_2^2}\\right)\n\\end{align*}\\]\nPor tanto, el intervalo de confianza para la comparación de varianzas de dos poblaciones normales es\n\nTeorema 6.10 (Intervalo de confianza para el cociente de varianzas de poblaciones normales) Si \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\), el intervalo de confianza para el cociente de varianzas \\(\\sigma_1/\\sigma_2\\) con nivel de confianza \\(1-\\alpha\\) es\n\\[\n\\left[f^{n_2-1,n_1-1}_{\\alpha/2}\\frac{\\hat{S}_1^2}{\\hat{S}_2^2},f^{n_2-1,n_1-1}_{1-\\alpha/2}\\frac{\\hat{S}_1^2}{\\hat{S}_2^2}\\right]\n\\]\n\nSi \\([l_i,l_s]\\) es un intervalo de confianza de nivel \\(1-\\alpha\\) para la razón de varianzas \\(\\frac{\\sigma_1^2}{\\sigma_2^2}\\), entonces\n\\[\n\\frac{\\sigma_1^2}{\\sigma_2^2} \\in [l_i,l_s]\n\\]\ncon una confianza del \\(1-\\alpha\\%\\).\nPor consiguiente, según los valores del intervalo de confianza se tiene:\n\nSi todos los valores del intervalo son menores que 1 \\((l_s&lt;1)\\), entonces se puede concluir que \\(\\frac{\\sigma_1^2}{\\sigma_2^2}&lt;1\\) y por tanto \\(\\sigma_1^2&lt;\\sigma_2^2\\).\nSi todos los valores del intervalo son mayores que 1 \\((l_i&gt;1)\\), entonces se puede concluir que \\(\\frac{\\sigma_1^2}{\\sigma_2^2}&gt;1\\) y por tanto \\(\\sigma_1^2&gt;\\sigma_2^2\\).\nSi el intervalo tiene tanto valores mayores como menores que 1, y por tanto contiene al 1 (\\(1\\in [l_i,l_s])\\), entonces no se puede afirmar que una varianza sea mayor que la otra. En este caso se suele asumir la hipótesis de que las varianzas son iguales \\(\\sigma_1^2=\\sigma_2^2\\).\n\n\nEjemplo 6.13 Siguiendo con el ejemplo de las puntuaciones en dos grupos:\n\\[\\begin{align*}\nX_1 &: 4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3 \\\\\nX_2 &: 8 - 9 - 5 - 3 - 8 - 7 - 8 - 6 - 8 - 7 - 5 - 7\n\\end{align*}\\]\nPara calcular el intervalo de confianza para la razón de varianzas con una confianza del \\(95\\%\\), se tiene:\n\n\\(\\bar{X}_1 = \\frac{4+\\cdots +3}{10}=5.3\\) puntos y \\(\\bar{X}_2=\\frac{8+\\cdots +7}{12}=6.75\\) puntos.\n\\(\\hat{S}_1^2= \\frac{(4-5.3)^2+\\cdots + (3-5.3)^2}{9}=3.5667\\) puntos\\(^2\\) y \\(\\hat{S}_2^2=\n\\frac{(8-6.75)^2+\\cdots + (3-6.75)^2}{11}=2.9318\\) puntos\\(^2\\).\n\\(f^{n_2-1,n_1-1}_{\\alpha/2}=f^{11,9}_{0.025}\\) es el valor de la F de Fisher de 11 y 9 grados de libertad que deja una probabilidad acumulada inferior de \\(0.025\\), y que vale aproximadamente \\(0.2787\\).\n\\(f^{n_2-1,n_1-1}_{1-\\alpha/2}=f^{11,9}_{0.975}\\) es el valor de la F de Fisher de 11 y 9 grados de libertad que deja una probabilidad acumulada inferior de \\(0.975\\), y que vale aproximadamente \\(3.9121\\).\n\n\nSustituyendo en la fórmula del intervalo se llega a\n\\[\n\\left[0.2787\\frac{3.5667}{2.9318},\\, 3.9121\\frac{3.5667}{2.9318}\\right] = [0.3391,\\, 4.7591] \\text{ puntos}^2.\n\\]\nEs decir, la razón de varianzas \\(\\frac{\\sigma_1^2}{\\sigma_2^2}\\) está entre \\(0.3391\\) y \\(4.7591\\) con una confianza del \\(95\\%\\).\nComo el intervalo tiene tanto valores menores como mayores que 1, no se puede concluir que una varianza sea mayor que la otra, y por tanto se mantiene la hipótesis de que ambas varianzas son iguales.\nSi ahora se quisiesen comparar las medias de ambas poblaciones, el intervalo de confianza para la diferencia de medias que habría que tomar es el que parte de la hipótesis de igualdad de varianzas, que precisamente es el que se ha utilizado antes.\n\n\n6.6.5 Intervalo de confianza para la diferencia de proporciones\nPara comparar las proporciones \\(p_1\\) y \\(p_2\\) de individuos que presentan una determinada característica en dos poblaciones independientes, se estima su diferencia \\(p_1-p_2\\).\nSi se toma una muestra de cada población, de tamaños \\(n_1\\) y \\(n_2\\) respectivamente, las variables que miden el número de individuos que presentan la característica en cada una de ellas siguen distribuciones\n\\[\nX_1\\sim B(n_1,p_1)\\quad \\mbox{y}\\quad X_2\\sim B(n_2,p_2)\n\\]\nCuando los tamaños muestrales son grandes (en realidad basta que se cumpla \\(n_1p_1\\geq 5\\), \\(n_1(1-p_1)\\geq 5\\), \\(n_2p_2\\geq 5\\) y \\(n_2(1-p_2)\\geq 5\\)), el teorema central de límite asegura que \\(X_1\\) y \\(X_2\\) tendrán distribuciones normales\n\\[\nX_1\\sim N(n_1p_1,\\sqrt{n_1p_1(1-p_1)}) \\quad \\mbox{y}\\quad X_2\\sim N(n_2p_2,\\sqrt{n_2p_2(1-p_2)}),\n\\]\ny las proporciones muestrales\n\\[\n\\hat{p}_1=\\frac{X_1}{n_1} \\sim N\\left(p_1,\\sqrt{\\frac{p_1(1-p_1)}{n_1}}\\right) \\quad \\mbox{y}\\quad\n\\hat{p}_2=\\frac{X_2}{n_2} \\sim N\\left(p_2,\\sqrt{\\frac{p_2(1-p_2)}{n_2}}\\right)\n\\]\nA partir de las proporciones muestrales se construye el estimador de referencia\n\\[\n\\hat{p}_1-\\hat{p}_2\\sim  N\\left(p_1-p_2,\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}\\right).\n\\]\nTipificando, se buscan valores \\(-z_{\\alpha/2}\\) y \\(z_{\\alpha/2}\\) que cumplan\n\\[\nP\\left(-z_{\\alpha/2}\\leq \\frac{(\\hat{p}_1-\\hat{p_2})-(p_1-p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}}\\leq z_{\\alpha/2} \\right) = 1-\\alpha.\n\\]\nY deshaciendo la tipificación, se llega a\n\n\\[\\begin{align*}\n1-\\alpha\n&= P\\left(-z_{\\alpha/2}\\leq \\frac{(\\hat{p}_1-\\hat{p_2})-(p_1-p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}}\\leq z_{\\alpha/2} \\right) \\\\\n&= P\\left(-z_{\\alpha/2}\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}\\leq (\\hat{p}_1-\\hat{p_2})-(p_1-p_2)\\leq z_{\\alpha/2}\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}} \\right) \\\\\n&= P\\left(\\hat{p}_1-\\hat{p_2} -z_{\\alpha/2}\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}\\leq \\hat{p}_1-\\hat{p_2} + p_1-p_2\\leq z_{\\alpha/2}\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}} \\right)\n\\end{align*}\\]\n\nAsí pues, el intervalo de confianza para la diferencia de proporciones es\n\nTeorema 6.11 (Intervalo de confianza para la diferencia de proporciones) Si \\(X_1\\sim B(n_1,p_1)\\) y \\(X_2\\sim B(n_2,p_2)\\), con \\(n_1p_1\\geq 5\\), \\(n_1(1-p_1)\\geq 5\\), \\(n_2p_2\\geq 5\\) y \\(n_2(1-p_2)\\geq 5\\), el intervalo de confianza para la diferencia de proporciones \\(p_1-p_2\\) con nivel de confianza \\(1-\\alpha\\) es\n\\[\n\\hat{p}_1-\\hat{p}_2\\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1}+\\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\]\n\n\nEjemplo 6.14 Supóngase que se quieren comparar las proporciones o porcentajes de aprobados en dos grupos que han seguido metodologías distintas. En el primer grupo han aprobado 24 alumnos de un total de 40, mientras que en el segundo han aprobado 48 de 60.\nPara calcular el intervalo de confianza para la diferencia de proporciones con un nivel de confianza del \\(95\\%\\), se tiene:\n\n\\(\\hat{p}_1=24/40= 0.6\\) y \\(\\hat{p}_2=48/60=0.8\\), de manera que se cumplen las hipótesis \\(n_1\\hat{p}_1=40\\cdot 0.6=24\\geq 5\\), \\(n_1(1-\\hat{p}_1)=40(1-0.6)=26\\geq 5\\), \\(n_2\\hat{p}_2=60\\cdot 0.8 =48\\geq 5\\) y \\(n_2(1-\\hat{p}_2)=60(1-0.8)=12\\geq 5\\).\n\\(z_{\\alpha/2}=z_{0.025}= 1.96\\).\n\nSustituyendo en la fórmula del intervalo se tiene\n\\[\n0.6-0.8\\pm 1.96 \\sqrt{\\frac{0.6(1-0.6)}{40}+\\frac{0.8(1-0.8)}{60}} = -0.2\\pm 0.17 = [-0.37,\\, -0.03].\n\\]\nComo el intervalo es negativo se tiene \\(p_1-p_2&lt;0\\Rightarrow p_1&lt;p_2\\), y se puede concluir que hay diferencias significativas en el porcentaje de aprobados.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimación de parámetros poblacionales</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html",
    "href": "09-contrastes-parametricos.html",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "",
    "text": "7.1 Hipótesis estadística y tipos de contrastes\nEn muchos estudios estadísticos, el objetivo, más que estimar el valor de un parámetro desconocido en la población, es comprobar la veracidad de una hipótesis formulada sobre la población objeto de estudio.\nEl investigador, de acuerdo a su experiencia o a estudios previos, suele tener conjeturas sobre la población estudiada que expresa en forma de hipótesis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#hipótesis-estadística-y-tipos-de-contrastes",
    "href": "09-contrastes-parametricos.html#hipótesis-estadística-y-tipos-de-contrastes",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "",
    "text": "Definición 7.1 (Hipótesis estadística) Una hipótesis estadística es cualquier afirmación o conjetura que determina, total o parcialmente, la distribución de una o varias variables de la población.\n\n\nEjemplo 7.1 Para contrastar el rendimiento académico de un grupo de alumnos en una determinada asignatura, podríamos platear la hipótesis de si el porcentaje de aprobados es mayor del 50%.\n\n\n7.1.1 Contraste de hipótesis\nEn general nunca se sabrá con absoluta certeza si una hipótesis estadística es cierta o falsa, ya que para ello habría que estudiar a todos los individuos de la población.\nPara comprobar la veracidad o falsedad de estas hipótesis hay que contrastarlas con los resultados empíricos obtenidos de las muestras. Si los resultados observados en las muestras coinciden, dentro del margen de error admisible debido al azar, con lo que cabría esperar en caso de que la hipótesis fuese cierta, la hipótesis se aceptará como verdadera, mientras que en caso contrario se rechazará como falsa y se buscarán nuevas hipótesis capaces de explicar los datos observados.\nComo las muestras se obtienen aleatoriamente, la decisión de aceptar o rechazar una hipótesis estadística se tomará sobre una base de probabilidad.\nLa metodología que se encarga de contrastar la veracidad de las hipótesis estadísticas se conoce como contraste de hipótesis.\n\n\n7.1.2 Tipos de contrastes de hipótesis\n\nContrastes de bondad de ajuste: El objetivo es comprobar una hipótesis sobre la forma de la distribución de la población.\nPor ejemplo, contrastar si las notas de un grupo de alumnos siguen una distribución normal.\nContrastes de conformidad: El objetivo es comprobar una hipótesis sobre alguno de los parámetros de la población.\nPor ejemplo, contrastar si las nota media en un grupo de alumnos es igual a 5.\nContrastes de homogeneidad : El objetivo es comparar dos poblaciones con respecto a alguno de sus parámetros.\nPor ejemplo, contrastar si el rendimiento de dos grupos de alumnos es el mismo comparando sus notas medias.\nContrastes de independencia: El objetivo es comprobar si existe relación entre dos variables de la población.\nPor ejemplo, contrastar si existe relación entre la notas de dos asignaturas diferentes.\n\nCuando las hipótesis se plantean sobre parámetros de la población, también se habla de contrastes paramétricos.\n\n\n7.1.3 Hipótesis nula e hipótesis alternativa\nEn la mayoría de los casos un contraste supone tomar una decisión entre dos hipótesis antagonistas:\n\nHipótesis nula: Es la hipótesis conservadora, ya que se mantendrá mientras que los datos de las muestras no reflejen claramente su falsedad. Se representa como \\(H_0\\).\nHipótesis alternativa: Es la negación de la hipótesis nula y generalmente representa la afirmación que se pretende probar. Se representa como \\(H_1\\).\n\nAmbas hipótesis se eligen de acuerdo con el principio de simplicidad científica:\n\n“Solamente se debe abandonar un modelo simple por otro más complejo cuando la evidencia a favor del último sea fuerte.” (Navaja de Occam)\n\nPor ejemplo, en el caso de un juicio, en el que el juez debe decidir si el acusado es culpable o inocente, la elección de hipótesis debería ser\n\\[\\begin{align*}\nH_0: & \\mbox{ Inocente} \\\\\nH_1: & \\mbox{ Culpable}\n\\end{align*}\\]\nya que la inocencia se asume, mientras que la culpabilidad hay que demostrarla.\nSegún esto, el juez sólo aceptaría la hipótesis alternativa cuando hubiese pruebas significativas de la culpabilidad del acusado.\nEl investigador jugaría el papel del fiscal, ya que su objetivo consistiría en intentar rechazar la hipótesis nula, es decir, demostrar culpabilidad del acusado.\n\n\n\n\n\n\nAdvertencia\n\n\n\n¡Esta metodología siempre favorece a la hipótesis nula!\n\n\n\n\n7.1.4 Contrastes de hipótesis paramétricos\nEn muchos contrastes, sobre todo en las pruebas de conformidad y de homogeneidad, las hipótesis se formulan sobre parámetros desconocidos de la población como puede ser una media, una varianza o una proporción.\nEn tal caso, la hipótesis nula siempre asigna al parámetro un valor concreto, mientras que la alternativa suele ser una hipótesis abierta que, aunque opuesta a la hipótesis nula, no fija el valor del parámetro.\nEsto da lugar a tres tipos de contrastes:\n\n\n\n\n\n\n\n\nBilateral\nUnilateral menor\nUnilateral mayor\n\n\n\n\n\\(H_0\\): \\(\\theta = \\theta_0\\)\n\\(H_0\\): \\(\\theta = \\theta_0\\)\n\\(H_0\\): \\(\\theta = \\theta_0\\)\n\n\n\\(H_1\\): \\(\\theta \\neq \\theta_0\\)\n\\(H_1\\): \\(\\theta &lt; \\theta_0\\)\n\\(H_1\\): \\(\\theta &gt; \\theta_0\\)\n\n\n\n\nEjemplo 7.2 Supóngase que existen sospechas de que en una población hay menos hombres que mujeres.\n¿Qué tipo de contraste debería plantearse para validar o refutar esta sospecha?\n\nLas sospechas se refieren al porcentaje o la proporción \\(p\\) de hombres en la población, por lo que se trata de un contraste paramétrico.\nEl objetivo es averiguar el valor de \\(p\\), por lo que se trata de una prueba de conformidad. En la hipótesis nula el valor de \\(p\\) se fijará a \\(0.5\\) ya que, de acuerdo a las leyes de la genética, en la población debería haber la misma proporción de hombres que de mujeres.\nFinalmente, existen sospechas de que el porcentaje de hombres es menor que el de mujeres, por lo que la hipótesis alternativa será de menor \\(p&lt;0.5\\).\n\nAsí pues, el contraste que debería plantearse es el siguiente:\n\\[\\begin{align*}\nH_0: & p=0.5 \\\\\nH_1: & p&lt;0.5\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#metodología-para-realizar-un-contraste-de-hipótesis",
    "href": "09-contrastes-parametricos.html#metodología-para-realizar-un-contraste-de-hipótesis",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.2 Metodología para realizar un contraste de hipótesis",
    "text": "7.2 Metodología para realizar un contraste de hipótesis\n\n7.2.1 Estadístico del contraste\nLa aceptación o rechazo de la hipótesis nula depende, en última instancia, de lo que se observe en la muestra.\nLa decisión se tomará según el valor que presente algún estadístico de la muestra relacionado con el parámetro o característica que se esté contrastando, y cuya distribución de probabilidad debe ser conocida suponiendo cierta la hipótesis nula y una vez fijado el tamaño de la muestra. Este estadístico recibe el nombre de estadístico del contraste.\nPara cada muestra, el estadístico dará una estimación a partir de la cual se tomará la decisión: si la estimación difiere demasiado del valor esperado bajo la hipótesis \\(H_0\\), entonces se rechazará, y en caso contrario se aceptará.\nLa lógica que guía la decisión es la de mantener la hipótesis nula a no ser que en la muestra haya pruebas contundentes de su falsedad. Siguiendo con el símil del juicio, se trataría de mantener la inocencia mientras no haya pruebas claras de culpabilidad.\n\nEjemplo 7.3 Volviendo al ejemplo del contraste sobre la proporción de hombres de una población\n\\[\\begin{align*}\nH_0: & p=0.5 \\\\\nH_1: & p&lt;0.5\n\\end{align*}\\]\nSi para resolver el contraste se toma una muestra aleatoria de 10 personas, podría tomarse como estadístico del contraste \\(X\\) el número de hombres en la muestra.\nSuponiendo cierta la hipótesis nula, el estadístico del contraste seguiría una distribución binomial \\(X\\sim B(10,\\,0.5)\\), de manera que el número esperado de hombres en la muestra sería 5.\nAsí pues, es lógico aceptar la hipótesis nula si en la muestra se obtiene un número de hombres próximo a 5 y rechazarla cuando el número de hombres sea muy inferior a 5. Pero, ¿dónde poner el límite entre los valores \\(X\\) que lleven a la aceptación y los que lleven al rechazo?\n\n\n\n7.2.2 Regiones de aceptación y de rechazo\nUna vez elegido el estadístico del contraste, lo siguiente es decidir para qué valores de este estadístico se decidirá aceptar la hipótesis nula y para que valores se rechazará. Esto divide del conjunto de valores posibles del estadístico en dos regiones:\n\nRegión de aceptación: Es el conjunto de valores del estadístico del contraste a partir de los cuales se decidirá aceptar la hipótesis nula.\nRegión de rechazo: Es el conjunto de valores del estadístico del contraste a partir de los cuales se decidirá rechazar la hipótesis nula, y por tanto, aceptar la hipótesis alternativa.\n\nDependiendo de la dirección del contraste, la región de rechazo quedará a un lado u otro del valor esperado del estadístico del contraste según la hipótesis nula:\n\nContraste bilateral \\(H_0:\\ \\theta=\\theta_0\\) \\(H_1:\\ \\theta\\neq\\theta_0\\).\n\n\n\n\n\n\n\nContraste unilateral de menor \\(H_0:\\ \\theta=\\theta_0\\) &H_1: &lt;_0$. \nContraste unilateral de mayor \\(H_0:\\ \\theta=\\theta_0\\) \\(H_1:\\ \\theta&gt;\\theta_0\\). \n\n\nEjemplo 7.4 Siguiendo con el ejemplo del contraste sobre la proporción de hombres de una población\n\\[\\begin{align*}\nH_0: & p=0.5 \\\\\nH_1: & p&lt;0.5\n\\end{align*}\\]\nComo el estadístico del contraste tenía una distribución binomial \\(X\\sim B(10,\\,0.5)\\) suponiendo cierta la hipótesis nula, su recorrido será de 0 a 10 y su valor esperado 5, por lo que, al tratarse de un contraste unilateral de menor, la región de rechazo quedará por debajo del 5. Pero, ¿dónde poner el límite entre las regiones de aceptación y de rechazo?\n\n\n\n\n\n\n\n\n7.2.3 Errores en un contraste de hipótesis\nHemos visto que un contraste de hipótesis se realiza mediante una regla de decisión que permite aceptar o rechazar la hipótesis nula dependiendo del valor que tome el estadístico del contraste.\nAl final el contraste se resuelve tomando una decisión de acuerdo a esta regla. El problema es que nunca se conocerá con absoluta certeza la veracidad o falsedad de una hipótesis, de modo que al aceptarla o rechazarla es posible que se esté tomando una decisión equivocada.\nLos errores que se pueden cometer en un contraste de hipótesis son de dos tipos:\n\nError de tipo I: Se comete cuando se rechaza la hipótesis nula siendo esta verdadera.\nError de tipo II: Se comete cuando se acepta la hipótesis nula siendo esta falsa.\n\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n\\mbox{Decisión} & H_0 \\mbox{ cierta} & H_1 \\mbox{ cierta} \\\\\n\\hline\n\\mbox{Aceptar } H_0 & \\color{green}{\\mbox{Decisión correcta}} & \\color{red}{\\mbox{Error de tipo II}} \\\\\n\\hline\n\\mbox{Rechazar }H_0 & \\color{red}{\\mbox{Error de tipo I}} & \\color{green}{\\mbox{Decisión correcta}} \\\\\n\\hline\n\\end{array}\n\\]\n\n\n7.2.4 Riesgos de los errores de un contraste de hipótesis\nLos riesgos de cometer cada tipo de error se cuantifican mediante probabilidades:\n\nDefinición 7.2 (Riesgos \\(\\alpha\\) y \\(\\beta\\)) En un contraste de hipótesis, se define el riesgo \\(\\alpha\\) como la máxima probabilidad de cometer un error de tipo I, es decir,\n\\[\nP(\\mbox{Rechazar }H_0|H_0) \\leq \\alpha,\n\\]\ny se define el riesgo \\(\\beta\\) como la máxima probabilidad de cometer un error de tipo II, es decir,\n\\[\nP(\\mbox{Aceptar }H_0|H_1) \\leq \\beta.\n\\]\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEn principio, puesto que esta metodología favorece a la hipótesis nula, el error del tipo I suele ser más grave que el error del tipo II, y por tanto, el riesgo \\(\\alpha\\) suele fijarse a niveles bajos de \\(0.1\\), \\(0.05\\) o \\(0.01\\), siendo \\(0.05\\) lo más habitual.\n\n\nDebe tenerse cuidado al interpretar el riesgo \\(\\alpha\\) ya que se trata de una probabilidad condicionada a que la hipótesis nula sea cierta. Por tanto, cuando se rechace la hipótesis nula con un riesgo \\(\\alpha=0.05\\), es erróneo decir 5 de cada 100 veces nos equivocaremos, ya que esto sería cierto sólo si la hipótesis nula fuese siempre verdadera.\nTampoco tiene sentido hablar de la probabilidad de haberse equivocado una vez tomada una decisión a partir de una muestra concreta, pues en tal caso, si se ha tomado la decisión acertada, la probabilidad de error es 0 y si se ha tomado la decisión equivocada, la probabilidad de error es 1.\n\n\n7.2.5 Determinación de las regiones de aceptación y de rechazo en función del riesgo \\(\\alpha\\)\nUna vez fijado el riesgo \\(\\alpha\\) que se está dispuesto a tolerar, es posible delimitar las regiones de aceptación y de rechazo para el estadístico del contraste de manera que la probabilidad acumulada en la región de rechazo sea \\(\\alpha\\), suponiendo cierta la hipótesis nula.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplo 7.5 Siguiendo con el contraste sobre la proporción de hombres de una población, como el estadístico del contraste sigue una distribución binomial \\(X\\sim B(10,0.5)\\), si se decide rechazar la hipótesis nula cuando en la muestra haya 2 o menos hombres, la probabilidad de cometer un error de tipo I será\n\\[P(X\\leq 2)= f(0)+f(1)+f(2)= 0.0010 + 0.0098 + 0.0439 = 0.0547.\\]\nSi riesgo máximo de error de tipo I que se está dispuesto a tolerar es \\(\\alpha=0.05\\), ¿qué valores del estadístico permitirán rechazar la hipótesis nula? \\[P(X\\leq 1)= f(0)+f(1) = 0.0010 + 0.0098 = 0.0107.\\] Es decir, sólo se podría rechazar la hipótesis nula con 0 o 1 hombres en la muestra.\n\n\n\n\n\n\n\n\n7.2.6 Riesgo \\(\\beta\\) y tamaño del efecto\nAunque el error de tipo II pueda parecer menos grave, también interesa que el riesgo \\(\\beta\\) sea bajo, ya que de lo contrario será difícil rechazar la hipótesis nula (que es lo que se persigue la mayoría de las veces), aunque haya pruebas muy claras de su falsedad.\nEl problema, en el caso de contrastes paramétricos, es que la hipótesis alternativa es una hipótesis abierta en la que no se fija el valor del parámetro a contrastar, de modo que, para poder calcular el riesgo \\(\\beta\\) es necesario fijar dicho valor.\nLo normal es fijar el valor del parámetro del contraste a la mínima cantidad para admitir diferencias significativas desde un punto de vista práctico o clínico. Esa mínima diferencia que se considera clínicamente significativa se conoce como tamaño del efecto y se representa por \\(\\delta\\).\n\n\n7.2.7 Potencia de un contraste\nPuesto que el objetivo del investigador suele ser rechazar la hipótesis nula, a menudo, lo más interesante de un contraste es su capacidad para detectar la falsedad de la hipótesis nula cuando realmente hay diferencias mayores que \\(\\delta\\) entre el verdadero valor del parámetro y el que establece la hipótesis nula.\n\nDefinición 7.3 (Potencia de un contraste) La potencia de un contraste de hipótesis se define como\n\\[\n\\mbox{Potencia} = P(\\mbox{Rechazar }H_0|H_1) = 1 - P(\\mbox{Aceptar }H_0|H_1) = 1-\\beta.\n\\]\n\nAsí pues, al reducir el riesgo \\(\\beta\\) se aumentará la potencia del contraste.\nUn contraste poco potente no suele ser interesante ya que no permitirá rechazar la hipótesis nula aunque haya evidencias en su contra.\n\n\n7.2.8 Cálculo del riesgo \\(\\beta\\) y de la potencia \\(1-\\beta\\)\n:::{#exm-** Supóngase que en el contraste sobre la proporción de hombres no se considera importante una diferencia de menos de un 10% con respecto al valor que establece la hipótesis nula, es decir, \\(\\delta=0.1\\).\nEsto permite fijar la hipótesis alternativa\n\\[H_1:\\ p=0.5-0.1=0.4.\\]\nSuponiendo cierta esta hipótesis el estadístico del contraste seguiría una distribución binomial \\(X\\sim B(10,\\,0.4)\\).\nEn tal caso, el riesgo \\(\\beta\\) para las regiones de aceptación y rechazo fijadas antes será\n\\[\\beta = P(\\mbox{Aceptar }H_0|H_1) = P(X\\geq 2) = 1 - P(X&lt;2) = 1-0.0464 = 0.9536.\\]\nComo puede apreciarse, se trata de un riesgo \\(\\beta\\) muy alto, por lo que la potencia del contraste sería sólo de\n\\[1-\\beta = 1-0.9536 = 0.0464,\\]\nlo que indica que no se trataría de un buen contraste para detectar diferencias de un 10% en el valor del parámetro.\n\n\n7.2.9 Relación del riesgo \\(\\beta\\) y el tamaño del efecto \\(\\delta\\)\nEl riesgo \\(\\beta\\) depende directamente de la mínima diferencia \\(\\delta\\) que se desea detectar con respecto al valor del parámetro que establece la hipótesis nula.\n\n\n\n\n\n\n\n\n\n\n\nEjemplo 7.6 Si en el contraste sobre la proporción de hombres se desease detectar una diferencia de al menos un 20% con respecto al valor que establece la hipótesis nula, es decir, \\(\\delta=0.2\\), entonces la hipótesis alternativa se fijaría a\n\\[H_1:\\ p=0.5-0.2=0.3,\\]\ny bajo esta hipótesis el estadístico del contraste seguiría una distribución binomial \\(X\\sim B(10,\\,0.3)\\).\nEn tal caso, el riesgo \\(\\beta\\) para las regiones de aceptación y rechazo fijadas antes sería\n\\[\\beta = P(\\mbox{Aceptar }H_0|H_1) = P(X\\geq 2) = 1 - P(X&lt;2) = 1-0.1493 = 0.8507,\\]\npor lo que el riesgo riesgo \\(\\beta\\) disminuiría y la potencia del contraste aumentaría\n\\[1-\\beta = 1-0.8507 = 0.1493,\\]\naunque seguiría siendo un contraste poco potente.\n\n\n\n7.2.10 Relación entre los riesgos \\(\\alpha\\) y \\(\\beta\\)\nLos riesgos \\(\\alpha\\) y \\(\\beta\\) están enfrentados, es decir, cuando uno aumenta el otro disminuye y viceversa.\n\n\n\n\n\n\n\n\n\n\n\nEjemplo 7.7 Si en el contraste sobre la proporción de hombres toma como riesgo \\(\\alpha=0.1,\\) entonces la región de rechazo sería \\(X\\leq 2\\) ya que, suponiendo cierta la hipótesis nula, \\(X\\sim B(10,\\, 0.5)\\), y\n\\[P(X\\leq 2) = 0.0547 \\leq 0.1=\\alpha.\\]\nEntonces, para una diferencia mínima \\(\\delta=0.1\\) y suponiendo cierta la hipótesis alternativa, \\(X\\sim B(10,\\,0.4)\\), el riesgo \\(\\beta\\) será\n\\[\\beta = P(\\mbox{Aceptar }H_0|H_1) = P(X\\geq 3) = 1- P(X&lt;3) = 1-0.1673 = 0.8327,\\]\ny ahora la potencia ha subido hasta\n\\[1-\\beta = 1-0.8327 = 0.1673.\\]\n\n\n\n7.2.11 Relación de los riesgos de error y el tamaño muestral\nLos riesgos de error también dependen el tamaño de la muestra, ya que al aumentar el tamaño de la muestra, la dispersión del estadístico del contraste disminuye y con ello también lo hacen los riesgos de error.\n\n\n\n\n\n\n\n\n\n\n\nEjemplo 7.8 Si para realizar el contraste sobre la proporción de hombres se hubiese tomado una muestra de tamaño 100, en lugar de 10, entonces, bajo la suposición de certeza de la hipótesis nula, el estadístico del contraste seguiría una distribución binomial \\(B(100,\\,0.5)\\), y ahora la región de rechazo sería \\(X\\leq 41\\), ya que\n\\[P(X\\leq 41) = 0.0443 \\leq 0.05 =\\alpha.\\]\nEntonces, para \\(\\delta=0.1\\) y suponiendo cierta la hipótesis alternativa, \\(X\\sim B(100,\\,0.4)\\), el riesgo \\(\\beta\\) sería\n\\[\\beta = P(\\mbox{Aceptar }H_0|H_1) = P(X\\geq 42) = 0.3775,\\]\ny ahora la potencia habría aumentado considerablemente\n\\[1-\\beta = 1-0.3775 = 0.6225.\\]\nEste contraste sería mucho más útil para detectar una diferencia de al menos un 10% con respecto al valor del parámetro que establece la hipótesis nula.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#curva-de-potencia",
    "href": "09-contrastes-parametricos.html#curva-de-potencia",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.3 Curva de potencia",
    "text": "7.3 Curva de potencia\nLa potencia de un contraste depende del valor del parámetro que establezca la hipótesis alternativa y, por tanto, es una función de este\n\\[\\mbox{Potencia}(x)= P(\\mbox{Rechazar }H_0|\\theta=x).\\]\nEsta función da la probabilidad de rechazar la hipótesis nula para cada valor del parámetro y se conoce como curva de potencia.\nCuando no se puede fijar el valor concreto del parámetro en la hipótesis alternativa, resulta útil representar esta curva para ver la bondad del contraste cuando no se rechaza la hipótesis nula. También es útil cuando sólo de dispone de un número determinado de individuos en la muestra, para ver si merece la pena hacer el estudio.\nUn contraste será mejor cuanto mayor sea el área encerrada por debajo de la curva de potencia.\n\nEjemplo 7.9 La curva de potencia correspondiente al contraste sobre la proporción de hombres en la población es la siguiente\n\n\n\n\n\n\n\n7.3.1 \\(p\\)-valor de un contraste de hipótesis\nEn general, siempre que la estimación del estadístico caiga dentro de la región de rechazo, rechazaremos la hipótesis nula, pero evidentemente, si dicha estimación se aleja bastante de la región de aceptación tendremos más confianza en el rechazo que si la estimación está cerca del límite entre las regiones de aceptación y rechazo.\nPor este motivo, al realizar un contraste, también se calcula la probabilidad de obtener una discrepancia mayor o igual a la observada entre la estimación del estadístico del contraste y su valor esperado según la hipótesis nula.\n\nDefinición 7.4 (\\(p\\)-valor) En un contraste de hipótesis, para cada estimación \\(x_0\\) del estadístico del contraste \\(X\\), dependiendo del tipo de contraste, se define el \\(p\\)-valor del contraste como\n\\[\n\\begin{array}{lc}\n\\mbox{Contraste bilateral}: & 2P(X\\geq x_0|H_0) \\\\\n\\mbox{Contraste unilateral de menor}: & P(X\\leq x_0|H_0) \\\\\n\\mbox{Contraste unilateral de mayor}: & P(X\\geq x_0|H_0)\n\\end{array}\n\\]\n\nEn cierto modo, el \\(p\\)-valor expresa la confianza que se tiene al tomar la decisión de rechazar la hipótesis nula. Cuanto más próximo esté el \\(p\\)-valor a 1, mayor confianza existe al aceptar la hipótesis nula, y cuanto más próximo esté a 0, mayor confianza hay al rechazarla.\n\n\n7.3.2 Regla de decisión de un contraste\nUna vez fijado el riesgo \\(\\alpha\\), la regla de decisión para realizar un contraste también puede expresarse de la siguiente manera:\n\n\n\n\n\n\nInterpretación\n\n\n\nRegla de decisión\n\\[\n\\begin{array}{ccc}\n\\mbox{Si $p$-valor $\\leq \\alpha$} & \\rightarrow & \\mbox{Rechazar $H_0$} \\\\\n\\mbox{Si $p$-valor $&gt; \\alpha$} & \\rightarrow & \\mbox{Aceptar $H_0$}.\n\\end{array}\n\\]\n\n\nDe este modo, el \\(p\\)-valor nos da información de para qué niveles de significación puede rechazarse la hipótesis nula y para cuales no.\n\nEjemplo 7.10 Si el contraste sobre la proporción de hombres se toma una muestra de tamaño 10 y se observa 1 hombre, entonces el \\(p\\)-valor, bajo a supuesta certeza de la hipótesis nula, \\(X\\sim B(10,\\, 0.5)\\), será\n\\[p = P(X\\leq 1)= 0.0107,\\]\nmientras que si en la muestra se observan 0 hombres, entonces el \\(p\\)-valor será\n\\[p = P(X\\leq 0)= 0.001.\\]\nEn el primer caso se rechazaría la hipótesis nula para un riesgo \\(\\alpha=0.05\\), pero no podría rechazarse par un riesgo \\(\\alpha=0.01\\), mientas que en el segundo caso también se rechazaría para \\(\\alpha=0.01\\). Es evidente que en el segundo la decisión de rechazar la hipótesis nula se tomaría con mayor confianza.\n\n\n\n7.3.3 Pasos para la realización de un contraste de hipótesis\n\nFormular la hipótesis nula \\(H_0\\) y la alternativa \\(H_1\\).\nFijar los riesgos \\(\\alpha\\) y \\(\\beta\\) deseados.\nSeleccionar el estadístico del contraste.\nFijar la mínima diferencia clínicamente significativa (tamaño del efecto) \\(\\delta\\).\nCalcular el tamaño muestral necesario \\(n\\).\nDelimitar las regiones de aceptación y rechazo.\nTomar una muestra de tamaño \\(n\\).\nCalcular el estadístico del contraste en la muestra.\nRechazar la hipótesis nula si la estimación cae en la región de rechazo o bien si el \\(p\\)-valor es menor que el riesgo \\(\\alpha\\) y aceptarla en caso contrario.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contrastes-paramétricos-más-importantes",
    "href": "09-contrastes-parametricos.html#contrastes-paramétricos-más-importantes",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.4 Contrastes paramétricos más importantes",
    "text": "7.4 Contrastes paramétricos más importantes\nPruebas de conformidad:\n\nContraste para la media de una población normal con varianza conocida.\nContraste para la media de una población normal con varianza desconocida.\nContraste para la media de una población con varianza desconocida a partir de muestras grandes.\nContraste para la varianza de una población normal.\nContraste para un proporción de una población.\n\nPruebas de homogeneidad:\n\nContraste de comparación de medias de dos poblaciones normales con varianzas conocidas.\nContraste de comparación de medias de dos poblaciones normales con varianzas desconocidas pero iguales.\nContraste de comparación de medias de dos poblaciones normales con varianzas desconocidas y diferentes.\nContraste de comparación de varianzas de dos poblaciones normales.\nContraste de comparación de proporciones de dos poblaciones.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contraste-para-la-media-de-una-población-normal-con-varianza-conocida",
    "href": "09-contrastes-parametricos.html#contraste-para-la-media-de-una-población-normal-con-varianza-conocida",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.5 Contraste para la media de una población normal con varianza conocida",
    "text": "7.5 Contraste para la media de una población normal con varianza conocida\nSea \\(X\\) una variable aleatoria que cumple las siguientes condiciones:\n\nSu distribución es normal \\(X\\sim N(\\mu,\\sigma)\\).\nLa media \\(\\mu\\) es desconocida, pero su varianza \\(\\sigma^2\\) es conocida.\n\nContraste:\n\\[\\begin{align*}\nH_0 &: \\mu=\\mu_0 \\\\\nH_1 &: \\mu\\neq \\mu_0\\end{align*}\\]\nEstadístico del contraste:\n\\[\n\\bar x\\sim N\\left(\\mu_0,\\frac{\\sigma}{\\sqrt{n}}\\right) \\Rightarrow Z=\\frac{\\bar x-\\mu_0}{\\sigma/\\sqrt{n}}\\sim N(0,1).\n\\]\nRegión de aceptación: \\(z_{\\alpha/2}&lt; Z &lt; z_{1-\\alpha/2}\\).\nRegión de rechazo: \\(Z\\leq z_{\\alpha/2}\\) y \\(Z\\geq z_{1-\\alpha/2}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contraste-para-la-media-de-una-población-normal-con-varianza-desconocida",
    "href": "09-contrastes-parametricos.html#contraste-para-la-media-de-una-población-normal-con-varianza-desconocida",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.6 Contraste para la media de una población normal con varianza desconocida",
    "text": "7.6 Contraste para la media de una población normal con varianza desconocida\nSea \\(X\\) una variable aleatoria que cumple las siguientes condiciones:\n\nSu distribución es normal \\(X\\sim N(\\mu,\\sigma)\\).\nTanto su media \\(\\mu\\) como su varianza \\(\\sigma^2\\) son desconocidas.\n\nContraste:\n\\[\\begin{align*}\nH_0 &: \\mu=\\mu_0 \\\\\nH_1 &: \\mu\\neq \\mu_0\\end{align*}\\]\nEstadístico del contraste: Utilizando la cuasivarianza como estimador de la varianza poblacional se tiene\n\\[\n\\bar x\\sim N\\left(\\mu_0,\\frac{\\sigma}{\\sqrt{n}}\\right) \\Rightarrow T=\\frac{\\bar x-\\mu_0}{\\hat s/\\sqrt{n}}\\sim T(n-1).\n\\]\nRegión de aceptación: \\(t^{n-1}_{\\alpha/2} &lt; T &lt; t^{n-1}_{1-\\alpha/2}\\).\nRegión de rechazo: \\(T\\leq t^{n-1}_{\\alpha/2}\\) y \\(T\\geq t^{n-1}_{1-\\alpha/2}\\).\n\nEjemplo 7.11 En un grupo de alumnos se quiere contrastar si la nota media de estadística es mayor que 5 puntos. Para ello se toma la siguiente muestra:\n\\[\n6.3, 5.4, 4.1, 5.0, 8.2, 7.6, 6.4, 5.6, 4.3, 5.2\n\\]\nEl contraste que se plantea es\n\\[H_0: \\mu=5 \\quad H_1: \\mu&gt;5\\]\nPara realizar el contraste se tiene:\n\n\\(\\bar x = \\frac{6.3+\\cdots+5.2}{10}=\\frac{58.1}{10}=5.81\\) puntos.\n\\(\\hat s^2 = \\frac{(6.3-5.81)^2+\\cdots+(5.2-5.81)^2}{9} = \\frac{15.949}{9}=1.7721\\) puntos\\(^2\\), y \\(\\hat s=1.3312\\) puntos.\n\nY el estadístico del contraste vale\n\\[\nT=\\frac{\\bar x-\\mu_0}{\\hat s/\\sqrt{n}} = \\frac{5.81-5}{1.3312/\\sqrt{10}}= 1.9246.\n\\]\nEl \\(p\\)-valor del contraste es \\(P(T(9)\\geq 1.9246) = 0.04323\\), lo que indica que se rechazaría la hipótesis nula para \\(\\alpha=0.05\\).\nLa región de rechazo es\n\\[\nT=\\frac{\\bar x-5}{1.3312/\\sqrt{10}} \\geq t^9_{0.95} = 1.8331 \\Leftrightarrow \\bar x \\geq 5+1.8331\\frac{1.3312}{\\sqrt\n10} = 5.7717,\n\\]\nde modo que se rechazará la hipótesis nula siempre que la media de la muestra sea mayor que \\(5.7717\\) y se aceptará en caso contrario.\nSuponiendo que en la práctica la mínima diferencia importante en la nota media fuese de un punto \\(\\delta=1\\), entonces bajo la hipótesis alternativa \\(H_1:\\mu=6\\), si se decidiese rechazar la hipótesis nula, el riesgo \\(\\beta\\) sería\n\\[\n\\beta = P\\left(T(9)\\leq \\frac{5.7717-6}{1.3312\\sqrt 10}\\right) = P(T(9)\\leq -0.5424) = 0.3004,\n\\]\nde manera que la potencia del contraste para detectar una diferencia de \\(\\delta=1\\) punto sería \\(1-\\beta=1-0.3004 = 0.6996\\).\n\n\n7.6.1 Determinación del tamaño muestral en un contraste para la media\nSe ha visto que para un riesgo \\(\\alpha\\) la región de rechazo era\n\\[\nT=\\frac{\\bar x-\\mu_0}{\\hat s/\\sqrt{n}} \\geq t^{n-1}_{1-\\alpha} \\approx z_{1-\\alpha}\\quad \\mbox{para } n\\geq 30.\n\\]\no lo que es equivalente\n\\[\n\\bar x \\geq \\mu_0+z_{1-\\alpha}\\frac{\\hat s}{\\sqrt n}.\n\\]\nSi el tamaño del efecto es \\(\\delta\\), para una hipótesis alternativa \\(H_1:\\mu=\\mu_0+\\delta\\), el riesgo \\(\\beta\\) es\n\\[\n\\beta = P\\left(Z&lt; \\frac{\\mu_0+z_{1-\\alpha}\\frac{\\hat s}{\\sqrt n}-(\\mu_0+\\delta)}{\\frac{\\hat s}{\\sqrt n}} \\right) = P\\left(Z&lt; \\frac{z_{1-\\alpha}\\frac{\\hat s}{\\sqrt n}-\\delta}{\\frac{\\hat s}{\\sqrt n}} \\right).\n\\]\nde modo que\n\\[\nz_\\beta = \\frac{z_{1-\\alpha}\\frac{\\hat s}{\\sqrt n}-\\delta}{\\frac{\\hat s}{\\sqrt n}} \\Leftrightarrow \\delta = (z_{1-\\alpha}-z_\\beta)\\frac{\\hat s}{\\sqrt n} \\Leftrightarrow n = (z_{1-\\alpha}-z_\\beta)^2\\frac{\\hat s^2}{\\delta^2} = (z_\\alpha+z_\\beta)^2\\frac{\\hat s^2}{\\delta^2}.\n\\]\n\nEjemplo 7.12 Se ha visto en el ejemplo anterior que la potencia del contraste para detectar una diferencia en la nota media de 1 punto era del \\(69.96%\\)%. Para aumentar la potencia del test hasta un \\(90\\)%, ¿cuántos alumnos habría que tomar en la muestra?\nComo se desea una potencia \\(1-\\beta=0.9\\), el riesgo \\(\\beta=0.1\\) y mirando en la tabla de la normal estándar se puede comprobar que \\(z_\\beta = z_{0.1}=1.2816\\).\nAplicando la fórmula anterior para determinar el tamaño muestral necesario, se tiene\n\\[\nn = (z_\\alpha+z_\\beta)^2\\frac{\\hat s^2}{\\delta^2} = (1.6449+1.2816)^2\\frac{1.7721}{1^2} = 15.18,\n\\]\nde manera que habría que haber tomado al menos 16 alumnos.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contraste-para-la-media-de-una-población-con-varianza-desconocida-y-muestras-grandes",
    "href": "09-contrastes-parametricos.html#contraste-para-la-media-de-una-población-con-varianza-desconocida-y-muestras-grandes",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.7 Contraste para la media de una población con varianza desconocida y muestras grandes",
    "text": "7.7 Contraste para la media de una población con varianza desconocida y muestras grandes\nSea \\(X\\) una variable aleatoria que cumple las siguientes condiciones:\n\nSu distribución puede ser de cualquier tipo.\nTanto su media \\(\\mu\\) como su varianza \\(\\sigma^2\\) son desconocidas.\n\nContraste:\n\\[\\begin{align*}\nH_0 &: \\mu=\\mu_0 \\\\\nH_1 &: \\mu\\neq \\mu_0\n\\end{align*}\\]\nEstadístico del contraste: Utilizando la cuasivarianza como estimador de la varianza poblacional y gracias al teorema central del límite por tratarse de muestras grandes (\\(n\\geq 30)\\) se tiene\n\\[\n\\bar x\\sim N\\left(\\mu_0,\\frac{\\sigma}{\\sqrt{n}}\\right) \\Rightarrow Z=\\frac{\\bar x-\\mu_0}{\\hat s/\\sqrt{n}}\\sim N(0,1).\n\\]\nRegión de aceptación: \\(-z_{\\alpha/2}&lt; Z &lt; z_{\\alpha/2}\\).\nRegión de rechazo: \\(Z\\leq -z_{\\alpha/2}\\) y \\(Z\\geq z_{\\alpha/2}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contraste-para-la-varianza-de-una-población-normal",
    "href": "09-contrastes-parametricos.html#contraste-para-la-varianza-de-una-población-normal",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.8 Contraste para la varianza de una población normal",
    "text": "7.8 Contraste para la varianza de una población normal\nSea \\(X\\) una variable aleatoria que cumple las siguientes hipótesis:\n\nSu distribución es normal \\(X\\sim N(\\mu,\\sigma)\\).\nTanto su media \\(\\mu\\) como su varianza \\(\\sigma^2\\) son desconocidas.\n\nContraste:\n\\[\\begin{align*}\nH_0 &: \\sigma=\\sigma_0 \\\\\nH_1 &: \\sigma\\neq \\sigma_0\n\\end{align*}\\]\nEstadístico del contraste: Partiendo de la cuasivarianza muestral como estimador de la varianza poblacional, se tiene\n\\[\nJ=\\frac{nS^2}{\\sigma_0^2} = \\frac{(n-1)\\hat{S}^2}{\\sigma_0^2}\\sim \\chi^2(n-1),\n\\]\nque sigue una distribución chi-cuadrado de \\(n-1\\) grados de libertad.\nRegión de aceptación: \\(\\chi_{\\alpha/2}^{n-1} &lt; J &lt; \\chi_{1-\\alpha/2}^{n-1}\\).\nRegión de rechazo: \\(J\\leq \\chi_{\\alpha/2}^{n-1}\\) y \\(J\\geq \\chi_{1-\\alpha/2}^{n-1}\\).\n\nEjemplo 7.13 En un grupo de alumnos se quiere contrastar si la desviación típica de la nota es mayor de 1 punto. Para ello se toma la siguiente muestra:\n\\[\n6.3, 5.4, 4.1, 5.0, 8.2, 7.6, 6.4, 5.6, 4.3, 5.2\n\\]\nEl contraste que se plantea es\n\\[\nH_0: \\sigma=1 \\quad H_1: \\sigma&gt;1\n\\]\nPara realizar el contraste se tiene:\n\n\\(\\bar x = \\frac{6.3+\\cdots+5.2}{10}=\\frac{58.1}{10}=5.81\\) puntos.\n\\(\\hat s^2 = \\frac{(6.3-5.81)^2+\\cdots+(5.2-5.81)^2}{9} = \\frac{15.949}{9}=1.7721\\) puntos\\(^2\\).\n\nEl estadístico del contraste vale\n\\[\nJ= \\frac{(n-1)\\hat{S}^2}{\\sigma_0^2} = \\frac{9\\cdot1.7721}{1^2} = 15.949,\n\\]\ny el \\(p\\)-valor del contraste es \\(P(\\chi(9)\\geq 15.949) = 0.068\\), por lo que no se puede rechazar la hipótesis nula para \\(\\alpha=0.05\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contraste-para-proporción-de-una-población",
    "href": "09-contrastes-parametricos.html#contraste-para-proporción-de-una-población",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.9 Contraste para proporción de una población",
    "text": "7.9 Contraste para proporción de una población\nSea \\(p\\) la proporción de individuos de una población que tienen una determinada característica.\nContraste:\n\\[\\begin{align*}\nH_0 &: p=p_0 \\\\\nH_1 &: p\\neq p_0\n\\end{align*}\\]\nEstadístico del contraste: La variable que mide el número de individuos con la característica en una muestra aleatoria de tamaño \\(n\\) sigue una distribución binomial \\(X\\sim B(n,p_0)\\). De acuerdo al teorema central del límite, para muestras grandes (\\(np\\geq 5\\) y \\(n(1-p)\\geq 5\\)), \\(X\\sim N(np_0,\\sqrt{np_0(1-p_0)})\\), y se cumple\n\\[\n\\hat{p}=\\frac{X}{n} \\sim N\\left(p_0,\\sqrt{\\frac{p_0(1-p_0)}{n}}\\right) \\Rightarrow Z = \\frac{\\hat\np-p_0}{\\sqrt{p_0(1-p_0)/n}}\\sim N(0,1).\n\\]\nRegión de aceptación: \\(z_{\\alpha/2}&lt; Z &lt; z_{1-\\alpha/2}\\).\nRegión de rechazo: \\(Z\\leq z_{\\alpha/2}\\) y \\(Z\\geq z_{1-\\alpha/2}\\).\n\nEjemplo 7.14 En un grupo de alumnos se desea estimar si el porcentaje de aprobados es mayor del \\(50\\%\\). Para ello se toma una muestra de 80 alumnos entre los que hay 50 aprobados.\nEl contraste que se plantea es\n\\[\\begin{align*}\nH_0 &: p=0.5 \\\\\nH_1 &: p&gt;0.5\n\\end{align*}\\]\nPara realizar el contraste se tiene que \\(\\hat p= 50/80 = 0.625\\) y como se cumple \\(n\\hat p=80\\cdot 0.625 = 50\\geq 5\\) y \\(n(1-\\hat p)=80(1-0.625)=30\\geq 5\\), el estadístico del contraste vale\n\\[\nZ = \\frac{\\hat p-p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.625-0.5}{\\sqrt{0.5(1-0.5)/80}} = 2.2361.\n\\]\ny el \\(p\\)-valor del contraste es \\(P(Z\\geq 2.2361)=0.0127\\), por lo que se rechaza la hipótesis nula para \\(\\alpha=0.05\\) y se concluye que el porcentaje de aprobados es mayor de la mitad.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contraste-de-comparación-de-medias-de-dos-poblaciones-normales-con-varianzas-conocidas",
    "href": "09-contrastes-parametricos.html#contraste-de-comparación-de-medias-de-dos-poblaciones-normales-con-varianzas-conocidas",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.10 Contraste de comparación de medias de dos poblaciones normales con varianzas conocidas",
    "text": "7.10 Contraste de comparación de medias de dos poblaciones normales con varianzas conocidas\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias que cumplen las siguientes condiciones:\n\nSu distribución es normal \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) \\(X_2\\sim N(\\mu_2,\\sigma_2)\\).\nSus medias \\(\\mu_1\\) y \\(\\mu_2\\) son desconocidas, pero sus varianzas \\(\\sigma^2_1\\) y \\(\\sigma^2_2\\) son conocidas.\n\nContraste:\n\\[\\begin{align*}\nH_0 &: \\mu_1=\\mu_2 \\\\\nH_1 &: \\mu_1\\neq \\mu_2\n\\end{align*}\\]\nEstadístico del contraste:\n\\[\n\\left.\n\\begin{array}{l}\n\\bar{X}_1\\sim N\\left(\\mu_1,\\frac{\\sigma_1}{\\sqrt{n_1}} \\right) \\\\\n\\bar{X}_2\\sim N\\left(\\mu_2,\\frac{\\sigma_2}{\\sqrt{n_2}} \\right)\n\\end{array}\n\\right\\}\n\\Rightarrow\n\\]\n\\[\n\\Rightarrow\n\\bar{X}_1-\\bar{X}_2 \\sim N\\left(\\mu_1-\\mu_2,\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\right)\n\\Rightarrow Z= \\frac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}}\\sim\nN(0,1).\n\\]\nRegión de aceptación: \\(-z_{\\alpha/2}&lt; Z &lt; z_{\\alpha/2}\\).\nRegión de rechazo: \\(Z\\leq -z_{\\alpha/2}\\) y \\(Z\\geq z_{\\alpha/2}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contraste-de-comparación-de-medias-de-dos-poblaciones-normales-con-varianzas-desconocidas-e-iguales",
    "href": "09-contrastes-parametricos.html#contraste-de-comparación-de-medias-de-dos-poblaciones-normales-con-varianzas-desconocidas-e-iguales",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.11 Contraste de comparación de medias de dos poblaciones normales con varianzas desconocidas e iguales",
    "text": "7.11 Contraste de comparación de medias de dos poblaciones normales con varianzas desconocidas e iguales\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias que cumplen las siguientes condiciones:\n\nSu distribución es normal \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\).\nSus medias \\(\\mu_1\\) y \\(\\mu_2\\) son desconocidas y sus varianzas también, pero son iguales \\(\\sigma^2_1=\\sigma^2_2=\\sigma^2\\).\n\nContraste:\n\\[\\begin{align*}\nH_0 &: \\mu_1=\\mu_2 \\\\\nH_1 &: \\mu_1\\neq \\mu_2\n\\end{align*}\\]\nEstadístico del contraste:\n\\[\n\\left.\n\\begin{array}{l}\n\\bar{X}_1-\\bar{X}_2\\sim N\\left(\\mu_1-\\mu_2,\\sigma\\sqrt{\\frac{n_1+n_2}{n_1n_2}} \\right) \\\\\n\\displaystyle \\frac{n_1S_1^2+n_2S_2^2}{\\sigma^2} \\sim \\chi^2(n_1+n_2-2)\n\\end{array}\n\\right\\}\n\\Rightarrow\nT=\\frac{\\bar{X}_1-\\bar{X}_2}{\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}} \\sim T(n_1+n_2-2).\n\\]\nRegión de aceptación: \\(-t_{\\alpha/2}^{n_1+n_2-2} &lt; T &lt; t_{\\alpha/2}^{n_1+n_2-2}\\).\nRegión de rechazo: \\(T\\leq -t_{\\alpha/2}^{n_1+n_2-2}\\) y \\(T\\geq t_{\\alpha/2}^{n_1+n_2-2}\\).\n\nEjemplo 7.15 Se quiere comparar el rendimiento académico de dos grupos de alumnos, uno con 10 alumnos y otro con 12, que han seguido metodologías diferentes. Para ello se les realiza un examen y se obtienen las siguientes puntuaciones:\n\\[\\begin{align*}\nX_1 &: 4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3  \\\\\nX_2 &: 8 - 9 - 5 - 3 - 8 - 7 - 8 - 6 - 8 - 7 - 5 - 7\n\\end{align*}\\]\nEl contraste que se plantea es\n\\[\nH_0: \\mu_1=\\mu_2\\qquad H_1: \\mu_1\\neq \\mu_2\n\\]\nPara realizar el contraste, se tiene\n\n\\(\\bar{X}_1 = \\frac{4+\\cdots +3}{10}=5.3\\) puntos y \\(\\bar{X}_2=\\frac{8+\\cdots +7}{12}=6.75\\) puntos.\n\\(S_1^2= \\frac{4^2+\\cdots + 3^2}{10}-5.3^2=3.21\\) puntos\\(^2\\) y \\(S_2^2= \\frac{8^2+\\cdots +3^2}{12}-6.75^2=2.69\\) puntos\\(^2\\).\n\\(\\hat{S}_p^2 = \\frac{10\\cdot 3.21+12\\cdot 2.6875}{10+12-2}= 3.2175\\) puntos\\(^2\\), y \\(\\hat S_p=1.7937\\).\n\nSi se suponen varianzas iguales, el estadístico del contraste vale\n\\[\nT=\\frac{\\bar{X}_1-\\bar{X}_2}{\\hat{S}_p\\sqrt{\\frac{n_1+n_2}{n_1n_2}}} = \\frac{5.3-6.75}{1.7937\\sqrt{\\frac{10+12}{10\\cdot 12}}} = -1.8879,\n\\]\ny el \\(p\\)-valor del contraste es \\(2P(T(20)\\leq -1.8879) = 0.0736\\), de modo que no se puede rechazar la hipótesis nula y se concluye que no hay diferencias significativas entre las notas medias de los grupos.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contraste-de-comparación-de-medias-de-dos-poblaciones-normales-con-varianzas-desconocidas",
    "href": "09-contrastes-parametricos.html#contraste-de-comparación-de-medias-de-dos-poblaciones-normales-con-varianzas-desconocidas",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.12 Contraste de comparación de medias de dos poblaciones normales con varianzas desconocidas",
    "text": "7.12 Contraste de comparación de medias de dos poblaciones normales con varianzas desconocidas\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias que cumplen las siguientes condiciones:\n\nSu distribución es normal \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\).\nSus medias \\(\\mu_1\\), \\(\\mu_2\\) y varianzas \\(\\sigma_1^2\\), \\(\\sigma_2^2\\), son desconocidas, pero \\(\\sigma^2_1\\not = \\sigma^2_2\\).\n\nContraste:\n\\[\\begin{align*}\nH_0 &: \\mu_1=\\mu_2 \\\\\nH_1 &: \\mu_1\\neq \\mu_2\n\\end{align*}\\]\nEstadístico del contraste:\n\\[\nT=\\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\hat{S}^2_1}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}} \\sim T(g),\n\\]\ncon \\(g=n_1+n_2-2-\\Delta\\) y\n\\[\\Delta = \\frac{(\\frac{n_2-1}{n_1}\\hat{S}_1^2-\\frac{n_1-1}{n_2}\\hat{S}_2^2)^2}{\\frac{n_2-1}{n_1^2}\\hat{S}_1^4+\\frac{n_1-1}{n_2^2}\\hat{S}_2^4}.\n\\]\nRegión de aceptación: \\(-t_{\\alpha/2}^{g} &lt; T &lt; t_{\\alpha/2}^{g}\\).\nRegión de rechazo: \\(T\\leq -t_{\\alpha/2}^{g}\\) y \\(T\\geq t_{\\alpha/2}^{g}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contraste-de-comparación-de-varianzas-de-dos-poblaciones-normales",
    "href": "09-contrastes-parametricos.html#contraste-de-comparación-de-varianzas-de-dos-poblaciones-normales",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.13 Contraste de comparación de varianzas de dos poblaciones normales",
    "text": "7.13 Contraste de comparación de varianzas de dos poblaciones normales\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias que cumplen las siguientes condiciones:\n\nSu distribución es normal \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\).\nSus medias \\(\\mu_1\\), \\(\\mu_2\\) y varianzas \\(\\sigma_1^2\\), \\(\\sigma_2^2\\) son desconocidas.\n\nContraste:\n\\[\\begin{align*}\nH_0 &: \\sigma_1=\\sigma_2 \\\\\nH_1 &: \\sigma_1\\neq \\sigma_2\n\\end{align*}\\]\nEstadístico del contraste: \\[\\left.\n\\begin{array}{l}\n\\displaystyle \\frac{(n_1-1)\\hat{S}_1^2}{\\sigma_1^2}\\sim \\chi^2(n_1-1) \\\\\n\\displaystyle \\frac{(n_2-1)\\hat{S}_2^2}{\\sigma_2^2}\\sim \\chi^2(n_2-1)\n\\end{array}\n\\right\\}\n\\Rightarrow\nF= \\frac{\\frac{\\frac{(n_1-1)\\hat{S}_1^2}{\\sigma_1^2}}{n_1-1}}{\\frac{\\frac{(n_2-1)\\hat{S}_2^2}{\\sigma_2^2}}{n_2-1}} =\n\\frac{\\sigma_2^2}{\\sigma_1^2}\\frac{\\hat{S}_1^2}{\\hat{S}_2^2}\\sim F(n_1-1,n_2-1).\n\\]\nRegión de aceptación: \\(F_{\\alpha/2}^{n_1-1,n_2-1} &lt; F &lt; F_{1-\\alpha/2}^{n_1-1,n_2-1}\\).\nRegión de rechazo: \\(F\\leq F_{\\alpha/2}^{n_1-1,n_2-1}\\) y \\(F\\geq F_{1-\\alpha/2}^{n_1-1,n_2-1}\\).\n\nEjemplo 7.16 Siguiendo con el ejemplo de las puntuaciones en dos grupos:\n\\[\\begin{align*}\nX_1 &: 4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3  \\\\\nX_2 &: 8 - 9 - 5 - 3 - 8 - 7 - 8 - 6 - 8 - 7 - 5 - 7\n\\end{align*}\\]\nSi se desea comparar las varianzas, el contraste que se plantea es\n\\[\nH_0: \\sigma_1=\\sigma_2\\qquad H_1: \\sigma_1\\neq \\sigma_2\n\\]\nPara realizar el contraste, se tiene\n\n\\(\\bar{X}_1 = \\frac{4+\\cdots +3}{10}=5.3\\) puntos y \\(\\bar{X}_2=\\frac{8+\\cdots +7}{12}=6.75\\) puntos.\n\\(\\hat{S}_1^2= \\frac{(4-5.3)^2+\\cdots + (3-5.3)^2}{9}=3.5667\\) y \\(\\hat{S}_2^2= \\frac{(8-6.75)^2+\\cdots + (3-6.75)^2}{11}=2.9318\\) puntos\\(^2\\).\n\nEl estadístico del contraste vale\n\\[\nF = \\frac{\\hat{S}_1^2}{\\hat{S}_2^2} = \\frac{3.5667}{2.9318}=1.2165,\n\\]\ny el \\(p\\)-valor del contraste es \\(2P(F(9,11)\\leq 1.2165)=0.7468\\), por lo que se mantiene la hipótesis de igualdad de varianzas.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#contraste-de-comparación-de-proporciones-de-dos-poblaciones",
    "href": "09-contrastes-parametricos.html#contraste-de-comparación-de-proporciones-de-dos-poblaciones",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.14 Contraste de comparación de proporciones de dos poblaciones",
    "text": "7.14 Contraste de comparación de proporciones de dos poblaciones\nSean \\(p_1\\) y \\(p_2\\) las respectivas proporciones de individuos que presentan una determinada característica en dos poblaciones.\nContraste:\n\\[\nH_0: p_1=p_2\\qquad H_1: p_1\\neq p_2\n\\]\nEstadístico del contraste: Las variables que miden el número de individuos con la característica en dos muestras aleatorias de tamaños \\(n_1\\) y \\(n_2\\) respectivamente, siguen distribuciones binomiales \\(X_1\\sim B(n_1,p_1)\\) y \\(X_2\\sim B(n_2,p_2)\\). Si las muestras son grandes (\\(n_ip_i\\geq 5\\) y \\(n_i(1-p_i)\\geq 5\\)), de acuerdo al teorema central del límite, \\(X_1\\sim N(np_1,\\sqrt{np_1(1-p_1)})\\) y \\(X_2\\sim N(np_2,\\sqrt{np_2(1-p_2)})\\), y se cumple\n\\[\n\\left.\n\\begin{array}{l}\n\\hat{p}_1=\\frac{X_1}{n_1} \\sim N\\left(p_1,\\sqrt{\\frac{p_1(1-p_1)}{n_1}}\\right) \\\\\n\\hat{p}_2=\\frac{X_2}{n_2} \\sim N\\left(p_2,\\sqrt{\\frac{p_2(1-p_2)}{n_2}}\\right)\n\\end{array}\n\\right\\}\n\\Rightarrow Z = \\frac{\\hat{p}_1-\\hat{p_2}}{\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}}\\sim N(0,1)\n\\]\nRegión de aceptación: \\(z_{\\alpha/2}&lt; Z &lt; z_{1-\\alpha/2}\\).\nRegión de rechazo: \\(z\\leq z_{\\alpha/2}\\) y \\(z\\geq z_{1-\\alpha/2}\\).\n\nEjemplo 7.17 Se quiere comparar los porcentajes de aprobados en dos grupos que han seguido metodologías distintas. En el primer grupo han aprobado 24 alumnos de un total de 40, mientras que en el segundo han aprobado 48 de 60.\nEl contraste que se plantea es\n\\[\nH_0: p_1=p_2\\qquad H_1: p_1\\neq p_2\n\\]\nPara realizar el contraste, se tiene \\(\\hat{p}_1=24/40= 0.6\\) y \\(\\hat{p}_2=48/60=0.8\\), de manera que se cumplen las condiciones \\(n_1\\hat{p}_1=40\\cdot 0.6=24\\geq 5\\), \\(n_1(1-\\hat{p}_1)=40(1-0.6)=26\\geq 5\\), \\(n_2\\hat{p}_2=60\\cdot 0.8=48\\geq 5\\) y \\(n_2(1-\\hat{p}_2)=60(1-0.8)=12\\geq 5\\), y el estadístico del contraste vale\n\\[\nZ = \\frac{\\hat{p}_1-\\hat{p_2}}{\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}} = \\frac{0.6-0.8}{\\sqrt{\\frac{0.6(1-0.6)}{40}+\\frac{0.8(1-0.8)}{60}}} = -2.1483,\n\\]\ny el \\(p\\)-valor del contraste es \\(2P(Z\\leq -2.1483)= 0.0317\\), de manera que se rechaza la hipótesis nula para \\(\\alpha=0.05\\) y se concluye que hay diferencias.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "09-contrastes-parametricos.html#realización-de-contrastes-mediante-intervalos-de-confianza",
    "href": "09-contrastes-parametricos.html#realización-de-contrastes-mediante-intervalos-de-confianza",
    "title": "7  Contrastes de hipótesis paramétricos",
    "section": "7.15 Realización de contrastes mediante intervalos de confianza",
    "text": "7.15 Realización de contrastes mediante intervalos de confianza\nUna interesante alternativa a la realización de un contraste\n\\[\nH_0: \\theta=\\theta_0\\qquad H_1: \\theta\\neq \\theta_0\n\\]\ncon un riesgo \\(\\alpha\\), es calcular el intervalo de confianza para \\(\\theta\\) con un nivel de confianza \\(1-\\alpha\\), ya que este intervalo se puede interpretar como el conjunto aceptable de hipótesis para \\(\\theta\\), de manera que si \\(\\theta_0\\) está fuera del intervalo, la hipótesis nula es poco creíble y puede rechazarse, mientras que si está dentro la hipótesis es creíble y se acepta.\nCuando el contraste sea unilateral de menor, el contraste se realizaría comparando \\(\\theta_0\\) con el límite superior del intervalo de confianza para \\(\\theta\\) con un nivel de confianza \\(1-2\\alpha\\), mientras que si el contraste es unilateral de mayor, se comparará con el límite inferior del intervalo.\n\n\n\n\n\n\n\n\nContraste\nIntervalo de confianza\nDecisión\n\n\n\n\nBilateral\n\\([l_i,l_s]\\) con nivel de confianza \\(1-\\alpha\\)\nRechazar \\(H_0\\) si \\(\\theta_0\\not \\in [l_i,l_s]\\)\n\n\nUnilateral menor\n\\([-\\infty,l_s]\\) con nivel de confianza \\(1-2\\alpha\\)\nRechazar \\(H_0\\) si \\(\\theta_0\\geq l_s\\)\n\n\nUnilateral mayor\n\\([l_i,\\infty]\\) con nivel de confianza \\(1-2\\alpha\\)\nRechazar \\(H_0\\) si \\(\\theta_0\\leq l_i\\)\n\n\n\n\nEjemplo 7.18 Volviendo al contraste para comparar el rendimiento académico de dos grupos de alumnos que han obtenido las siguientes puntuaciones:\n\\[\\begin{align*}\nX_1 &: 4 - 6 - 8 - 7 - 7 - 6 - 5 - 2 - 5 - 3  \\\\\nX_2 &: 8 - 9 - 5 - 3 - 8 - 7 - 8 - 6 - 8 - 7 - 5 - 7\n\\end{align*}\\]\nEl contraste que se planteaba era\n\\[\nH_0: \\mu_1=\\mu_2\\qquad H_1: \\mu_1\\neq \\mu_2\n\\]\nComo se trata de un contraste bilateral, el intervalo de confianza para la diferencia de medias \\(\\mu_1-\\mu_2\\) con nivel de confianza \\(1-\\alpha=0.95\\), suponiendo varianzas iguales, vale \\([-3.0521, 0.1521]\\) puntos. Y como según la hipótesis nula \\(\\mu_1-\\mu_2=0\\), y el 0 cae dentro del intervalo, se acepta la hipótesis nula.\nLa ventaja del intervalo es que, además de permitirnos realizar el contraste, nos da una idea de la magnitud de la diferencia entre las medias de los grupos.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrastes de hipótesis paramétricos</span>"
    ]
  },
  {
    "objectID": "10-anova-ancova.html",
    "href": "10-anova-ancova.html",
    "title": "8  Análisis de la Varianza",
    "section": "",
    "text": "8.1 Análisis de la varianza de 1 factor\nEl Análisis de la Varianza con un Factor (ANOVA por sus siglas en inglés), es una técnica estadística de contraste de hipótesis, que sirve para comparar las medias una variable cuantitativa, que suele llamarse variable dependiente o respuesta, en distintos grupos o muestras definidas por una variable cualitativa, llamada variable independiente o factor. Las distintas categorías del factor que definen los grupos a comparar se conocen como niveles o tratamientos del factor.\nSe trata, por tanto, de una generalización de la prueba T para la comparación de medias de dos muestras independientes, para diseños experimentales con más de dos muestras. Y se diferencia de un análisis de regresión simple, donde tanto la variable dependiente como la independiente eran cuantitativas, en que en el análisis de la varianza de un factor, la variable independiente o factor es una variable cualitativa, aunque como veremos más adelante en los contrastes de regresión, se puede plantear un contraste de ANOVA como si fuese un contraste de regresión lineal.\nUn ejemplo de aplicación de esta técnica podría ser la comparación del nivel de colesterol medio según el grupo sanguíneo. En este caso, la dependiente o factor es el grupo sanguíneo, con cuatro niveles (A, B, O, AB), mientras que la variable respuesta es el nivel de colesterol.\nPara comparar las medias de la variable respuesta según los diferentes niveles del factor, se plantea un contraste de hipótesis en el que la hipótesis nula, \\(H_0\\), es que la variable respuesta tiene igual media en todos los niveles, mientras que la hipótesis alternativa, \\(H_1\\), es que hay diferencias estadísticamente significativas entre al menos dos de las medias. Dicho contraste se realiza mediante la descomposición de la varianza total de la variable respuesta; de ahí procede el nombre de esta técnica.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análisis de la Varianza</span>"
    ]
  },
  {
    "objectID": "10-anova-ancova.html#análisis-de-la-varianza-de-1-factor",
    "href": "10-anova-ancova.html#análisis-de-la-varianza-de-1-factor",
    "title": "8  Análisis de la Varianza",
    "section": "",
    "text": "8.1.1 El contraste de ANOVA\nLa notación habitual en ANOVA es la siguiente:\n\n\\(k\\): es el número de niveles del factor.\n\\(n_i\\): es el tamaño de la muestra aleatoria correspondiente al nivel \\(i\\)-ésimo del factor.\n\\(n = \\sum_{i = 1}^k {n_i}\\): es el número total de observaciones.\n\\(X_{ij}\\ (i = 1,...,k;\\,j = 1,...,n_i)\\): es una variable aleatoria que indica la respuesta del \\(j\\)-ésimo individuo al \\(i\\)-ésimo nivel del factor.\n\\(x_{ij}\\): es el valor concreto, en una muestra dada, de la variable \\(X_{ij}\\).\n\n\\[\n\\begin{array}{c}\n\\mbox{Niveles del factor}\\\\\n\\begin{array}{cccc}\n\\hline\n1 & 2 & \\cdots & k\\\\\n\\hline\nX_{11} & X_{21} & \\cdots & X_{k1}\\\\\nX_{12} & X_{22} & \\cdots & X_{k2}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots\\\\\nX_{1n_1} & X_{2n_2} & \\cdots & X_{kn_k}\\\\\n\\hline\n\\end{array}\n\\end{array}\n\\]\n\n\\(\\mu_i\\): es la media de la población del nivel \\(i\\).\n\\(\\bar X_i = \\sum_{j = 1}^{n_i} X_{ij}/n_i\\): es la variable media muestral del nivel \\(i\\), y estimador de \\(\\mu_i\\).\n\\(\\bar x_i = \\sum_{j = 1}^{n_i} x_{ij}/n_i\\): es la estimación concreta para una muestra dada de la variable media muestral del nivel \\(i\\).\n\\(\\mu\\): es la media global de la población (incluidos todos los niveles).\n\\(\\bar X  = \\sum_{i = 1}^k \\sum_{j = 1}^{n_i } X_{ij}/n\\): es la variable media muestral de todas las respuestas, y estimador de \\(\\mu\\).\n\\(\\bar x  = \\sum_{i = 1}^k \\sum_{j = 1}^{n_i }x_{ij}/n\\): es la estimación concreta para una muestra dada de la variable media muestral.\n\nCon esta notación podemos expresar la variable respuesta mediante un modelo matemático que la descompone en componentes atribuibles a distintas causas:\n\\[\nX_{ij} = \\mu + (\\mu_i-\\mu) + (X_{ij}-\\mu_i),\n\\]\nes decir, la respuesta \\(j\\)-ésima en el nivel \\(i\\)-ésimo puede descomponerse como resultado de una media global, más la desviación con respecto a la media global debida al hecho de que recibe el tratamiento \\(i\\)-ésimo, más una nueva desviación con respecto a la media del nivel debida a influencias aleatorias.\nSobre este modelo se plantea la hipótesis nula: las medias correspondientes a todos los niveles son iguales; y su correspondiente alternativa: al menos hay dos medias de nivel que son diferentes.\n\\[\\begin{align*}\nH_0: & \\mu_1 = \\mu_2  = \\cdots = \\mu _k\\\\\nH_1: & \\mu_i \\neq \\mu_j \\textrm{ para algún } i\\neq j.\n\\end{align*}\\]\nPara poder realizar el contraste con este modelo es necesario plantear ciertas hipótesis estructurales (supuestos del modelo):\n\nndependencia: Las \\(k\\) muestras, correspondientes a los \\(k\\) niveles del factor,representan muestras aleatorias independientes de \\(k\\) poblaciones con medias \\(\\mu_1  = \\mu_2  = \\cdots = \\mu_k\\) desconocidas.\nNormalidad: Cada una de las \\(k\\) poblaciones es normal.\nHomocedasticidad: Cada una de las \\(k\\) poblaciones tiene la misma varianza \\(\\sigma^2\\).\n\nTeniendo en cuenta la hipótesis nula y los supuestos del modelo, si se sustituye en el modelo las medias poblacionales por sus correspondientes estimadores muestrales, se tiene\n\\[\nX_{ij} = \\bar X+(\\bar X_i-\\bar X)+(X_{ij}-\\bar X_i),\\] o lo que es lo mismo, \\[X_{ij}-\\bar X= (\\bar X_i-\\bar X)+(X_{ij}-\\bar X_i).\n\\]\nElevando al cuadrado y teniendo en cuenta las propiedades de los sumatorios, se llega a la ecuación que recibe el nombre de identidad de la suma de cuadrados:\n\\[\n\\sum_{i=1}^k \\sum_{j=1}^{n_i} (X_{ij}-\\bar X)^2  = \\sum_{i=1}^k n_i(\\bar X_i-\\bar X)^2 +\\sum_{i=1}^k \\sum_{j = 1}^{n_i}(X_{ij}-\\bar X_i)^2,\n\\]\ndonde:\n\n\\(\\sum_{i=1}^k \\sum_{j=1}^{n_i} (X_{ij}- \\bar X)^2\\): recibe el nombre de suma total de cuadrados, (\\(STC\\)), y es la suma de cuadrados de las desviaciones con respecto a la media global; por lo tanto, una medida de la variabilidad total de los datos.\n\\(\\sum_{j=1}^k n_i(\\bar X_i-\\bar X)^2\\): recibe el nombre de suma de cuadrados de los tratamientos o suma de cuadrados intergrupos, (\\(SCInter\\)), y es la suma ponderada de cuadrados de las desviaciones de la media de cada nivel con respecto a la media global; por lo tanto, una medida de la variabilidad atribuida al hecho de que se utilizan diferentes niveles o tratamientos.\n\\(\\sum_{i=1}^k \\sum_{j=1}^{n_i}(X_{ij}-\\bar X_i )^2\\): recibe el nombre de suma de cuadrados residual o suma de cuadrados intragrupos, (\\(SCIntra\\)), y es la suma de cuadrados de las desviaciones de las observaciones con respecto a las medias de sus respectivos niveles o tratamientos; por lo tanto, una medida de la variabilidad en los datos atribuida a las fluctuaciones aleatorias dentro del mismo nivel.\n\nCon esta notación la identidad de suma de cuadrados se expresa:\n\\[\nSCT = SCInter + SCIntra\n\\]\nY un último paso para llegar al estadístico que permitirá contrastar \\(H_0\\), es la definición de los Cuadrados Medios, que se obtienen al dividir cada una de las sumas de cuadrados por sus correspondientes grados de libertad. Para \\(SCT\\) el número de grados de libertad es \\(n-1\\); para \\(SCInter\\) es \\(k-1\\); y para \\(SCIntra\\) es \\(n-k\\).\nPor lo tanto,\n\\[\\begin{align*}\nCMT &= \\frac{SCT}{n - 1}\\\\\nCMInter &= \\frac{SCInter}{k - 1}\\\\\nCMIntra &= \\frac{SCIntra}{n -k}\n\\end{align*}\\]\nY se podría demostrar que, en el supuesto de ser cierta la hipótesis nula y los supuestos del modelo, el cociente\n\\[\n\\frac{{CMInter}}{{CMIntra}}\n\\]\nsigue una distribución \\(F\\) de Fisher con \\(k-1\\) y \\(n-k\\) grados de libertad.\nDe esta forma, si \\(H_0\\) es cierta, el valor del cociente para un conjunto de muestras dado, estará próximo a 0 (aún siendo siempre mayor que 0); pero si no se cumple \\(H_0\\) crece la variabilidad intergrupos y la estimación del estadístico crece. En definitiva, realizaremos un contraste de hipótesis unilateral con cola a la derecha de igualdad de varianzas, y para ello calcularemos el \\(p\\)-valor de la estimación de \\(F\\) obtenida y aceptaremos o rechazaremos en función del nivel de significación fijado.\n\n8.1.1.1 Tabla de ANOVA\nTodos los estadísticos planteados en el apartado anterior se recogen en una tabla denominada Tabla de ANOVA, en la que se ponen los resultados de las estimaciones de dichos estadísticos en las muestras concretas objeto de estudio. Esas tablas también son las que aportan como resultado de cualquier ANOVA los programas estadísticos, que suelen añadir al final de la tabla el \\(p\\)-valor del estadístico \\(F\\) calculado, y que permite aceptar o rechazar la hipótesis nula de que las medias correspondientes a todos los niveles del factor son iguales.\n\n\n\n\n\n\n\n\n\n\n\n\nSuma de cuadrados\nGrados de libertad\nCuadrados medios\nEstadístico F\np-valor\n\n\n\n\nIntergrupos\n\\(SCInter\\)\n\\(k-1\\)\n\\(CMInter=\\frac{SCInter}{k-1}\\)\n\\(f=\\frac{CMInter}{CMIntra}\\)\n\\(P(F&gt;f)\\)\n\n\nIntragrupos\n\\(SCIntra\\)\n\\(n-k\\)\n\\(CMIntra=\\frac{SCIntra}{n-k}\\)\n\n\n\n\nTotal\n\\(SCT\\)\n\\(n-1\\)\n\n\n\n\n\n\n\n\n\n8.1.2 Test de comparaciones múltiples y por parejas\nUna vez realizado el ANOVA de un factor para comparar las \\(k\\) medias correspondientes a los \\(k\\) niveles o tratamientos del factor, se puede concluir aceptando la hipótesis nula, en cuyo caso se da por concluido el análisis de los datos en cuanto a detección de diferencias entre los niveles, o rechazándola, en cuyo caso es natural continuar con el análisis para tratar de localizar con precisión dónde está la diferencia, cuáles son los niveles cuyas respuestas son estadísticamente diferentes.\nEn el segundo caso, hay varios métodos que permiten detectar las diferencias entre las medias de los diferentes niveles, y que reciben el nombre de test de comparaciones múltiples. A su vez este tipo de test se suele clasificar en:\n\nTest de comparaciones por parejas: Su objetivo es la comparación una a una de todas las posibles parejas de medias que se pueden tomar al considerar los diferentes niveles. Su resultado es una tabla en la que se reflejan las diferencias entre todas las posibles parejas y los intervalos de confianza para dichas diferencias, con la indicación de si hay o no diferencias significativas entre las mismas. Hay que aclarar que los intervalos obtenidos no son los mismos que resultarían si se considera cada pareja de medias por separado, ya que el rechazo de \\(H_0\\) en el contraste general de ANOVA implica la aceptación de una hipótesis alternativa en la que están involucrados varios contrastes individuales a su vez; y si queremos mantener un nivel de significación \\(\\alpha\\) en el general, en los individuales debemos utilizar un \\(\\alpha'\\) considerablemente más pequeño.\nTest de rango múltiple: Su objetivo es la identificación de subconjuntos homogéneos de medias que no se diferencian entre sí.\n\nPara los primeros se puede utilizar el test de Bonferroni; para los segundos, el test de Duncan; y para ambas categorías a la vez los test HSD de Tukey y Scheffé.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análisis de la Varianza</span>"
    ]
  },
  {
    "objectID": "10-anova-ancova.html#anova-de-dos-o-más-factores",
    "href": "10-anova-ancova.html#anova-de-dos-o-más-factores",
    "title": "8  Análisis de la Varianza",
    "section": "8.2 ANOVA de dos o más factores",
    "text": "8.2 ANOVA de dos o más factores\nEn muchos problemas aparece no ya un único factor que permite clasificar los individuos de la muestra en \\(k\\) diferentes niveles, sino que pueden presentarse dos o más factores que permiten clasificar a los individuos de la muestra en múltiples grupos según diferentes criterios, que se pueden analizar para ver si hay o no diferencias significativas entre las medias de la variable respuesta.\nPara tratar con este tipo de problemas surge el ANOVA de Dos o Más Factores (o también ANOVA de Dos o Más Vías) como una generalización del proceso de un factor, que además de permitir el análisis de la influencia de cada uno de los factores por separado también hace posible el estudio de la interacción entre ellos.\nPor otra parte, también son frecuentes los problemas en los que se toma más de una medida de una variable cuantitativa (respuesta) en cada sujeto de la muestra, y se procede al análisis de las diferencias entre las diferentes medidas. Si sólo se toman dos, el procedimiento adecuado es la T de Student de datos pareados, o su correspondiente no paramétrico, el test de Wilcoxon; pero si se han tomado tres o más medidas, el test paramétrico correspondiente a la T de Student de datos pareados es el ANOVA de Medidas Repetidas.\nIncluso también se puede dar el caso de un problema en el que se analice una misma variable cuantitativa medida en varias ocasiones en cada sujeto de la muestra pero teniendo en cuenta a la vez la influencia de uno, dos o más factores que permiten clasificar a los individuos en varios subgrupos diferentes. En definitiva, pueden aparecer problemas donde a la par que un ANOVA de medidas repetidas se requiera realizar un ANOVA de dos o más vías.\nPor último, la situación más compleja que se puede plantear en el análisis de una respuesta cuantitativa se presenta cuando, añadida a medidas repetidas y dos o más vías o factores de clasificación, se tienen una o más variables cuantitativas, llamadas covariables, que se piensa que pueden influir en la variable respuesta. Se procede entonces a realizar un ANCOVA o Análisis de Covarianza, con el que se pretende analizar la influencia de los factores y también ver si hay diferencias entre las medidas repetidas pero habiendo eliminado previamente la influencia (variabilidad) debida a la presencia de las covariables que se pretenden controlar.\n\n8.2.1 ANOVA de dos factores con dos niveles cada factor\nPara entender qué es un ANOVA de dos o más factores, conviene partir de un caso sencillo con dos factores y dos niveles en cada factor. Por ejemplo, se puede plantear un experimento con individuos que siguen o no una dieta (primer factor: dieta, con dos niveles: sí y no), y que a su vez toman o no un determinado fármaco (segundo factor: fármaco, con dos niveles: sí y no) para reducir su peso corporal (variable respuesta numérica: reducción del peso corporal expresada en Kg). En esta situación, se generan cuatro grupos diferentes: los que no hacen dieta ni toman fármaco (No-No), los que no hacen dieta pero sí toman fármaco (No-Sí), los que hacen dieta y no toman fármaco (Sí-No), y los que hacen dieta y toman fármaco (Sí-Sí). Y se pueden plantear tres efectos diferentes:\n\nEl de la dieta: viendo si hay o no diferencias significativas en los Kg perdidos entre los individuos que la han seguido y los que no.\nEl del fármaco: viendo si hay o no diferencias significativas en los Kg perdidos entre los individuos que lo han tomado y los que no.\nEl de la interacción: viendo si el efecto combinado de dieta y fármaco es diferente del que tendrían sumando sus efectos por separado, y entonces se diría que sí que hay interacción; o si por el contrario el efecto de la combinación de dieta y fármaco es el mismo que la suma de los efectos por separado, y entonces se diría que no hay interacción.\nA su vez, si hay interacción se puede dar en dos sentidos: si la combinación de dieta y fármaco ha hecho perder más kilos a los pacientes de los que cabría esperar con la suma de dieta y fármaco por separado, entonces la interacción de ambos factores ha actuado en sinergia con los mismos, mientras que si la combinación ha hecho perder menos kilos de los que cabría esperar con dieta y fármaco por separado, entonces la interacción ha actuado en antagonismo con ambos.\n\nSiguiendo con el ejemplo, supongamos que la tabla que aparece a continuación refleja la media de Kg perdidos dentro de cada uno de los grupos comentados. Por simplificar el ejemplo, no se reflejan los Kg en cada individuo con la consiguiente variabilidad de los mismos, pero el ANOVA de dos vías sí que tendría en cuenta esa variabilidad para poder hacer inferencia estadística, plantear contrastes de hipótesis y calcular sus correspondientes p-valores.\n\n\n\n\nFármaco No\nFármaco Sí\n\n\n\n\nDieta No\n0\n5\n\n\nDieta Sí\n3\n8\n\n\n\nSi los resultados obtenidos fuesen los de la tabla anterior, se diría que no hay interacción entre fármaco y dieta, ya que el efecto del fármaco en el grupo de los que no hacen dieta ha hecho perder 5 Kg en media a los individuos, el efecto de la dieta en el grupo de los que no toman fármaco les ha hecho perder 3 Kg en media, y el efecto combinado de dieta y fármaco ha hecho perder 8 Kg con respecto a los que no hacen dieta y tampoco toman fármaco. Estos 8 Kg son iguales a la suma de 3 y 5, es decir iguales a la suma de los efectos de los factores por separado, sin ningún tipo de interacción (de término añadido) que cambie el resultado de la suma.\nCon las medias de los cuatro grupos que se generan en el cruce de los dos factores, cada uno con dos niveles (2x2), se representan los gráficos de medias que aparecen más adelante. En estos gráficos, cuando no hay interacción las rectas que unen las medias correspondientes a un mismo nivel de uno de los factores son paralelas dentro de cierto margen de variabilidad.\n\n\n\nGráfico de medias de dos factores sin interacción\n\n\nPor el contrario, también podría obtenerse una tabla en la que la suma de los efectos por separado fuese menor que el efecto combinado de dieta y fármaco:\n\n\n\n\nFármaco No\nFármaco Sí\n\n\n\n\nDieta No\n0\n5\n\n\nDieta Sí\n3\n12\n\n\n\nEn este caso, dejando al margen las variabilidad dentro de cada uno de los grupos y suponiendo que la misma es lo suficientemente pequeña como para que las diferencias sean significativas, los 8 Kg en media que se perderían al sumar los efectos por separado de dieta y fármaco son menores que los 12 que, en media, han perdido los individuos que han tomado el fármaco y han seguido la dieta a la vez. Por lo tanto, se ha producido una interacción de los dos factores que, al unirlos, ha servido para potenciar sus efectos por separado. Dicho de otra forma, para explicar el resultado final de los individuos que han tomado el fármaco y también han seguido la dieta habría que introducir un nuevo término en la suma, el término de interacción, que contribuiría con 4 Kg de pérdida añadidos a los 8 Kg que se perderían considerando simplemente la suma de dieta y fármaco. Como este nuevo término contribuye a aumentar la pérdida que se obtendría al sumar los efectos por separado de ambos factores, se trataría de un caso de interacción en sinergia con los dos factores de partida.\n\n\n\nGráfico de medias de dos factores con interacción sinérgica\n\n\nPor último, también se podría obtener una tabla en la que la suma de los efectos por separado fuese mayor que el efecto combinado de los dos factores:\n\n\n\n\nFármaco No\nFármaco Sí\n\n\n\n\nDieta No\n0\n5\n\n\nDieta Sí\n3\n4\n\n\n\nIgualmente, en este nuevo ejemplo los 8 Kg en media que se perderían al sumar los efectos por separado de los dos factores son mayores que los 4 que en realidad pierden, en media, los individuos que han seguido la dieta y utilizado el fármaco. Por lo tanto, para explicar el resultado obtenido en el grupo de los que toman el fármaco y siguen la dieta habría que introducir un término añadido a la suma de efectos sin más, que se restaría a los 8 Kg hasta dejarlos en 4 Kg. Se trataría de un caso de interacción en antagonismo con los dos factores de partida.\n\n\n\nGráfico de medias de dos factores con interacción antagónica\n\n\nEn realidad, la interacción también puede producirse en sinergia con uno de los factores y en antagonismo con el otro, ya que a veces los dos factores pueden producir un efecto con signo contrario. Por ejemplo, al hablar del factor dieta, se tiende a pensar que se trata de una dieta que sirve para bajar el peso, pero también cabe plantearse un experimento con personas que siguen una dieta de alto contenido calórico que en principio debería hacerles subir peso y ver qué evolución siguen cuando a la vez toman un fármaco para bajarlo.\nComo puede deducirse fácilmente de las tablas y gráficas anteriores, la presencia de interacción implica que la diferencia entre las medias de los dos grupos dentro de un mismo nivel de uno de los factores no es la misma que para el otro nivel. Por ejemplo, en la segunda tabla, la diferencia entre las medias de Kg perdidos entre los que sí que toman el fármaco y los que no lo toman vale: 5-0=5 Kg en los que no hacen dieta, y 12-3=9 Kg en los que sí que hacen dieta. Lo cual gráficamente se traduce en que la pendiente de la recta que une las medias dentro del grupo de los que sí que toman el fármaco es diferente de la pendiente que une las medias dentro del grupo de los que no lo toman. En las ideas anteriores se basará el planteamiento del contraste de hipótesis para ver si la interacción ha resultado o no significativa.\nComo ya se ha comentado, en cualquiera de las tablas anteriores se podrían analizar tres efectos diferentes: el de la dieta, el del fármaco y el de la interacción de dieta con fármaco; lo cual, en términos matemáticos, se traduce en tres contrastes de hipótesis diferentes:\n\nEfecto de la dieta sobre la cantidad de peso perdido:\n\\(H_0: \\mu_{\\text{con dieta}}=\\mu_{\\text{sin dieta}}\\)\n\\(H_1: \\mu_{\\text{con dieta}}\\neq\\mu_{\\text{sin dieta}}\\)\nEfecto del fármaco sobre la cantidad de peso perdido:\n\\(H_0: \\mu_{\\text{con fármaco}}=\\mu_{\\text{sin fármaco}}\\)\n\\(H_1: \\mu_{\\text{con fármaco}}\\neq\\mu_{\\text{sin fármaco}}\\)\nEfecto de la interacción entre dieta y fármaco, que a su vez se puede plantear de dos formas equivalentes:\n\nViendo si dentro dentro de los grupos definidos en función de la dieta la diferencia de Kg perdidos entre los que toman fármaco y los que no lo toman es la misma:\n\\(H_0: (\\mu_{\\text{con fármaco}}-\\mu_{\\text{sin fármaco}})_{\\text{sin dieta}}=(\\mu_{\\text{con fármaco}}-\\mu_{\\text{sin fármaco}})_{\\text{con dieta}}\\)\n\\(H_1: (\\mu_{\\text{con fármaco}}-\\mu_{\\text{sin fármaco}})_{\\text{sin dieta}}\\neq(\\mu_{\\text{con fármaco}}-\\mu_{\\text{sin fármaco}})_{\\text{con dieta}}\\)\nViendo si dentro de los grupos definidos en función del fármaco la diferencia de Kg perdidos entre los que hacen dieta y los que no la hacen es la misma:\n\\(H_0: (\\mu_{\\text{con dieta}}-\\mu_{\\text{sin dieta}})_{\\text{sin fármaco}}=(\\mu_{\\text{con dieta}}-\\mu_{\\text{sin dieta}})_{\\text{con fármaco}}\\)\n\\(H_1: (\\mu_{\\text{con dieta}}-\\mu_{\\text{sin dieta}})_{\\text{sin fármaco}}\\neq(\\mu_{\\text{con dieta}}-\\mu_{\\text{sin dieta}})_{\\text{con fármaco}}\\)\n\n\nAunque los detalles matemáticos más precisos sobre cómo el ANOVA de dos o más vías da respuesta a los contrastes expuestos quedan fuera del nivel de esta práctica, la idea general es sencilla y muy parecida a la explicada con más detalle en la práctica de ANOVA de una vía. En el ANOVA de una vía, la variabilidad total de los datos, expresada como suma de distancias al cuadrado con respecto a la media global (llamada Suma de Cuadrados Total), se descompone en dos diferentes fuentes de variabilidad: las distancias al cuadrado de los datos de cada grupo con respecto a la media del grupo, Suma de Cuadrados Intra, más las distancias al cuadrado entre las diferentes medias de los grupos y la media general, Suma de Cuadrados Inter. La suma de cuadrados intra-grupos es también llamada Variabilidad Residual o Suma de Cuadrados Residual, ya que su cuantía es una medida de la dispersión residual, remanente incluso después de haber dividido los datos en grupos. Estas sumas de cuadrados, una vez divididas por sus correspondientes grados de libertad, generan varianzas llamadas Cuadrados Medios, y el cociente de cuadrados medios (cuadrado medio inter dividido entre cuadrado medio intra) bajo la hipótesis nula de igualdad de medias en todos los grupos sigue una distribución F de Fisher que se puede utilizar para calcular un \\(p\\)-valor del contraste de igualdad de medias. En el ANOVA de dos factores, en lugar de dos fuentes de variabilidad tenemos cuatro: una por el primer factor, otra por el segundo, otra por la interacción y otra más que contempla la variabilidad residual o variabilidad intragrupos. En el ejemplo anterior, las cuatro fuentes de variabilidad son:\n\nLa debida al primer factor: la dieta.\nLa debida al segundo factor: el fármaco.\nLa debida a la interacción entre ambos.\nLa residual.\n\nLas tres primeras fuentes de variabilidad llevan asociadas sus correspondientes sumas de cuadrados, similares a la suma de cuadrados inter del ANOVA de una vía, mientras que la variabilidad residual lleva asociada su suma de cuadrados residual, similar a la suma de cuadrados intra del ANOVA de una vía. Dividiendo las sumas de cuadrados entre sus respectivos grados de libertad se obtienen varianzas, que divididas entre la varianza residual generan, bajo la hipótesis nula de igualdad de medias, valores f de la distribución F de Fisher que pueden utilizarse para calcular el p-valor del correspondiente contraste.\nLo anterior se resume en forma de tabla de un ANOVA de dos vías, considerando un primer factor con \\(k_1\\) niveles, un segundo factor con \\(k_2\\) niveles y un total de datos \\(n\\). Si se denomina F1 al primer factor, F2 al segundo, I a la interacción y R al residual, la tabla de un ANOVA de dos vías tiene la siguiente forma:\n\n\n\n\nFuente\nSuma Cuad\nGrad Lib\nCuad Medios\nf\n\\(p\\)-valor\n\n\n\n\nF1\n\\(SF1\\)\n\\(GF1=k_1-1\\)\n\\(CF1=\\dfrac{{SF1}}{{GF1}}\\)\n\\(f1=\\dfrac{{CF1}}{{CR}}\\)\n\\(P(F&gt;f1)\\)\n\n\nF2\n\\(SF2\\)\n\\(GF2=k_2-1\\)\n\\(CF2=\\dfrac{{SF2}}{{GF2}}\\)\n\\(f2=\\dfrac{{CF2}}{{CR}}\\)\n\\(P(F&gt;f2)\\)\n\n\nI\n\\(SI\\)\n\\(GI=GF1 \\cdot GF2\\)\n\\(CI=\\dfrac{{SI}}{{GI}}\\)\n\\(fI=\\dfrac{{CI}}{{CR}}\\)\n\\(P(F&gt;fI)\\)\n\n\nR\n\\(SR\\)\n\\(GR=n-1-GF1-GF2-GI\\)\n\\(CR=\\dfrac{{SR}}{{GR}}\\)\n\n\n\n\nTotal\n\\(ST\\)\n\\(GT=n-1\\)\n\n\n\n\n\n\n\nUna vez obtenida la tabla, habitualmente mediante un programa de estadística para evitar realizar la gran cantidad de cálculos que conlleva (los distintos programas pueden proporcionar tablas ligeramente diferentes a la expuesta en esta práctica, en las que pueden aparecer filas añadidas cuya interpretación dependerá del programa utilizado), el siguiente paso es la interpretación de los \\(p\\)-valores obtenidos en cada uno de los factores y en la interacción. Para ello, resulta clave el \\(p\\)-valor de la interacción porque condicionará completamente el análisis:\n\nSi la interacción no ha resultado significativa (\\(p\\)-valor de la interacción mayor que el nivel de significación, habitualmente \\(0.05\\)), se puede considerar por separado la actuación de los dos factores y ver si hay o no diferencias significativas en sus niveles atendiendo al \\(p\\)-valor que aparece en la tabla para cada uno de ellos. Por ejemplo, en la primera de las tablas del análisis de Kg perdidos en función de la dieta y el fármaco, se obtendría que la interacción no es significativa, lo cual implicaría que habría que analizar el efecto de los factores por separado. Para ello, se acudiría al \\(p\\)-valor del factor dieta y si es menor que el nivel de significación fijado, entonces el factor dieta habría resultado significativo, lo cual quiere decir que habría diferencias significativas (más allá de las asumibles por azar) entre los Kg perdidos por los individuos que hacen dieta y los que no; y todo ello, independientemente de si los individuos están tomando o no el fármaco, ya que no hay una interacción significativa que ligue los resultados de la dieta con el fármaco. Igualmente, con el factor fármaco, se acudiría a su \\(p\\)-valor y se vería si hay o no diferencias significativas entre los Kg perdidos por los que toman el fármaco y los que no lo hacen, independientemente de si siguen o no la dieta.\nSi la interacción ha resultado significativa (\\(p\\)-valor de la interacción menor que el nivel de significación, habitualmente \\(0.05\\)), no se puede considerar por separado la actuación de los dos factores, la presencia de uno de los factores condiciona lo que sucede en el otro y el análisis de diferencias debidas al segundo factor debe realizarse por separado dentro de cada uno de los niveles del primero; y a la inversa, el análisis de diferencias debidas al primero debe realizarse por separado dentro de cada uno de los niveles del segundo. Por ejemplo, en la segunda de las tablas del análisis de Kg perdidos en función de la dieta y el fármaco, muy probablemente se obtendría que la interacción sí que es significativa, con lo cual no habría un único efecto del fármaco: en el grupo de los que no toman el fármaco, la diferencia de Kg perdidos entre los que sí que hacen dieta y los que no la hacen no sería la misma que en el grupo de los que sí que toman el fármaco. E igualmente, tampoco habría un único efecto de la dieta: en el grupo de los que no hacen dieta, la diferencia de Kg perdidos entre los que sí que toman el fármaco y los que no lo hacen no sería la misma que en el grupo de los que sí que hacen dieta.\n\nUna aclaración final importante es que en ningún caso un ANOVA de dos factores con dos niveles en cada vía equivale a hacer por separado una T de Student de datos independientes en cada uno de los factores. Ni siquiera en el caso de que no haya interacción el \\(p\\)-valor que se obtiene en cada uno de los dos factores coincide con el que se obtendría en la comparación de los niveles mediante la T de Student. El ANOVA de dos factores es una técnica multivariante que cuantifica la influencia de cada una de las variables independientes en la variable dependiente después de haber eliminado la parte de la variabilidad que se debe a las otras variables independientes que forman parte del modelo. En el ejemplo de los Kg perdidos, no sería lo mismo analizar la influencia de la variable dieta después de eliminar la variabilidad explicada mediante la variable fármaco e incluso la interacción entre dieta y fármaco, que es lo que haría el ANOVA de dos factores, que analizar simplemente la influencia de la variable dieta sin más, o fármaco sin más, que es lo que podríamos hacer mediante una T de Student de datos independientes. Tampoco el análisis de la interacción en el ANOVA de dos factores equivale a realizar un ANOVA de una vía considerando una nueva variable independiente con cuatro categorías diferentes (1:Sí-Sí, 2:Sí-No, 3:No-Sí, 4:No-No), por el mismo motivo: las conclusiones del ANOVA de dos vías hay que entenderlas en el contexto de una técnica multivariante en que la importancia de cada variable independiente se obtiene después de eliminar de los datos la variabilidad debida a las demás.\n\n\n8.2.2 ANOVA de dos factores con tres o más niveles en algún factor\nEl planteamiento y resolución de un ANOVA de dos factores con tres o más niveles en algún factor es muy parecido al ya expuesto de dos niveles en cada factor. Únicamente cambian ligeramente las hipótesis nulas planteadas en los factores en las que habría que incluir la igualdad de tantas medias como niveles tenga el factor analizado, y las alternativas en las que se supone que alguna de las medias es diferente. En cuanto a las interacciones, también se contemplarían diferencias de medias pero teniendo en cuenta que hay más diferencias posibles al tener más niveles dentro de cada factor.\nEn cuanto a la interpretación final de los resultados de la tabla del ANOVA, si no hay interacción y sin embargo hay diferencias significativas en cualquiera de los factores con 3 o más niveles, el siguiente paso sería ver entre qué medias se dan esas diferencias. Por ejemplo, si no hay interacción y se ha rechazado la hipótesis nula de igualdad de medias entre los tres niveles del factor 1, habría que ver si esas diferencias aparecen entre los niveles 1 y 2, o entre el 1 y 3, e incluso entre el 2 y el 3, independientemente del factor 2; e igualmente con el factor 2. Para poder ver entre qué niveles hay diferencias, habría que realizar Test de Comparaciones Múltiples y por Parejas; por ejemplo un test de Bonferroni o cualquier otro de los vistos en la práctica de ANOVA de una vía. Si la interacción saliese significativa, habría que hacer lo mismo pero considerando las posibles diferencias entre los 3 niveles del factor 1 dentro de cada nivel del factor 2 y viceversa.\nComo ya se ha comentado para el ANOVA de dos factores con dos niveles en cada factor y la T de Student de datos independientes, igualmente el ANOVA de dos factores con tres o más niveles en algún factor no equivale a dos ANOVAS de una vía. El \\(p\\)-valor que se obtiene en el de dos factores no es el mismo que que se obtendría en los ANOVAS de una vía realizados teniendo en cuenta cada uno de los factores por separado,incluso si la interacción no es significativa.\n\n\n8.2.3 ANOVA de tres o más factores\nAunque los fundamentos del ANOVA de tres o más factores son muy parecidos a los de dos y la tabla obtenida es muy similar, la complejidad en la interpretación sube un escalón. Por ejemplo, en un ANOVA de tres factores la tabla presentaría los tres efectos de cada uno de los factores por separado, las tres interacciones dobles (1 con 2, 1 con 3 y 2 con 3), e incluso también podría mostrar la interacción triple (los programas de estadística permiten considerar o no las interacciones de cualquier orden). Si la interacción triple fuese significativa, entonces no se podría hablar del efecto general del factor 1, sino que habría que analizar el efecto del factor 1 dentro de cada nivel del 2 y a su vez dentro de cada nivel del 3, y así sucesivamente. Si la interacción triple no fuese significativa pero sí que lo fuese la del factor 1 con el 2, entonces habría que analizar el efecto del factor 1 dentro de cada uno de los niveles del 2 pero independientemente del factor 3. Y así hasta completar un conjunto muy grande de análisis posibles y de Test de Comparaciones Múltiples aplicados. No obstante, es el propio experimentador el que debe limitar el conjunto de análisis a realizar con un planteamiento muy claro del experimento, reduciendo en la medida de lo posible el número de factores considerados y teniendo claro que no merece la pena considerar interacciones triples, o de órdenes superiores, si no hay forma clara de interpretar su resultado.\nEn ningún caso un ANOVA de tres o más factores equivale a tres ANOVAS de una vía realizados teniendo en cuenta los factores considerados por separado.\n\n\n8.2.4 Factores fijos y Factores aleatorios\nA la hora de realizar un ANOVA de varios factores, el tratamiento de la variabilidad debida a cada uno de ellos y también las conclusiones que se pueden obtener después de realizarlo, son diferentes dependiendo de que los factores sean fijos o aleatorios.\nSe entiende como Factor Fijo o Factor de Efectos Fijos aquel cuyos niveles los establece, los fija de antemano, el investigador (por ejemplo, cantidades concretas de fármaco o de tiempo transcurrido), o vienen dados por la propia naturaleza del factor (por ejemplo, el sexo o la dieta). Su variabilidad es más fácil de controlar y también resulta más sencillo su tratamiento en los cálculos que hay que hacer para llegar a la tabla final del ANOVA, pero tienen el problema de que los niveles concretos que toma el factor constituyen la población de niveles sobre los que se hace inferencia. Es decir, no se pueden sacar conclusiones poblacionales que no se refieran a esos niveles fijos con los que se ha trabajado.\nPor contra, un Factor Aleatorio o Factor de Efectos Aleatorios es aquel cuyos niveles son seleccionados de forma aleatoria entre todos los posibles niveles del factor (por ejemplo, cantidad de fármaco, con niveles 23 mg, 132 mg y 245 mg, obtenidos al escoger 3 niveles de forma aleatoria entre 0 y 250 mg). Su tratamiento es más complicado, pero al constituir una muestra aleatoria de niveles, se pretende sacar conclusiones extrapolables a todos los niveles posibles.\n\n8.2.4.1 Supuestos del modelo de ANOVA de dos o más vías\nComo ya sucedía con el ANOVA de una vía, el de dos o más vías es un test paramétrico que supone que:\n\nLos qdatos deben seguir distribuciones normales dentro de cada categoría, entendiendo por categorías todas las que se forman del cruce de todos los niveles de todos los factores. Por ejemplo, en un ANOVA de 2 factores con 3 niveles en cada factor, se tienen \\(3^2\\) categorías diferentes.\nTodas las distribuciones normales deben tener igualdad de varianzas (homocedasticidad).\n\nCuando no se cumplen las condiciones anteriores y además las muestras son pequeñas, no se debería aplicar el ANOVA de dos o más vías, con el problema añadido de que no hay un test no paramétrico que lo sustituya. Mediante test no paramétricos (sobre todo mediante el test de Kruskall-Wallis) se podría controlar la influencia de cada uno de los factores por separado en los datos, pero nunca el importantísimo papel de la interacción.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análisis de la Varianza</span>"
    ]
  },
  {
    "objectID": "10-anova-ancova.html#anova-de-medidas-repetidas",
    "href": "10-anova-ancova.html#anova-de-medidas-repetidas",
    "title": "8  Análisis de la Varianza",
    "section": "8.3 ANOVA de medidas repetidas",
    "text": "8.3 ANOVA de medidas repetidas\nEn muchos problemas se cuantifica el valor de una variable dependiente en varias ocasiones en el mismo sujeto (por ejemplo: en un grupo de individuos que están siguiendo una misma dieta, se puede anotar el peso perdido al cabo de un mes, al cabo de dos y al cabo de tres), y se intenta comparar la media de esa variable en las diferentes ocasiones en que se ha medido, es decir, ver si ha habido una evolución de la variable a lo largo de las diferentes medidas (en el ejemplo anterior, una evolución del peso perdido). Conceptualmente es una situación análoga a la estudiada al comparar dos medias con datos emparejados mediante una T de Student de datos emparejados, o su correspondiente no paramétrico, el test de Wilcoxon, pero ahora hay más de dos medidas emparejadas, realizadas en el mismo individuo. En estas situaciones se utiliza el ANOVA de medidas repetidas.\nEl ANOVA de medidas repetidas, como también sucede con cualquier otro test que utilice datos emparejados, tiene la ventaja de que las comparaciones que se realizan están basadas en lo que sucede dentro de cada sujeto (intra-sujetos), lo cual reduce el ruido o variabilidad que se produce en comparaciones entre diferentes grupos de sujetos. Por ejemplo, en el estudio sobre la evolución del peso perdido con personas que siguen la misma dieta, se podría haber cuantificado la variable al cabo de uno, dos y tres meses, pero en tres grupos diferentes que hubiesen seguido la misma dieta, pero con este diseño del estudio no se controlan otras variables que pueden influir en el resultado final, por ejemplo el sexo, la edad, o la cantidad de ejercicio que se hace al día. Dicho de otra forma, en el diseño con grupos independientes es posible que alguno de los grupos tenga una media de edad superior, o no haya igual número de hombres que de mujeres, y todo ello tener su reflejo en el número de Kg perdidos. Mientras que, con el diseño de datos emparejados, la segunda medida se compara con la primera que también se ha realizado en la misma persona, y por lo tanto es igual su sexo, su edad y la cantidad de deporte que realiza; y así con todas las demás medidas que se comparan entre sí pero dentro del mismo individuo. Eso permite controlar la variabilidad y detectar pequeñas diferencias que de otra forma serían indetectables.\n\n8.3.0.1 ANOVA de medidas repetidas como ANOVA de dos vías sin interacción\nEl ANOVA de medidas repetidas puede realizarse como un ANOVA de dos vías sin interacción sin más que realizar los cálculos oportunos introduciendo adecuadamente los datos en un programa estadístico.\nEn la situación de partida, si suponemos que tenemos \\(k\\) medidas emparejadas de una variable dependiente numérica y \\(n\\) individuos en los que hemos tomado las medidas, los datos se pueden organizar como aparecen en la tabla siguientes:\n\n\n\n\n2-5\nVarDep 1\nVarDep 2\n...\nVarDep k\n\n\n\n\nIndividuo 1\n\\(x_{1,1}\\)\n\\(x_{1,2}\\)\n...\n\\(x_{1,k}\\)\n\n\nIndividuo 2\n\\(x_{2,1}\\)\n\\(x_{2,2}\\)\n...\n\\(x_{2,k}\\)\n\n\n...\n...\n...\n...\n...\n\n\nIndividuo n\n\\(x_{n,1}\\)\n\\(x_{n,2}\\)\n...\n\\(x_{n,k}\\)\n\n\n\n\nPero esos mismos datos también se pueden ordenar en un formato de tabla mucho más conveniente para poderles aplicar un ANOVA de dos vías:\n\n\n\n\n2-4\nVar Dep\nIndividuo\nMedida\n\n\n\n\nFila 1\n\\(x_{1,1}\\)\n1\n1\n\n\nFila 2\n\\(x_{2,1}\\)\n2\n1\n\n\n...\n...\n...\n...\n\n\nFila n\n\\(x_{n,1}\\)\nn\n1\n\n\nFila n+1\n\\(x_{1,2}\\)\n1\n2\n\n\nFila n+2\n\\(x_{2,2}\\)\n2\n2\n\n\n...\n...\n...\n...\n\n\nFila 2n\n\\(x_{n,2}\\)\nn\n2\n\n\n...\n...\n...\n...\n\n\nFila (k-1)n+1\n\\(x_{1,k}\\)\n1\nk\n\n\nFila (k-1)n+2\n\\(x_{2,k}\\)\n2\nk\n\n\n...\n...\n...\n..\n\n\nFila kn\n\\(x_{n,k}\\)\nn\nk\n\n\n\n\nCon ello, tanto Individuo como Medida son variables categóricas que dividen la muestra total (\\(n\\cdot k\\) datos de la variable dependiente) en grupos: \\(n\\) grupos en la variable Individuo y \\(k\\) grupos en la variable Medida. Además, considerando el cruce de ambas variables (Medida x Individuo) se forman \\(n\\cdot k\\) grupos con un único dato de la variable dependiente en cada grupo.\nPara explicar la variabilidad de los datos de la variable dependiente cuantitativa se pueden considerar tres fuentes: la debida a la variable Medida, la debida a la variable Individuo, y la residual. Ahora no cabe hablar de la variabilidad debida a la interacción entre Medida e Individuo ya que los grupos que surgen del cruce de los dos factores sólo tienen un dato y no es viable calcular medias y dispersiones dentro de un grupo con un único dato. Y el análisis de la influencia de cada uno de los factores se realiza mediante un ANOVA de dos factores sin interacción, que genera la siguiente tabla:\n\n\n\n\nFuente\nSuma Cuad\nGrad Lib\nCuad Med\nF\np-valor\n\n\n\n\nF1=Medida\n\\(SF1\\)\n\\(GF1=k-1\\)\n\\(CF1=\\dfrac{SF1}{GF1}\\)\n\\(f1=\\dfrac{CF1}{CR}\\)\n\\(P(F&gt;f1)\\)\n\n\nF2=Individuo\n\\(SF2\\)\n\\(GF2=n-1\\)\n\\(CF2=\\dfrac{SF2}{GF2}\\)\n\\(f2=\\dfrac{CF2}{CR}\\)\n\\(P(F&gt;f2)\\)\n\n\nResidual\n\\(SR\\)\n\\(GR=(n \\cdot k)-1-GF1-GF2\\)\n\\(CR=\\dfrac{SR}{GR}\\)\n\n\n\n\nTotal\n\\(ST\\)\n\\(GT=(n\\cdot k)-1\\)\n\n\n\n\n\n\n\nY permite dar respuesta a los siguientes contrastes:\n\nEn la variable Medida:\n\\(H_0: \\mu_{\\text{Medida 1}}=\\mu_{\\text{Medida 2}}=...=\\mu_{\\text{Medida k}}\\)\n\\(H_1\\): Alguna de las medias es diferente.\nSi el \\(p\\)-valor obtenido es menor que el nivel de significación fijado querrá decir que alguna de las medias es significativamente diferente del resto. Este es el contraste más importante del ANOVA de medidas repetidas y supone que la variabilidad dentro de cada individuo (intra-sujeto) es lo suficientemente grande como para que se descarte el azar como su causa. Por lo tanto la variable Medida ha tenido un efecto significativo.\nEn la variable Individuo:\n\\(H_0: \\mu_{\\text{Individuo 1}}=\\mu_{\\text{Individuo 2}}=...=\\mu_{\\text{Individuo n}}\\)\n\\(H_1\\): Alguna de las medias es diferente.\nSi el \\(p\\)-valor obtenido es menor que el nivel de significación fijado querrá decir que alguna de las medias es significativamente diferente del resto, y por lo tanto alguno de los individuos analizados ha tenido un comportamiento en la variable dependiente diferente del resto. En realidad no es un contraste importante en el ANOVA de medidas repetidas ya que supone un análisis de la variabilidad entre individuos (inter-sujetos), pero es muy difícil que en un experimento dado esta variabilidad no esté presente.\n\nSi la conclusión del ANOVA es que hay que rechazar alguna de las dos hipótesis nulas, ya sea la de igualdad de medias en los grupos formados por la variable Medida o la de igualdad de medias en los grupos formados por la variable Individuo, entonces en el siguiente paso se podría aplicar un Test de Comparaciones Múltiples y por Parejas, por ejemplo un test de Bonferroni, para ver qué medias son diferentes, especialmente para ver entre qué niveles del la variable Medida se dan las diferencias.\n\n\n8.3.0.2 Supuestos del ANOVA de medidas repetidas\nComo en cualquier otro ANOVA, en el de medidas repetidas se exige que:\n\nLos datos de la variable dependiente deben seguir distribuciones normales dentro de cada grupo, ya sea formado por la variable Medida o por la variable Individuo. Como el contraste más importante se realiza en la variable Medida, resultará especialmente importante que sean normales las distribuciones de todas las Medidas.\nTodas las distribuciones normales deben tener igualdad de varianzas (homocedasticidad), especialmente las de las diferentes Medidas.\n\nCuando en un ANOVA de medidas repetidas se cumple la normalidad y la homocedasticidad de todas las distribuciones se dice que se cumple la Esfericidad de los datos, y hay tests estadísticos especialmente diseñados para contrastar la esfericidad como la prueba de Mauchly.\nCuando no se cumplen las condiciones anteriores y además las muestras son pequeñas, no se debería aplicar el ANOVA de medidas repetidas, pero al menos sí que hay una prueba no paramétrica que permite realizar el contraste de si hay o no diferencias significativas entre los distintos niveles de la variable Medida, que es el test de Friedman.\n\n\n8.3.1 ANOVA de medidas repetidas + ANOVA de una o más vías\nNo son pocos los problemas en los que, además de analizar el efecto intra-sujetos en una variable dependiente cuantitativa medida varias veces en los mismos individuos para el que cabría plantear un ANOVA de medidas repetidas, también aparecen variables cualitativas que se piensa que pueden estar relacionadas con la variable dependiente. Estas últimas variables introducen un efecto que aunque habitualmente es catalogado como inter-sujetos más bien se trataría de un efecto inter-grupos, yaque permiten definir grupos entre los que se podría plantear un ANOVA de una o más vías. Por ejemplo, se podría analizar la pérdida de peso en una muestra de individuos al cabo de uno, dos y tres meses de tratamiento (ANOVA de medidas repetidas), pero teniendo en cuenta que los individuos de la muestra han sido divididos en seis grupos que se forman por el cruce de dos factores, Dieta y Ejercicio, con tres dietas diferentes: a, b y c, y dos niveles de ejercicio físico diferentes: bajo y alto. Para analizar la influencia de estos dos factores inter-sujetos, habría que plantear un ANOVA de dos vías con interacción. Para un ejemplo como el comentado, aunque los datos podrían disponerse de una forma similar a la que permite realizar el ANOVA de medidas repetidas como un ANOVA de dos factores (variables Medida e Individuo), y añadirle dos factores más (Dieta y Ejercicio), no resulta cómodo tener que introducir en la matriz de datos varias filas para un mismo individuo (tantas como medidas repetidas diferentes se hayan realizado). Por ello, determinados programas de estadística, como PASW, permiten realizar ANOVAs de medidas repetidas introduciendo los datos en el formato clásico, una fila para cada individuo y una variable para cada una de las medidas repetidas, definiendo factores intra-sujeto que en realidad estarían compuestos por todas las variables que forman parte de las medidas repetidas. Además, a los factores intra-sujeto permiten añadirle nuevos factores inter-sujeto (categorías) que pueden influir en las variables respuesta (las diferentes medidas), e incluso comprobar si hay o no interacción entre los factores inter-sujeto entre sí y con los factores intra-sujeto. Por lo tanto, son procedimientos que realizan a la vez un ANOVA de medidas repetidas y un ANOVA de una o más vías, con la ventaja de que se pueden introducir los datos en la forma clásica: una fila para cada individuo.\nEl resultado de la aplicación de estos procedimientos es muy parecido a los comentados en apartados previos: se generan tablas de ANOVA en las que se calcula un \\(p\\)-valor para cada uno de los factores, ya sean intra-sujeto (medidas repetidas) o inter-sujeto (categorías), y también para la interacción, ya sea de los factores inter-sujeto entre sí o de factores inter-sujeto con los intra-sujeto.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análisis de la Varianza</span>"
    ]
  },
  {
    "objectID": "10-anova-ancova.html#análisis-de-la-covarianza-ancova",
    "href": "10-anova-ancova.html#análisis-de-la-covarianza-ancova",
    "title": "8  Análisis de la Varianza",
    "section": "8.4 Análisis de la covarianza: ANCOVA",
    "text": "8.4 Análisis de la covarianza: ANCOVA\nEl análisis de la covarianza, ANCOVA, es una extensión del ANOVA (ya sea de una o varias vías y de medidas repetidas), que permite analizar la influencia que sobre la variable dependiente cuantitativa tienen todas las variables independientes categóricas (factores) y las medidas repetidas contempladas en el ANOVA, pero, además, eliminando el efecto que otra u otras variables independientes cuantitativas podrían tener sobre la variable respuesta. Las variables independientes cuyo efecto se pretende eliminar (controlar o ajustar) son llamadas Covariables o Covariantes porque se se espera que covaríen, es decir, que estén correlacionadas con la variable dependiente.\nAunque la explicación detallada de cómo se realiza el ANCOVA va más allá del nivel de lo expuesto en esta práctica, la idea es sencilla: se puede plantear un análisis de regresión de la variable dependiente en función de la covariable (o de las covariables si hay más de una), y eliminar la parte de la variabilidad de la dependiente que se puede explicar gracias a la covariable sin más que trabajar con los residuos del modelo de regresión en lugar de con los datos originales. Posteriormente, se procede a realizar una ANOVA, de uno o varios factores e incluso de medidas repetidas, aplicado a los residuos.\nEl resultado final de la aplicación del ANCOVA es una tabla muy parecida a la del ANOVA, pero con una línea añadida por para cada una de las covariables. En esas líneas se recoge la cantidad de variabilidad explicada por cada una de las covariables y su correspondiente \\(p\\)-valor, que da respuesta al contraste de si la covariable es o no prescindible para explicar lo que sucede en la variable dependiente (en términos más técnicos, el contraste sería si la pendiente del modelo de regresión de la variable independiente en función de la covariable puede o no ser igual a 0). En la tabla del ANCOVA no hay ninguna línea añadida que contemple la posible interacción entre la covariable y los distintos factores inter-sujetos, simplemente porque si hubiese interacción no debería aplicarse un modelo de ANCOVA ya que el efecto del factor no podría estimarse porque dependería del valor concreto considerado en la covariable, que, por ser continua, tiene infinitos valores, luego habría infinitos diferentes efectos del factor y no se le podría asignar un \\(p\\)-valor concreto. Pero sí que la tabla añade una línea para la interacción de cada uno de los factores intra-sujetos con cada una de las covariables, ya que cada factor intra-sujetos internamente está compuesto por varias variables cuantitativas que pueden presentar diferentes pendientes en la regresión en función de la covariable.\nSi la representación gráfica habitual para ver si una serie de factores influyen o no en una variable respuesta cuantitativa (ANOVA) es el denominado gráfico de medias, en el ANCOVA el efecto de la covariable en la variable respuesta se puede ver mediante la nube puntos de la variable respuesta en función de la covariable, que presentará un aspecto más o menos rectilíneo dependiendo del nivel de correlación lineal entre ambas. Además, también se puede intuir si un determinado factor influye en la variable respuesta una vez eliminada la influencia de la covariable:\n\nSi la nube de puntos puede ajustarse adecuadamente mediante una única recta de pendente nula, independientemente de los niveles del factor, entonces quiere decir que ni la covariable ni el factor son significativos para explicar la variable respuesta.\nSi la nube de puntos se ajusta adecuadamente mediante una única recta de pendiente no nula, independientemente de los niveles del factor, entonces quiere decir que la covariable sí que es significativa pero no así el factor, ya que, una vez eliminada la influencia de la covariable (es decir, tomando como variable dependiente los residuos del ajuste inicial) no habría diferencias entre los distintos niveles (los puntos de las diferentes categorías quedarían a la misma altura).\nPor ejemplo, en la siguiente figura aparece el resultado de un experimento en el que se han anotado los Kg perdidos por personas que han seguido dos tipos diferentes de dieta, pero teniendo en cuenta como covariable el índice de masa corporal, que se piensa que también puede influir en el número de Kg perdidos pero que, sin embargo, no se ha controlado a la hora de elaborar los grupos y claramente han quedado desequilibrados en la covariable (los que han tomado la dieta 2 tienen en media mayor índice de masa corporal que los que han tomado la dieta 1). Según la figura, cabría esperar que haya diferencias significativas en los Kg perdidos según la dieta (parece que la dieta 2 hace perder más Kg que la dieta 1), pero en realidad todo se debe a la covariable, y eliminado su efecto (si la pendiente de la recta fuese 0) los dos grupos habrían perdido cantidades muy similares de peso. En definitiva, en similares condiciones de índice de masa corporal, la dieta 2 no haría perder más Kg.\n\n\n\nNube de puntos con covariable significativa y factor no significativo\n\n\nSi la nube de puntos se ajusta adecuadamente mediante varias rectas de pendiente nula, una por cada nivel del factor, entonces la covariable no es significativa, pero sí el factor.\nSi la nube de puntos se ajusta adecuadamente con rectas, una por cada nivel del factor, con igual pendiente no nula, y al menos una de las rectas es diferente de todas las demás (al menos uno de los niveles aparece desplazado), entonces tanto la covariable como el factor serían significativos a la hora de explicar la variable dependiente.\nPor ejemplo, en la siguiente figura aparece el resultado de un experimento similar al ya comentado: Kg perdidos en función de la dieta y de la covariable índice de masa corporal que no se ha controlado adecuadamente a la hora de hacer los grupos (el grupo de los que toman la dieta 2 tiene mayor IMC de partida). A la vista de la gráfica incluso parece que hay diferencias significativas en el número de Kg perdidos de tal forma que los de la dieta 2 haría perder más, pero todo es consecuencia de la covariable; eliminado su efecto, el número de Kg perdidos por los individuos que toman la dieta 1 es mayor (eliminada la pendiente de la recta, los puntos de la dieta 1 quedarían por arriba).\n\n\n\nNube de puntos con covariable significativa y factor también significativo\n\n\nSi la nube de puntos se ajusta adecuadamente con diferentes rectas, una por cada nivel del factor, con pendientes no nulas pero diferentes, entonces quiere decir que habría interacción entre covariable y factor y no debería plantearse un modelo de ANCOVA.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análisis de la Varianza</span>"
    ]
  }
]