{
  "hash": "d544a5f57a2e06fce9591e2b72fb94b1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Probabilidad\nlang: es\n---\n\n\n\n\n\n\nLa estadística descriptiva permite describir el comportamiento y las relaciones entre las variables en la muestra, pero no permite sacar conclusiones sobre el resto de la población.\n\nHa llegado el momento de dar el salto de la muestra a la población y pasar de la estadística descriptiva a la inferencia estadística, y el puente que lo permite es la **Teoría de la Probabilidad**.\n\nHay que tener en cuenta que el conocimiento que se puede obtener de la población a partir de la muestra es limitado, y que para obtener conclusiones válidas para la población la muestra debe ser\nrepresentativa de esta. Por esta razón, para garantizar la representatividad de la muestra, esta debe extraerse _aleatoriamente_, es decir, al _azar_.\n\nLa teoría de la probabilidad precisamente se encarga de controlar ese azar para saber hasta qué punto son fiables las conclusiones obtenidas a partir de una muestra.\n\n## Experimentos y sucesos aleatorios\n\nEl estudio de una característica en una población se realiza a través de experimentos aleatorios.\n\n:::{#def-experimento-aleatorio}\n## Experimento aleatorio\nUn *experimento aleatorio* es un experimento que cumple dos condiciones:\n\n1. El conjunto de posibles resultados es conocido.\n2. No se puede predecir con absoluta certeza el resultado del experimento.\n:::\n\n:::{#exm-experimento-aleatorio}\nUn ejemplo típico de experimentos aleatorios son los juegos de azar. El lanzamiento de un dado, por ejemplo, es un  experimento aleatorio ya que:\n\n1. Se conoce el conjunto posibles de resultados $\\{1,2,3,4,5,6\\}$.\n2. Antes de lanzar el dado, es imposible predecir con absoluta certeza el valor que saldrá.\n\nOtro ejemplo de experimento aleatorio sería la selección de un individuo de una población al azar y la determinación de su grupo sanguíneo.\n\nEn general, la obtención de cualquier muestra mediante procedimientos aleatorios será un experimento\naleatorio.\n:::\n\n:::{#def-espacio-muestral}\n## Espacio muestral\nAl conjunto $\\Omega$ de todos los posibles resultados de un\nexperimento aleatorio se le llama _espacio muestral_.\n:::\n\n:::{#exm-espacios-muestrales}\nAlgunos ejemplos de espacios muestrales son:\n\n- Lanzamiento de una moneda: $\\Omega=\\{c,x\\}$.\n- Lanzamiento de un dado: $\\Omega=\\{1,2,3,4,5,6\\}$.\n- Grupo sanguíneo de un individuo seleccionado al azar:\n$\\Omega=\\{\\mbox{A},\\mbox{B},\\mbox{AB},\\mbox{0}\\}$.\n- Estatura de un individuo seleccionado al azar:\n$\\Omega=\\mathbb{R}^+$.\n:::\n\nEn experimentos donde se mide más de una variable, la determinación del espacio muestral puede resultar compleja. En tales casos es recomendable utilizar un para construir el espacio muestral.\n\nEn un diagrama de árbol cada variable se representa en un nivel del árbol y cada posible valor de la variable como una rama.\n\n:::{#exm-diagrama-arbol}\nEl siguiente diagrama de árbol representa el espacio muestral de un experimento aleatorio en el que se mide el sexo y el grupo sanguíneo de un individuo al azar.\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de árbol del espacio muestral del sexo y el grupo sanguineo.](img/probabilidad/espacio_muestral.svg){width=500}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de árbol del espacio muestral del sexo y el grupo sanguineo.](img/probabilidad/espacio_muestral.pdf)\n:::\n:::\n\n:::{#def-suceso-aleatorio}\n## Suceso aleatorio\nUn *suceso aleatorio* es cualquier subconjunto del espacio muestral $\\Omega$ de un experimento aleatorio.\n:::\n\nExisten distintos tipos de sucesos:\n\n- **Suceso imposible**: Es el suceso vacío $\\emptyset$. Este suceso nunca ocurre.\n- **Sucesos elementales**: Son los sucesos formados por un solo elemento.\n- **Sucesos compuestos**: Son los sucesos formados por dos o más elementos.\n- **Suceso seguro**: Es el suceso que contiene el propio espacio muestral $\\Omega$. Este suceso siempre ocurre.\n\n:::{#exm-sucesos-aleatorios}\nEn el experimento aleatorio del lanzamiento de un dado, con espacio muestral $\\Omega=\\{1, 2, 3, 4, 5, 6\\}$, el subconjunto $\\{2, 4, 6\\}$ es un suceso aleatorio que se cumple cuando sale un número par, y el subconjunto $\\{1, 2, 3, 4\\}$ es un suceso aleatorio que se cumple cuando sale un número menor que 5.\n:::\n\n### Espacio de sucesos\n\n:::{#def-espacio-sucesos}\n## Espacio de sucesos\nDado un espacio muestral $\\Omega$ de un experimento aleatorio, el conjunto formado por todos los posibles sucesos de $\\Omega$ se llama *espacio de sucesos de $\\Omega$* y se denota $\\mathcal{P}(\\Omega)$.\n:::\n\n:::{#exm-espacio-sucesos}\nDado el espacio muestral $\\Omega=\\{a,b,c\\}$, su espacio de sucesos es\n\n$$\\mathcal{P}(\\Omega)=\\left\\{\\emptyset, \\{a\\},\\{b\\},\\{c\\},\\{a,b\\},\\{a,c\\},\\{b,c\\},\\{a,b,c\\}\\right\\}$$\n:::\n\nPuesto que los sucesos son conjuntos, por medio de la teoría de conjuntos se pueden definir las siguientes operaciones entre sucesos:\n\n- Unión.\n- Intersección.\n- Complementario.\n- Diferencia.\n\n### Unión de suscesos\n\n:::{#def-union-sucesos}\n## Suceso unión\nDados dos sucesos $A,B\\subseteq \\Omega$, se llama _suceso unión_ de $A$ y $B$, y se denota $A\\cup B$, al suceso formado por los elementos de $A$ junto a los elementos de $B$, es decir, \n\n$$A\\cup B = \\{x\\,|\\, x\\in A\\mbox{ o }x\\in B\\}.$$\n:::\n\n:::{.content-visible when-format=\"html\"}\n![Union de dos sucesos.](img/probabilidad/union.svg){width=400}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Union de dos sucesos.](img/probabilidad/union.pdf)\n:::\n\nEl suceso unión $A\\cup B$ ocurre siempre que ocurre $A$ <span style=\"color:red;\">o</span> $B$.\n\n:::{#exm-union-sucesos}\nDado el espacio muestral correspondiente al lanzamiento de un dado $\\Omega=\\{1,2,3,4,5,6\\}$ y los sucesos $A=\\{2,4,6\\}$ y $B=\\{1,2,3,4\\}$, la unión de $A$ y $B$ es $A\\cup B=\\{1,2,3,4,6\\}$.\n:::\n\n### Intersección de sucesos\n\n:::{#def-interseccion-sucesos}\n## Suceso intersección\nDados dos sucesos $A,B\\subseteq \\Omega$, se llama _suceso intersección_ de $A$ y $B$, y se denota $A\\cap B$, al suceso formado por los elementos comunes de $A$ y $B$, es decir,\n\n$$A\\cap B = \\{x\\,|\\, x\\in A\\mbox{ y }x\\in B\\}.$$\n:::\n\n:::{.content-visible when-format=\"html\"}\n![Intersección de dos sucesos.](img/probabilidad/interseccion.svg){width=400}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Intersección de dos sucesos.](img/probabilidad/interseccion.pdf)\n:::\n\nEl suceso intersección $A\\cap B$ ocurre siempre que ocurren $A$ <span style=\"color:red;\">y</span> $B$.\n\nDiremos que dos sucesos son **incompatibles** si su intersección es vacía.\n\n:::{#exm-interseccion-sucesos}\nDado el espacio muestral correspondiente al lanzamiento de un dado $\\Omega=\\{1,2,3,4,5,6\\}$ y los sucesos $A=\\{2,4,6\\}$ y $B=\\{1,2,3,4\\}$, la intersección de $A$ y $B$ es $A\\cap B=\\{2,4\\}$, y por tanto, se trata de sucesos compatibles. Sin embargo, el suceso $C=\\{1, 3\\}$ es incompatible con $A$ ya que $A\\cap C=\\emptyset$.\n:::\n\n### Contrario de un suceso\n\n:::{#def-contrario-suceso}\n## Suceso contrario\nDado suceso $A\\subseteq \\Omega$, se llama _suceso contrario_ o _complementario_ de $A$, y se denota $\\overline A$, al suceso formado por los elementos de $\\Omega$ que no pertenecen a $A$, es decir,\n\n$$\\overline A = \\{x\\,|\\, x\\not\\in A\\}.$$\n:::\n\n:::{.content-visible when-format=\"html\"}\n![Contrario de un suceso.](img/probabilidad/contrario.svg){width=400}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Contrario de un suceso.](img/probabilidad/contrario.pdf)\n:::\n\nEl suceso contrario $\\overline A$ ocurre siempre que <span style=\"color:red;\">no</span> ocurre $A$.\n\n:::{#exm-contrario-suceso}\nDado el espacio muestral correspondiente al lanzamiento de un dado $\\Omega=\\{1,2,3,4,5,6\\}$ y los sucesos $A=\\{2,4,6\\}$ y $B=\\{1,2,3,4\\}$, el contrario de $A$ es $\\overline A=\\{1,3,5\\}$.\n:::\n\n### Diferencia de sucesos\n\n:::{#def-diferencia-sucesos}\n## Suceso diferencia\nDados dos sucesos $A,B\\subseteq \\Omega$, se llama _suceso diferencia_ de $A$ y $B$, y se denota $A-B$, al suceso formado por los elementos de $A$ que no pertenecen a $B$, es decir,\n\n$$A-B = \\{x\\,|\\, x\\in A\\mbox{ y }x\\not\\in B\\} = A \\cap \\overline B.$$\n:::\n\n:::{.content-visible when-format=\"html\"}\n![Diferencia de sucesos.](img/probabilidad/diferencia.svg){width=400}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diferencia de sucesos.](img/probabilidad/diferencia.pdf)\n:::\n\nEl suceso diferencia $A-B$ ocurre siempre que ocurre $A$ pero no ocurre $B$, y también puede expresarse como $A\\cap \\bar B$.\n\n:::{#exm-diferencia-sucesos}\nDado el espacio muestral correspondiente al lanzamiento de un dado $\\Omega=\\{1,2,3,4,5,6\\}$ y los sucesos $A=\\{2,4,6\\}$ y $B=\\{1,2,3,4\\}$, la diferencia de $A$ y $B$ es $A-B=\\{6\\}$, y la diferencia de $B$ y $A$ es $B-A=\\{1,3\\}$.\n:::\n\n### Álgebra de sucesos\n\nDados los sucesos $A,B,C\\in  \\mathcal{P}(\\Omega)$, se cumplen las\nsiguientes propiedades:\n\n1. $A\\cup A=A$, $A\\cap A=A$ (idempotencia).\n2. $A\\cup B=B\\cup A$, $A\\cap B = B\\cap A$ (conmutativa).\n3. $(A\\cup B)\\cup C = A\\cup (B\\cup C)$, $(A\\cap B)\\cap C = A\\cap (B\\cap C)$ (asociativa).\n4. $(A\\cup B)\\cap C = (A\\cap C)\\cup (B\\cap C)$, $(A\\cap B)\\cup C = (A\\cup C)\\cap (B\\cup C)$ (distributiva).\n5. $A\\cup \\emptyset=A$, $A\\cap E=A$ (elemento neutro).\n6. $A\\cup E=E$, $A\\cap \\emptyset=\\emptyset$ (elemento absorbente).\n7. $A\\cup \\overline A = E$, $A\\cap \\overline A= \\emptyset$ (elemento simétrico complementario).\n8. $\\overline{\\overline A} = A$ (doble contrario).\n9. $\\overline{A\\cup B} = \\overline A\\cap \\overline B$, $\\overline{A\\cap B} = \\overline A\\cup \\overline B$ (leyes de Morgan).\n10. $A\\cap B\\subseteq A\\cup B$.\n\n## Definición de probabilidad\n\n### Definición clásica de probabilidad\n\n:::{#def-laplace}\n## Probabilidad - Laplace\nDado un espacio muestral $\\Omega$ de un experimento aleatorio donde todos los elementos de $\\Omega$ son equiprobables, la _probabilidad_ de un suceso $A\\subseteq \\Omega$ es el cociente entre el número de elementos de $A$ y el número de elementos de $\\Omega$ \n\n$$P(A) = \\frac{|A|}{|\\Omega|} = \\frac{\\mbox{nº casos favorables a A}}{\\mbox{nº casos posibles}}$$\n:::\n\nEsta definición es ampliamente utilizada, aunque tiene importantes restricciones:\n\n  - Es necesario que todos los elementos del espacio muestral tengan la\n    misma probabilidad de ocurrir (_equiprobabilidad_).\n\n  - No puede utilizarse con espacios muestrales infinitos, o de los que\n    no se conoce el número de casos posibles.\n\n:::{.callout-caution}\nEsto no se cumple en muchos experimentos aleatorios reales.\n:::\n\n:::{#exm-experimento-sin-equiprobabilidad}\nDado el espacio muestral correspondiente al lanzamiento de un dado $\\Omega=\\{1,2,3,4,5,6\\}$ y el suceso $A=\\{2,4,6\\}$, la probabilidad de $A$ es\n\n$$P(A) = \\frac{|A|}{|\\Omega|} = \\frac{3}{6} = 0.5.$$\n\nSin embargo, si se considera el espacio muestral correspondiente a observar el grupo sanguíneo de un individuo al azar, $\\Omega=\\{O,A,B,AB\\}$, no se puede usar la definición clásica de probabilidad para calcular la probabilidad de que tenga grupo sanguíneo $A$,\n\n$$P(A) \\neq \\frac{|A|}{|\\Omega|} = \\frac{1}{4} = 0.25,$$\n\nya que los grupos sanguíneos no son igualmente probables en las poblaciones humanas.\n:::\n\n### Definición frecuentista de probabilidad\n\n:::{#thm-ley-grandes-numeros}\n## Ley de los grandes números\nCuando un experimento aleatorio se repite un gran número de veces, las frecuencias relativas de los sucesos del experimento tienden a estabilizarse en torno a cierto número, que es precisamente su probabilidad.\n:::\n\nDe acuerdo al teorema anterior, podemos dar la siguiente definición\n\n:::{#def-probabilidad-frecuentista}\n## Probabilidad frecuentista\nDado un espacio muestral $\\Omega$ de un experimento aleatorio\nreproducible, la *probabilidad* de un suceso $A\\subseteq \\Omega$ es la frecuencia relativa del suceso $A$ en infinitas repeticiones del experimento \n\n$$P(A) = lim_{n\\rightarrow \\infty}\\frac{n_{A}}{n}$$\n:::\n\nAunque esta definición es muy útil en experimentos científicos reproducibles, también tiene serios inconvenientes, ya que\n\n  - Sólo se calcula una aproximación de la probabilidad real.\n  - La repetición del experimento debe ser en las mismas condiciones.\n\n:::{#exm-probabilidad-frecuentista}\nDado el espacio muestral correspondiente al lanzamiento de una moneda $\\Omega=\\{C,X\\}$, si después de lanzar la moneda 100 veces obtenemos 54 caras, entonces la probabilidad de $C$ es aproximadamente\n\n$$P(C) = \\frac{n_C}{n} = \\frac{54}{100} = 0.54.$$\n\nSi se considera el espacio muestral correspondiente a observar el grupo sanguíneo de un individuo al azar, $\\Omega=\\{O,A,B,AB\\}$, si se toma una muestra aleatoria de 1000 personas y se observa que 412 tienen grupo sanguíneo $A$, entonces la probabilidad del grupo sanguíneo $A$ es aproximadamente\n\n$$P(A) = \\frac{n_A}{n} = \\frac{412}{1000} = 0.412.$$\n:::\n\n### Definición axiomática de probabilidad\n\n:::{#def-probabilidad-kolmogorov}\n## Probabilidad - Kolmogórov\nDado un espacio muestral $\\Omega$ de un experimento aleatorio, una función de *probabilidad* es una aplicación que asocia a cada suceso $A\\subseteq \\Omega$ un número real $P(A)$, conocido como probabilidad de $A$, que cumple los siguientes axiomas:\n\n1.  La probabilidad de un suceso cualquiera es positiva o nula, \n    $$P(A)\\geq 0.$$\n2.  La probabilidad del suceso seguro es igual a la unidad, \n    $$P(\\Omega)=1.$$\n3.  La probabilidad de la unión de dos sucesos incompatibles ($A\\cap B=\\emptyset$) es igual a la suma de las probabilidades de cada uno de ellos, \n    $$P(A\\cup B) = P(A)+P(B).$$\n:::\n\n:::{#thm-consecuencias-axiomas-probabilidad}\nSi $P$ es una función de de probabilidad de un espacio muestral $\\Omega$, entonces para cualesquiera sucesos $A, B\\in \\Omega$, se cumple\n\n1.  $P(\\overline A) = 1-P(A)$.\n2.  $P(\\emptyset)= 0$.\n3.  Si $A\\subseteq B$ entonces $P(A)\\leq P(B)$.\n4.  $P(A) \\leq 1$.\n5.  $P(A-B) = P(A)-P(A\\cap B)$.\n6.  Si $A$ y $B$ son sucesos compatibles, es decir, su intersección no es vacía, entonces \n\n    $$P(A\\cup B)= P(A) + P(B) - P(A\\cap B).$$\n\n7.  Si el suceso $A$ está compuesto por los sucesos elementales\n    $e_1,e_2,...,e_n$, entonces \n    \n    $$P(A)=\\sum_{i=1}^n P(e_i).$$\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Demostración\n:::{.proof}\n1.  $\\overline A = \\Omega \\Rightarrow P(A\\cup \\overline A) = P(\\Omega) \\Rightarrow P(A)+P(\\overline A) = 1 \\Rightarrow P(\\overline A)=1-P(A)$.\n\n2.  $\\emptyset = \\overline \\Omega \\Rightarrow P(\\emptyset) = P(\\overline \\Omega) = 1-P(\\Omega) = 1-1 = 0.$\n\n3.  $B = A\\cup (B-A)$. Como $A$ y $B-A$ son incompatibles, $P(B) = P(A\\cup (B-A)) = P(A)+P(B-A) \\geq P(A).$\n    \n    Si pensamos en probabilidades como áreas, es fácil de ver gráficamente,\n\n    :::{.content-visible when-format=\"html\"}\n    ![Probabilidad de un suceso incluido en otro.](img/probabilidad/probabilidad_inclusion.svg){width=400}\n    :::\n\n    :::{.content-visible unless-format=\"html\"}\n    ![Probabilidad de un suceso incluido en otro.](img/probabilidad/probabilidad_inclusion.pdf)\n    :::\n    \n4.  $A\\subseteq \\Omega \\Rightarrow P(A)\\leq P(\\Omega)=1.$\n\n5.  $A=(A-B)\\cup (A\\cap B)$. Como $A-B$ y $A\\cap B$ son incompatibles, $P(A)=P(A-B)+P(A\\cap B) \\Rightarrow P(A-B)=P(A)-P(A\\cap B)$.\n    \n    Si pensamos en probabilidades como áreas, es fácil de ver gráficamente,\n\n    :::{.content-visible when-format=\"html\"}\n    ![Probabilidad de la diferencia de dos sucesos.](img/probabilidad/probabilidad_diferencia.svg){width=400}\n    :::\n\n    :::{.content-visible unless-format=\"html\"}\n    ![Probabilidad de la diferencia de dos sucesos.](img/probabilidad/probabilidad_diferencia.pdf)\n    :::\n    \n6.  $A\\cup B= (A-B) \\cup (B-A) \\cup (A\\cap B)$. Como $A-B$, $B-A$ y $A\\cap B$ son incompatibles, $P(A\\cup\n    B)=P(A-B)+P(B-A)+P(A\\cap B) = P(A)-P(A\\cap B)+P(B)-P(A\\cap B)+P(A\\cap B)= P(A)+P(B)-P(A\\cup B)$.\n    \n    Si pensamos en probabilidades como áreas, es fácil de ver gráficamente,\n    \n    :::{.content-visible when-format=\"html\"}\n    ![Probabilidad de la unión de dos sucesos.](img/probabilidad/probabilidad_union.svg){width=400}\n    :::\n\n    :::{.content-visible unless-format=\"html\"}\n    ![Probabilidad de la unión de dos sucesos.](img/probabilidad/probabilidad_union.pdf)\n    :::\n    \n7.  $A=\\{e_1,\\cdots,e_n\\} = \\{e_1\\}\\cup \\cdots \\cup \\{e_n\\} \\Rightarrow$ $P(A)=P(\\{e_1\\}\\cup \\cdots \\cup \\{e_n\\}) = P(\\{e_1\\})+ \\cdots P(\\{e_n\\}).$\n:::\n::: \n\n### Interpretación de la probabilidad\n\nComo ha quedado claro en los axiomas anteriores, la probabilidad de un evento $A$ es un número real $P(A)$ que está siempre entre 0 y 1.\n\nEn cierto modo, este número expresa la verosimilitud del evento, es decir, la confianza que hay en que ocurra $A$ en el experimento. Por tanto, también nos da una medida de la incertidumbre sobre el suceso.\n\n- La mayor incertidumbre corresponde a $P(A)=0.5$ (Es tan probable que ocurra $A$ como que no ocurra).\n\n- La menor incertidumbre corresponde a $P(A)=1$ ($A$ sucederá con absoluta certeza) y $P(A)=0$ ($A$ no sucederá con absoluta certeza).\n\nCuando $P(A)$ está más próximo a 0 que a 1, la confianza en que no ocurra $A$ es mayor que la de que ocurra $A$. Por el contrario, cuando $P(A)$ está más próximo a 1 que a 0, la confianza en que ocurra\n$A$ es mayor que la de que no ocurra $A$.\n\n## Probabilidad condicionada\n\n### Experimentos condicionados\n\nEn algunas ocasiones, es posible que tengamos alguna información sobre el experimento antes de su realización. Habitualmente esa información se da en forma de un suceso $B$ del mismo espacio muestral que sabemos que es cierto antes de realizar el experimento.\n\nEn tal caso se dice que el suceso $B$ es un suceso _condicionante_, y la probabilidad de otro suceso $A$ se conoce como y se expresa \n\n$$P(A|B).$$\n\nEsto debe leerse como _probabilidad de $A$ dado $B$_ o _probabilidad de $A$ bajo la condición de $B$_.\n\nLos condicionantes suelen cambiar el espacio muestral del experimento y por tanto las probabilidades de sus sucesos.\n\n:::{#exm-sucesos-condicionados}\nSupongamos que tenemos una muestra de 100 hombres y 100 mujeres con las siguientes frecuencias \n\n$$\n\\begin{array}{|c|c|c|}\n\\hline \n & \\mbox{No fumadores} & \\mbox{Fumadores} \\\\\n \\hline \n \\mbox{Mujeres} & 80 & 20 \\\\\n \\hline\n \\mbox{Hombres} & 60 & 40 \\\\\n \\hline\n\\end{array}\n$$ \n\nEntonces, usando la definición frecuentista de probabilidad, la probabilidad de que una persona elegida al azar sea fumadora es \n\n$$P(\\mbox{Fumadora})= \\frac{60}{200}=0.3.$$\n\nSin embargo, si se sabe que la persona elegida es mujer, entonces la muestra se reduce a la primera fila, y la probabilidad de ser fumadora es \n\n$$P(\\mbox{Fumadora}|\\mbox{Mujer})=\\frac{20}{100}=0.2.$$\n:::\n\n### Probabilidad condicionada\n\n:::{#def-probabilidad-condicionada}\n## Probabilidad condicionada\nDado un espacio muestral $\\Omega$ de un experimento aleatorio, y dos dos sucesos $A,B\\subseteq \\Omega$, la probabilidad de $A$ _condicionada_ por $B$ es \n\n$$P(A|B) = \\frac{P(A\\cap B)}{P(B)},$$\n\nsiempre y cuando, $P(B)\\neq 0$.\n:::\n\nEsta definición permite calcular probabilidades sin tener que alterar el espacio muestral original del experimento.\n\n:::{#exm-probabilidad-condicionada}\nEn el ejemplo anterior\n\n$$P(\\mbox{Fumadora}|\\mbox{Mujer})= \\frac{P(\\mbox{Fumadora}\\cap \\mbox{Mujer})}{P(\\mbox{Mujer})} =  \\frac{20/200}{100/200}=\\frac{20}{100}=0.2.$$\n:::\n\n### Probabilidad del suceso intersección\n\nA partir de la definición de probabilidad condicionada es posible obtener la fórmula para calcular la probabilidad de la intersección de dos sucesos. \n\n$$P(A\\cap B) = P(A)P(B|A) = P(B)P(A|B).$$\n\n:::{#exm-probabilidad-interseccion}\nEn una población hay un 30% de fumadores y se sabe que el 40% de los fumadores tiene cáncer de pulmón. La probabilidad de que una persona elegida al azar sea fumadora y tenga cáncer de pulmón es\n\n$$P(\\mbox{Fumadora}\\cap \\mbox{Cáncer})= P(\\mbox{Fumadora})P(\\mbox{Cáncer}|\\mbox{Fumadora}) = 0.3\\times 0.4 = 0.12.$$\n:::\n\n### Independencia de sucesos\n\nEn ocasiones, la ocurrencia del suceso condicionante no cambia la probabilidad original del suceso principal.\n\n:::{#def-sucesos-independientes}\n## Sucesos independientes\nDado un espacio muestral $\\Omega$ de un experimento aleatorio, dos sucesos $A,B\\subseteq \\Omega$ son _independientes_ si la probabilidad de $A$ no se ve alterada al condicionar por $B$, y viceversa, es decir, \n\n$$P(A|B) = P(A) \\quad \\mbox{and} \\quad P(B|A)=P(B),$$ \n\nsi $P(A)\\neq 0$ y $P(B)\\neq 0$.\n:::\n\nEsto significa que la ocurrencia de uno evento no aporta información relevante para cambiar la incertidumbre sobre el otro.\n\nCuando dos eventos son independientes, la probabilidad de su intersección es igual al producto de sus probabilidades,\n\n$$P(A\\cap B) = P(A)P(B).$$\n\n## Espacio probabilístico\n\n:::{#def-espacio-probabilistico}\n## Espacio probabilístico\nUn *espacio probabilístico* de un experimento aleatorio es una terna $(\\Omega,\\mathcal{F},P)$ donde\n\n  - $\\Omega$ es el espacio muestral del experimento.\n  - $\\mathcal{F}$ es un un conjunto de sucesos del experimento.\n  - $P$ es una función de probabilidad.\n:::\n\nSi conocemos la probabilidad de todos los elementos de $\\Omega$, entonces podemos calcular la  probabilidad de cualquier suceso en $\\mathcal{F}$ y se puede construir fácilmente el espacio probabilístico.\n\nPara determinar la probabilidad de cada suceso elemental se puede utilizar un diagrama de árbol, mediante las siguientes reglas:\n\n1.  Para cada nodo del árbol, etiquetar la rama que conduce hasta él con la probabilidad de que la variable en ese nivel tome el valor del nodo, condicionada por los sucesos correspondientes a sus nodos antecesores en el árbol.\n\n2.  La probabilidad de cada suceso elemental en las hojas del árbol es el producto de las probabilidades de las ramas que van desde la raíz a la hoja del árbol.\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de árbol de un espacio probabilístico.](img/probabilidad/espacio_probabilistico.svg){width=700}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de árbol de un espacio probabilístico.](img/probabilidad/espacio_probabilistico.pdf)\n:::\n\n### Árboles de probabilidad con variables dependientes\n\n:::{#exm-arbol-probabilidad-variables-dependientes}\nSea una población en la que el 30% de las personas fuman, y que la incidencia del cáncer de pulmón en fumadores es del 40% mientras que en los no fumadores es del 10%.\n\nEl espacio probabilístico del experimento aleatorio que consiste en elegir una persona al azar y medir las variables Fumar y Cáncer de pulmón se muestra a continuación.\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de árbol del espacio probabilístico de fumar y tener cáncer de pulmón.](img/probabilidad/espacio_probabilistico_fumar_cancer.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de árbol del espacio probabilístico de fumar y tener cáncer de pulmón.](img/probabilidad/espacio_probabilistico_fumar_cancer.pdf)\n:::\n:::\n\n### Árboles de probabilidad con variables independientes\n\n:::{#exm-arbol-probabilidad-variables-independientes}\nEl árbol de probabilidad asociado al experimento aleatorio que consiste en el lanzamiento de dos monedas se muestra a continuación.\n\n:::{.content-visible when-format=\"html\"}\n![Diágrama de árbol del espacio probabilístico del lanzamiento de dos monedas.](img/probabilidad/espacio_probabilistico_monedas.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diágrama de árbol del espacio probabilístico del lanzamiento de dos monedas.](img/probabilidad/espacio_probabilistico_monedas.pdf)\n:::\n:::\n\n:::{#exm-arbol-probabilidad-variables-independientes-2}\nDada una población en la que hay un 40% de hombres y un 60% de mujeres, el experimento aleatorio que consiste en tomar una muestra aleatoria de tres personas tiene el árbol de probabilidad que se muestra a continuación.\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de árbol del espacio probabilístico del sexo de tres individuos elegidos al azar.](img/probabilidad/espacio_probabilistico_muestra.svg){width=700}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de árbol del espacio probabilístico del sexo de tres individuos elegidos al azar.](img/probabilidad/espacio_probabilistico_muestra.pdf)\n:::\n:::\n\n## Teorema de la probabilidad total\n\n:::{#def-sistema-completo-sucesos}\n## Sistema completo de sucesos\nUna colección de sucesos $A_1,A_2,\\ldots,A_n$ de un mismo espacio muestral $\\Omega$ es un _sistema completo_ si cumple las siguientes condiciones:\n\n1.  La unión de todos es el espacio muestral:\n    $A_1\\cup \\cdots\\cup A_n =\\Omega$.\n\n2.  Son incompatibles dos a dos: $A_i\\cap A_j = \\emptyset$\n    $\\forall i\\neq j$.\n:::\n\n:::{.content-visible when-format=\"html\"}\n![Partición del espacio muestral en un sistema completo de sucesos.](img/probabilidad/particion_espacio_muestral.svg){width=300}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Partición del espacio muestral en un sistema completo de sucesos.](img/probabilidad/particion_espacio_muestral.pdf)\n:::\n\nEn realidad un sistema completo de sucesos es una partición del espacio muestral de acuerdo a algún atributo, como por ejemplo el sexo o el grupo sanguíneo.\n\n### Teorema de la probabilidad total\n\nConocer las probabilidades de un determinado suceso en cada una de las partes de un sistema completo puede ser útil para calcular su probabilidad.\n\n:::{#thm-probabilidad-total}\n## Probabilidad total\nDado un sistema completo de sucesos $A_1,\\ldots,A_n$ y un suceso $B$ de un espacio muestral $\\Omega$, la probabilidad de cualquier suceso $B$ del espacio muestral se puede calcular mediante la fórmula \n\n$$P(B) = \\sum_{i=1}^n P(A_i\\cap B) = \\sum_{i=1}^n P(A_i)P(B|A_i).$$\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Demostración\n:::{.proof}\nLa demostración del teorema es sencilla, ya que al ser $A_1,\\ldots,A_n$ un sistema completo tenemos\n\n$$B = B\\cap E = B\\cap (A_1\\cup \\cdots \\cup A_n) = (B\\cap A_1)\\cup \\cdots \\cup (B\\cap A_n)$$\n\ny como estos sucesos son incompatibles entre sí, se tiene\n\n\\begin{align*}\nP(B) &= P((B\\cap A_1)\\cup \\cdots \\cup (B\\cap A_n)) = P(B\\cap A_1)+\\cdots + P(B\\cap A_n) =\\\\\n&= P(A_1)P(B/A_1)+\\cdots + P(A_n)P(B/A_n) = \\sum_{i=1}^n P(A_i)P(B/A_i).\n\\end{align*}\n\n:::{.content-visible when-format=\"html\"}\n![Teorema de la probabilidad total.](img/probabilidad/probabilidad_total.svg){width=300}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Teorema de la probabilidad total.](img/probabilidad/probabilidad_total.pdf)\n:::\n:::\n:::\n\n:::{#exm-probabilidad-total}\nUn determinado síntoma $S$ puede ser originado por una enfermedad $E$ pero también lo pueden presentar las personas sin la enfermedad.\nSabemos que la prevalencia de la enfermedad $E$ es $0.2$. Además, se sabe que el $90\\%$ de las personas con la enfermedad presentan el síntoma, mientras que sólo el $40\\%$ de las personas sin la enfermedad lo presentan. Si se toma una persona al azar de la población, _¿qué probabilidad hay de que tenga el síntoma?_\n\nPara responder a la pregunta se puede aplicar el teorema de la probabilidad total usando el sistema completo $\\{E,\\overline{E}\\}$:\n\n$$P(S) = P(E)P(S|E)+P(\\overline E)P(S|\\overline E) = 0.2\\cdot 0.9 + 0.8\\cdot 0.4 = 0.5.$$\n\nEs decir, la mitad de la población tendrá el síntoma.\n\n_¡En el fondo se trata de una media ponderada de probabilidades!_\n\nLa respuesta a la pregunta anterior es evidente a la luz del árbol de probabilidad del espacio probabilístico del experimento.\n\n:::{.content-visible when-format=\"html\"}\n![Aplicación del teorema de la probabilidad total en un espacio probabilístico.](img/probabilidad/espacio_probabilistico_total.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Aplicación del teorema de la probabilidad total en un espacio probabilístico.](img/probabilidad/espacio_probabilistico_total.pdf)\n:::\n\n\\begin{align*}\nP(S) &= P(E,S) + P(\\overline E,S) = P(E)P(S|E)+P(\\overline E)P(S|\\overline E)\\\\\n& = 0.2\\cdot 0.9+ 0.8\\cdot 0.4 = 0.18 + 0.32 = 0.5.\n\\end{align*}\n:::\n\n## Teorema de Bayes\n\nLos sucesos de un sistema completo de sucesos $A_1,\\cdots,A_n$ también pueden verse como las distintas hipótesis ante un determinado hecho $B$.\n\nEn estas condiciones resulta útil poder calcular las probabilidades a posteriori $P(A_i|B)$ de cada una de las hipótesis.\n\n:::{#thm-bayes}\n## Bayes\nDado un sistema completo de sucesos $A_1,\\ldots,A_n$ y un suceso $B$ de un espacio muestral $\\Omega$ y otro suceso $B$ del mismo espacio muestral, la probabilidad de cada suceso $A_i$ $i=1,\\ldots,n$ condicionada por $B$ puede calcularse con la siguiente fórmula\n\n$$P(A_i|B) = \\frac{P(A_i\\cap B)}{P(B)} = \\frac{P(A_i)P(B|A_i)}{\\sum_{i=1}^n P(A_i)P(B|A_i)}.$$\n:::\n\n:::{#exm-teorema-bayes}\nEn el ejemplo anterior, una pregunta más interesante es qué diagnosticar a una persona que presenta el síntoma.\n\nEn este caso se puede interpretar $E$ y $\\overline{E}$ como las dos posibles hipótesis para el síntoma $S$. Las probabilidades a priori para ellas son $P(E)=0.2$ y $P(\\overline E)=0.8$. Esto quiere decir que si no se dispone de información sobre el síntoma, el diagnóstico será que la persona no tiene la enfermedad.\n\nSin embargo, si al reconocer a la persona se observa que presenta el síntoma, dicha información condiciona a las hipótesis, y para decidir entre ellas es necesario calcular sus probabilidades a posteriori, es\ndecir, $P(E|S)$ y $P(\\overline{E}|S)$.\n\nPara calcular las probabilidades a posteriori se puede utilizar el teorema de Bayes: \n\n\\begin{align*}\nP(E|S) &= \\frac{P(E)P(S|E)}{P(E)P(S|E)+P(\\overline{E})P(S|\\overline{E})} = \\frac{0.2\\cdot 0.9}{0.2\\cdot 0.9 + 0.8\\cdot 0.4} = \\frac{0.18}{0.5}=0.36,\\\\\nP(\\overline{E}|S) &= \\frac{P(\\overline{E})P(S|\\overline{E})}{P(E)P(S|E)+P(\\overline{E})P(S|\\overline{E})} = \\frac{0.8\\cdot 0.4}{0.2\\cdot 0.9 + 0.8\\cdot 0.4} = \\frac{0.32}{0.5}=0.64.\n\\end{align*}\n\nComo se puede ver la probabilidad de tener la enfermedad ha aumentado.\nNo obstante, la probabilidad de no tener la enfermedad sigue siendo mayor que la de tenerla, y por esta razón el diagnóstico seguirá siendo que no tiene la enfermedad.\n\nEn este caso se dice que el síntoma $S$ *no es determinante* a la hora de diagnosticar la enfermedad.\n:::\n\n## Epidemiología\n\nUna de las ramas de la Medicina que hace un mayor uso de la probabilidad es la , que estudia la distribución y las causas de las enfermedades en las poblaciones, identificando factores de riesgos para las enfermedades de cara a la atención médica preventiva.\n\nEn Epidemiología interesa la frecuencia de un *suceso médico* $E$ (típicamente una enfermedad como la gripe, un factor de riesgo como fumar o un factor de protección como vacunarse) que se mide mediante una\nvariable nominal con dos categorías (ocurrencia o no del suceso).\n\nHay diferentes medidas relativas a la frecuencia de un suceso médico. Las más importantes son:\n\n- Prevalencia\n- Incidencia\n- Riesgo relativo\n- Odds ratio\n\n### Prevalencia\n\n:::{#def-prevalencia}\n## Prevalencia\nLa _prevalencia_ de un suceso médico $E$ es la proporción de una población que está afectada por el suceso.\n\n$$\\mbox{Prevalencia}(E) = \\frac{\\mbox{Nº individuos afectados por $E$}}{\\mbox{Tamaño poblacional}}$$\n:::\n\nA menudo, la prevalencia se estima mediante una muestra como la frecuencia relativa de los individuos afectados por el suceso en la muestra. Es también común expresarla esta frecuencia como un porcentaje.\n\n:::{#exm-prevalencia}\nPara estimar la prevalencia de la gripe se estudió una muestra de $1000$ personas de las que $150$ presentaron gripe. Así, la prevalencia de la gripe es aproximadamente $150/1000=0.15$, es decir, un\n15%.\n:::\n\n### Incidencia\n\nLa mide la probabilidad de ocurrencia de un suceso médico en una población durante un periodo de tiempo específico. La incidencia puede medirse como una proporción acumulada o como una tasa.\n\n:::{#def-incidencia-acumulada}\n## Incidencia acumulada\nLa _incidencia acumulada_ de un suceso médico $E$ es la proporción de individuos que experimentaron el evento en un periodo de tiempo, es decir, el número de nuevos casos afectados por el evento en el periodo de tiempo, divido por el tamaño de la población inicialmente en riesgo de verse afectada.\n\n$$R(E)=\\frac{\\mbox{Nº de nuevos casos con $E$}}{\\mbox{Tamaño de la población en riesgo}}.$$\n:::\n\n:::{#exm-incidencia-acumulada}\nUna población contenía inicialmente $1000$ personas sin gripe y después de dos años se observó que $160$ de ellas sufrieron gripe. La incidencia acumulada de la gripe es 160 casos pro 1000 personas por dos años, es decir, 16% en dos años.\n:::\n\n### Tasa de incidencia o Riesgo absoluto\n\n:::{#def-riesgo-absoluto}\n## Riesgo absoluto\nLa _tasa de incidencia_ o _riesgo absoluto_ de un suceso médico $E$ es el número de nuevos casos afectados por el evento divido por la población en riesgo y por el número de unidades temporales del periodo considerado.\n\n$$R(E)=\\frac{\\mbox{Nº nuevos casos con $E$}}{\\mbox{Tamaño población en riesgo}\\times \\mbox{Nº unidades de tiempo}}$$\n:::\n\n:::{#exm-riesgo-absoluto}\nUna población contenía inicialmente $1000$ personas sin gripe y después de dos años se observó que $160$ de ellas sufrieron gripe. Si se considera el año como intervalo de tiempo, la tasa de incidencia de la gripe es $160$ casos dividida por $1000$ personas y por dos años, es decir, $80$ casos por $1000$ personas-año o 8% de personas al año.\n:::\n\n### Prevalencia vs Incidencia\n\nLa prevalencia no debe confundirse con la incidencia. La prevalencia indica cómo de extendido está el suceso médico en una población, sin preocuparse por cuándo los sujetos se han expuesto al riesgo o durante\ncuánto tiempo, mientras que la incidencia se fija en el riesgo de verse afectado por el suceso en un periodo concreto de tiempo.\n\nAsí, la prevalencia se calcula en estudios transversales en un momento temporal puntual, mientras que para medir la incidencia se necesita un estudio longitudinal que permita observar a los individuos durante un\nperiodo de tiempo.\n\nLa incidencia es más útil cuando se pretende entender la causalidad del suceso: por ejemplo, si la incidencia de una enfermedad en una población aumenta, seguramente hay un factor de riesgo que lo está promoviendo.\n\nCuando la tasa de incidencia es aproximadamente constante en la duración del suceso, la prevalencia es aproximadamente el producto de la incidencia por la duración media del suceso, es decir,\n\n$$ \\mbox{Prevalencia} = \\mbox{Incidencia} \\times \\mbox{duración}$$\n\n### Comparación de riesgos\n\nPara determinar si un factor o característica está asociada con el suceso médico es necesario comparar el riesgo del suceso en dos poblaciones, una expuesta al factor y la otra no. El grupo expuesto al factor se conoce como _grupo tratamiento_ o _grupo experimental_ $T$ y el grupo no expuesto como _grupo control_ $C$.\n\nHabitualmente los casos observados para cada grupo se representan en una tabla de $2\\times2$ como la siguiente:\n\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: center;\">Suceso $E$</th>\n<th style=\"text-align: center;\">No suceso $\\overline E$</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Grupo tratamiento $T$</td>\n<td style=\"text-align: center;\">$a$</td>\n<td style=\"text-align: center;\">$b$</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Grupo control $C$</td>\n<td style=\"text-align: center;\">$c$</td>\n<td style=\"text-align: center;\">$d$</td>\n</tr>\n</tbody>\n</table>\n\n### Riesgo atribuible o diferencia de riesgos $RA$\n\n:::{#def-riesgo-atribuible}\n## Riesgo atribuible\nEl _riesgo atribuible_ o _diferencia de riesgo_ de un suceso médico $E$ para los individuos expuestos a un factor es la diferencia entre los riesgos absolutos de los grupos tratamiento y control.\n\n$$RA(E)=R_T(E)-R_C(E)=\\frac{a}{a+b}-\\frac{c}{c+d}.$$\n:::\n\nEl riesgo atribuible es el riesgo de un suceso que es debido específicamente al factor de interés.\n\nObsérvese que el riesgo atribuible puede ser positivo, cuando el riesgo del grupo tratamiento es mayor que el del grupo control, o negativo, de lo contrario.\n\n:::{#exm-riesgo-atribuible}\nPara determinar la efectividad de una vacuna contra la gripe, una muestra de $1000$ personas sin gripe fueron seleccionadas al comienzo del año. La mitad de ellas fueron vacunadas (grupo tratamiento) y la otra mitad recibieron un placebo (grupo control). La tabla siguiente resume los resultados al final del año.\n\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: center;\">Gripe $E$</th>\n<th style=\"text-align: center;\">No gripe $\\overline E$</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Grupo tratamiento (vacunados)</td>\n<td style=\"text-align: center;\">20</td>\n<td style=\"text-align: center;\">480</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Grupo control (No vacunados)</td>\n<td style=\"text-align: center;\">80</td>\n<td style=\"text-align: center;\">420</td>\n</tr>\n</tbody>\n</table>\n\nEl riesgo atribuible de contraer la gripe cuando se es vacunado es \n\n$$AR(D) = \\frac{20}{20+480}-\\frac{80}{80+420} = -0.12.$$ \n\nEsto quiere decir que el riesgo de contraer la gripe es un 12% menor en vacunados\nque en no vacunados.\n:::\n\n### Riesgo relativo $RR$\n\n:::{#def-riesgo-relativo}\n## Riesgo relativo\nEl *riesgo relativo* de un suceso médico $E$ para los individuos expuestos a un factor es el cociente entre las proporciones de individuos afectados por el suceso en un periodo de tiempo de los grupos tratamiento y control. Es decir, el cociente entre las incidencias de grupo tratamiento y el grupo control.\n\n$$RR(D)=\\frac{\\mbox{Riesgo grupo tratamiento}}{\\mbox{Riesgo grupo control}}=\\frac{R_T(E)}{R_C(E)}=\\frac{a/(a+b)}{c/(c+d)}$$\n:::\n\n:::{.callout-note title=Interpretación}\nEl riesgo relativo compara el riesgo de desarrollar un suceso médico entre el grupo tratamiento y el grupo control.\n\n- $RR=1$ $\\Rightarrow$ No hay asociación entre el suceso y la exposición al factor.\n- $RR<1$ $\\Rightarrow$ La exposición al factor disminuye el riesgo del suceso.\n- $RR>1$ $\\Rightarrow$ La exposición al factor aumenta el riesgo del suceso.\n\nCuanto más lejos de 1, más fuerte es la asociación.\n:::\n\n:::{#exm-riesgo-relativo}\nPara determinar la efectividad de una vacuna contra la gripe, una muestra de $1000$ personas sin gripe fueron seleccionadas al comienzo del año. La mitad de ellas fueron vacunadas (grupo tratamiento) y la otra mitad recibieron un placebo (grupo control). La tabla siguiente resume los resultados al final del año.\n\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: center;\">Gripe $E$</th>\n<th style=\"text-align: center;\">No gripe $\\overline E$</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Grupo tratamiento (vacunados)</td>\n<td style=\"text-align: center;\">20</td>\n<td style=\"text-align: center;\">480</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Grupo control (No vacunados)</td>\n<td style=\"text-align: center;\">80</td>\n<td style=\"text-align: center;\">420</td>\n</tr>\n</tbody>\n</table>\n\nEl riesgo relativo de contraer la gripe cuando se es vacunado es \n\n$$RR(D) = \\frac{20/(20+480)}{80/(80+420)} = 0.25.$$\n\nAsí, la probabilidad de contraer la gripe en los individuos vacunados fue la cuarta parte de\nla de contraerla en el caso de no haberse vacunado, es decir, la vacuna reduce el riesgo de gripe un 75%.\n:::\n\n### Odds\n\nUna forma alternativa de medir el riesgo de un suceso médico es el _odds_.\n\n:::{#def-odds}\nEl _odds_ de un suceso médico $E$ en una población es el cociente entre el número de individuos que adquirieron el suceso y los que no en un periodo de tiempo.\n\n$$ODDS(E)=\\frac{\\mbox{Nº nuevos casos con $E$}}{\\mbox{Nº casos sin $E$}}=\\frac{P(E)}{P(\\overline E)}.$$\n:::\n\nA diferencia de la incidencia, que es una proporción menor o igual que 1, el odds puede ser mayor que 1. No obstante es posible convertir el odds en una probabilidad con al fórmula \n\n$$P(E) = \\frac{ODDS(E)}{ODDS(E)+1}.$$\n\n:::{#exm-odds}\nUna población contenía inicialmente $1000$ personas sin gripe. Después de un año $160$ de ellas tuvieron gripe. Entonces el odds de la gripe es $160/840$.\n\nObsérvese que la incidencia es $160/1000$.\n:::\n\n### Odds ratio $OR$ \n\n:::{#def-odds-ratio}\n## Odds ratio\nEl _odds ratio_ o la _oportunidad relativa_ de un suceso médico $E$ para los individuos expuestos a un factor es el cociente entre los odds del sucesos de los grupos tratamiento y control.\n\n$$OR(E)=\\frac{\\mbox{Odds en grupo tratamiento}}{\\mbox{Odds en grupo control}}=\\frac{a/b}{c/d}=\\frac{ad}{bc}.$$\n:::\n\n:::{.callout-note title=Interpretación}\nEl odds ratio compara los odds de un suceso médico entre el grupo tratamiento y control. La interpretación es similar a la del riesgo relativo:\n\n- $OR=1$ $\\Rightarrow$ No existe asociación entre el suceso y la exposición al factor.\n- $OR<1$ $\\Rightarrow$ La exposición al factor disminuye el riesgo del suceso.\n- $OR>1$ $\\Rightarrow$ La exposición al factor aumenta el riesgo del suceso.\n\nCuanto más lejos de 1, más fuerte es la asociación.\n:::\n\n:::{#exm-odds-ratio}\nPara determinar la efectividad de una vacuna contra la gripe, una muestra de 1000 personas sin gripe fueron seleccionadas al comienzo del año. La mitad de ellas fueron vacunadas (grupo tratamiento) y la otra mitad recibieron un placebo (grupo control). La tabla siguiente resume los resultados al final del año.\n\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: center;\">Gripe $E$</th>\n<th style=\"text-align: center;\">No gripe $\\overline E$</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Grupo tratamiento (vacunados)</td>\n<td style=\"text-align: center;\">20</td>\n<td style=\"text-align: center;\">480</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Grupo control (No vacunados)</td>\n<td style=\"text-align: center;\">80</td>\n<td style=\"text-align: center;\">420</td>\n</tr>\n</tbody>\n</table>\n\nEl odds ratio de sufrir la gripe para los individuos vacunados es \n\n$$OR(D) = \\frac{20/480}{80/420} = 0.21875.$$ \n\nEsto quiere decir que el odds de sufrir la gripe frente a no sufrirla en los vacunados es casi un quinto del de los no vacunados, es decir, que aproximadamente por cada 22 personas vacunadas con gripe habrá 100 personas no vacunadas con gripe.\n:::\n\n### Riesgo relativo vs Odds ratio\n\nEl riesgo relativo y el odds ratio son dos medidas de asociación pero su interpretación es ligeramente diferente. Mientras que el riesgo relativo expresa una comparación de riesgos entre los grupos tratamiento y control, el odds ratio expresa una comparación de odds, que no es lo mismo que el riesgo. Así, un odds ratio de 2 *no* significa que el grupo tratamiento tiene el doble de riesgo de adquirir el suceso.\n\nLa interpretación del odds ratio es un poco más enrevesada porque es contrafactual, y nos da cuántas veces es más frecuente el suceso en el grupo tratamiento en comparación con el control, asumiendo que en el\ngrupo control es tan frecuente que ocurra el suceso como que no.\n\nLa ventaja del odds ratio es que no depende de la prevalencia o la incidencia del suceso, y debe usarse siempre que el número de individuos que presenta el suceso se selecciona arbitrariamente en ambos grupos,\ncomo ocurre en los estudios casos-control.\n\n:::{#exm-riesgo-relativo-vs-odds-ratio}\nPara determinar la asociación entre el cáncer de pulmón y fumar se tomaron dos muestras (la segunda con el doble de individuos sin cáncer) obteniendo los siguientes resultados:\n\n**Muestra 1**\n\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: center;\">Cáncer</th>\n<th style=\"text-align: center;\">No cáncer</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Fumadores</td>\n<td style=\"text-align: center;\">60</td>\n<td style=\"text-align: center;\">80</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">No fumadores</td>\n<td style=\"text-align: center;\">40</td>\n<td style=\"text-align: center;\">320</td>\n</tr>\n</tbody>\n</table>\n\n\\begin{align*}\nRR(D) &= \\frac{60/(60+80)}{40/(40+320)} = 3.86.\\\\\nOR(D) &= \\frac{60/80}{40/320} = 6. \n\\end{align*}\n\n**Muestra 2**\n\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: center;\">Cáncer</th>\n<th style=\"text-align: center;\">No cáncer</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Fumadores</td>\n<td style=\"text-align: center;\">60</td>\n<td style=\"text-align: center;\">160</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">No fumadores</td>\n<td style=\"text-align: center;\">40</td>\n<td style=\"text-align: center;\">640</td>\n</tr>\n</tbody>\n</table>\n\n\\begin{align*}\nRR(D) &= \\frac{60/(60+160)}{40/(40+640)} = 4.64.\\\\\nOR(D) &= \\frac{60/160}{40/640} = 6. \n\\end{align*}\n\nAsí, cuando cambia la incidencia o prevalencia de un suceso (cáncer de pulmón) el riesgo relativo cambia, mientras que el odds ratio no.\n:::\n\nLa relación entre el riesgo relativo y el odds ratio viene dada por la siguiente fórmula\n\n$$RR = \\frac{OR}{1-R_0+R_0\\cdot OR} = OR \\frac{1-R_1}{1-R_0},$$\n\ndonde $R_C$ and $R_T$ son la prevalencia o la incidencia en los grupos control y tratamiento respectivamente.\n\nEl odds ratio siempre sobrestima el riesgo relativo cuando este es mayor que 1 y lo subestima cuando es menor que 1. No obstante, con sucesos médicos raros (con una prevalencia o incidencia baja) el riesgo\nrelativo y el odds ratio son casi iguales.\n\n:::{.content-visible when-format=\"html\"}\n![Odss ratio versus riesgo relativo.](img/probabilidad/odds_ratio_vs_riesgo_relativo.svg){width=500}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Odss ratio versus riesgo relativo.](img/probabilidad/odds_ratio_vs_riesgo_relativo.pdf)\n:::\n\n## Tests diagnósticos\n\nEn Epidemiología es común el uso de test para diagnosticar enfermedades.\n\nGeneralmente estos test no son totalmente fiables, sino que hay cierta probabilidad de acierto o fallo en el diagnóstico, que suele representarse en la siguiente tabla:\n\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: center;\">Presencia enfermedad $E$</th>\n<th style=\"text-align: center;\">Ausencia enfermedad $\\overline E$</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Test positivo $+$</td>\n<td style=\"text-align: center;\"><span style=\"color: green\">Verdadero positivo </span>$VP$</td>\n<td style=\"text-align: center;\"><span style=\"color: red\">Falso positivo </span>$FP$</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Test negativo $−$</td>\n<td style=\"text-align: center;\"><span style=\"color: red\">Falso negativo </span>$FN$</td>\n<td style=\"text-align: center;\"><span style=\"color: green\">Verdadero Negativo </span>$VN$</td>\n</tr>\n</tbody>\n</table>\n\n### Sensibilidad y especificidad de un test diagnóstico\n\nLa fiabilidad de un test diagnóstico depende de las siguientes probabilidades.\n\n:::{#def-sensibilidad}\n## Sensibilidad\nLa _sensibilidad_ de un test diagnóstico es la proporción de resultados positivos del test en personas con la enfermedad, \n\n$$P(+|E)=\\frac{VP}{VP+FN}.$$\n:::\n\n:::{#def-especificidad}\n## Especificidad\nLa _especificidad_ de un test diagnóstico es la proporción de resultados negativos del test en personas sin la enfermedad,\n\n$$P(-|\\overline{E})=\\frac{VN}{VN+FP}.$$\n:::\n\nNormalmente existe un balance entre la sensibilidad y la especificidad.\n\nUn test con una alta sensibilidad detectará la enfermedad en la mayoría de las personas enfermas, pero también dará más falsos positivos que un test menos sensible. De este modo, un resultado positivo en un test con una gran sensibilidad no es muy útil para confirmar la enfermedad, pero un resultado negativo es útil para descartar la enfermedad, ya que raramente da resultados negativos en personas con la enfermedad.\n\nPor otro lado, un test con una alta especificidad descartará la enfermedad en la mayoría de las personas sin la enfermedad, pero también producirá más falsos negativos que un test menos específico. Así, un\nresultado negativo en un test con una gran especificidad no es útil para descartar la enfermedad, pero un resultado positivo es muy útil para confirmar la enfermedad, ya que raramente da resultados positivos en\npersonas sin la enfermedad.\n\n:::{#exm-sensibilidad-especificidad}\nUn test diagnóstico para la gripe se ha aplicado a una muestra aleatoria de $1000$ personas. Los resultados aparecen resumidos en la siguiente\ntabla.\n\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: center;\">Presencia de gripe $E$</th>\n<th style=\"text-align: center;\">Ausencia de gripe $\\overline E$</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Test $+$</td>\n<td style=\"text-align: center;\">95</td>\n<td style=\"text-align: center;\">90</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Test $−$</td>\n<td style=\"text-align: center;\">5</td>\n<td style=\"text-align: center;\">810</td>\n</tr>\n</tbody>\n</table>\n\nSegún esta muestra, la prevalencia de la gripe puede estimarse como\n\n$$P(E) = \\frac{95+5}{1000} = 0.1.$$\n\nLa sensibilidad del test diagnóstico es\n\n$$P(+|E) = \\frac{95}{95+5}= 0.95.$$\n\nY la especificidad es \n\n$$P(-|\\overline{E}) = \\frac{810}{90+810}=0.9.$$\n\nAsí pues, se trata de un buen test tanto para descartar la enfermedad como para confirmarla, pero es un poco mejor para confirmarla que para descartarla porque la especificidad es mayor que la sensibilidad.\n:::\n\nDecidir entre un test con una gran sensibilidad o un test con una gran especificidad depende del tipo de enfermedad y el objetivo del test. En general, utilizaremos un test sensible cuando:\n\n- La enfermedad es grave y es importante detectarla.\n- La enfermedad es curable.\n- Los falsos positivos no provocan traumas serios.\n\nY utilizaremos un test específico cuando:\n\n- La enfermedad es importante pero difícil o imposible de curar.\n- Los falsos positivos pueden provocar traumas serios.\n- El tratamiento de los falsos positivos puede tener graves consecuencias.\n\n\n### Valores predictivos de un test diagnóstico\n\nPero el aspecto más importante de un test diagnóstico es su poder predictivo, que se mide con las siguientes probabilidades a posteriori.\n\n:::{#def-valor-predictivo-positivo}\n## Valor predictivo positivo\nEl _valor predictivo positivo_ de un test diagnóstico es la proporción de personas con la enfermedad entre las personas con resultado positivo\nen el test, \n\n$$P(E|+) = \\frac{VP}{VP+FP}.$$\n:::\n\n:::{#def-valor-predictivo-negativo}\n## Valor predictivo negativo\nEl _valor predictivo negativo_ de un test diagnóstico es la proporción de personas sin la enfermedad entre las personas con resultado negativo en el test, \n\n$$P(\\overline{E}|-) = \\frac{VN}{VN+FN}.$$\n:::\n\n:::{.callout-note title=Interpretación}\nLos valores predictivos positivo y negativo permiten confirmar o descartar la enfermedad, respectivamente, si alcanzan al menos el umbral de $0.5$. \n\n$$\n\\begin{array}{rcl}\nVPP>0.5 & \\Rightarrow & \\mbox{Diagnosticar la enfermedad}\\\\\nVPN>0.5 & \\Rightarrow & \\mbox{Diagnosticar la no enfermedad} \n\\end{array}\n$$\n:::\n\nNo obstante, estas probabilidades dependen de la prevalencia de la enfermedad $P(E)$. Pueden calcularse a partir de la sensibilidad y la especificidad del test diagnóstico usando el teorema de Bayes.\n\n\\begin{align*}\nVPP=P(E|+) &= \\frac{P(E)P(+|E)}{P(E)P(+|E)+P(\\overline{E})P(+|\\overline{E})}\\\\\nVPN=P(\\overline{E}|-) &= \\frac{P(\\overline{E})P(-|\\overline{E})}{P(E)P(-|E)+P(\\overline{E})P(-|\\overline{E})}\n\\end{align*}\n\nAsí, con enfermedades frecuentes, el valor predictivo positivo aumenta, y con enfermedades raras, el valor predictivo negativo aumenta.\n\n:::{#exm-valores-predictivos}\nSiguiendo con el ejemplo anterior de la gripe, se tiene que el valor predictivo positivo del test es \n\n$$VPP = P(E|+) = \\frac{95}{95+90} = 0.5135.$$\n\nComo este valor es mayor que $0.5$, eso significa que se diagnosticará la gripe si el resultado del test es positivo. No obstante, la confianza en el diagnóstico será baja, ya que el valor es poco mayor que $0.5$.\n\nPor otro lado, el valor predictivo negativo es \n\n$$VPN = P(\\overline{E}|-) = \\frac{810}{5+810} = 0.9939.$$\n\nComo este valor es casi 1, eso significa que es casi seguro que no se tiene la gripe cuando el resultado del test es negativo.\n\nAsí, se puede concluir que este test es muy potente para descartar la gripe, pero no lo est tanto para confirmarla.\n:::\n\n### Razón de verosimilitud de un test diagnóstico\n\nLa siguientes medidas también se derivan de la sensibilidad y la especificidad de un test diagnóstico.\n\n:::{#def-razon-verosimilitud-positiva}\n## Razón de verosimilitud positiva\nLa _razón de verosimilitud positiva_ de un test diagnóstico es el cociente entre la probabilidad de un resultado positivo en personas con la enfermedad y personas sin la enfermedad, respectivamente.\n\n$$RV+=\\frac{P(+|E)}{P(+|\\overline{E})} = \\frac{\\mbox{Sensibilidad}}{1-\\mbox{Especificidad}}.$$\n:::\n\n:::{#def-razon-verosimilitud-negativa}\n## Razón de verosimilitud negativa\nLa _razón de verosimilitud negativa_ de un test diagnóstico es el cociente entre la probabilidad de un resultado negativo en personas con la enfermedad y personas sin la enfermedad, respectivamente.\n\n$$RV-=\\frac{P(-|E)}{P(-|\\overline{E})} = \\frac{1-\\mbox{Sensibilidad}}{\\mbox{Especificidad}}.$$\n:::\n\n:::{.callout-note title=Interpretación}\nLa razón de verosimilitud positiva puede interpretarse como el número de veces que un resultado positivo es más probable en personas con la enfermedad que en personas sin la enfermedad.\n\nPor otro lado, la razón de verosimilitud negativa puede interpretarse como el número de veces que un resultado negativo es más probable en personas con la enfermedad que en personas sin la enfermedad.\n\nLas probabilidades a posteriori pueden calculares a partir de las probabilidades a priori usando las razones de verosimilitud\n\n$$P(E|+) = \\frac{P(E)P(+|E)}{P(E)P(+|E)+P(\\overline{E})P(+|\\overline{E})} = \\frac{P(E)RV+}{1-P(E)+P(E)RV+}$$\n\nAsí,\n\n- Una razón de verosimilitud positiva mayor que 1 aumenta la probabilidad de la enfermedad.\n- Una razón de verosimilitud positiva menor que 1 disminuye la probabilidad de la enfermedad.\n- Una razón de verosimilitud 1 no cambia la probabilidad a priori de la de tener la enfermedad.\n:::\n\n:::{.content-visible when-format=\"html\"}\n![Razón de verosimilitud.](img/probabilidad/razon_verosimilitud.svg){width=500}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Razón de verosimilitud.](img/probabilidad/razon_verosimilitud.pdf)\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}