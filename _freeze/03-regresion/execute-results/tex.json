{
  "hash": "4f54dc0474ddc52812ed34dc4e1e1a62",
  "result": {
    "markdown": "---\ntitle: Regresión\nlang: es\n---\n\n\n\n\n\nHasta ahora se ha visto como describir el comportamiento de una variable, pero en los fenómenos naturales normalmente aparecen más de una variable que suelen estar relacionadas. Por ejemplo, en un estudio sobre el peso de las personas, deberíamos incluir todas las variables con las que podría tener relación: altura, edad, sexo, dieta, tabaco,\nejercicio físico, etc.\n\nPara comprender el fenómeno no basta con estudiar cada variable por separado y es preciso un estudio conjunto de todas las variables para ver cómo interactúan y qué relaciones se dan entre ellas. El objetivo de la estadística en este caso es dar medidas del grado y del tipo de relación entre dichas variables.\n\nGeneralmente, en un _estudio de dependencia_ se considera una **variable dependiente** $Y$ que se supone relacionada con otras variables $X_1,\\ldots,X_n$ llamadas **variables independientes**.\n\nEl caso más simple es el de una sola variable independiente, y en tal caso se habla de _estudio de dependencia simple_. Para más de una\nvariable independiente se habla de _estudio de dependencia múltiple_.\n\nEn este capítulo se verán los estudios de dependencia simple que son más sencillos.\n\n## Distribución de frecuencias conjunta\n\n### Frecuencias conjuntas\n\nAl estudiar la dependencia simple entre dos variables $X$ e $Y$, no se pueden estudiar sus distribuciones por separado, sino que hay que estudiar la distribución conjunta de la **variable bidimensional** $(X,Y)$, cuyos valores son los pares $(x_i,y_j)$ donde el primer elemento es un valor $X$ y el segundo uno de $Y$.\n\n:::{#def-frecuencias-muestrales-conjuntas}\n## Frecuencias muestrales conjuntas\nDada una muestra de tamaño $n$ de una variable bidimensional $(X,Y)$, para cada valor de la variable $(x_i,y_j)$ observado en la muestra se define\n\n- **Frecuencia absoluta** $n_{ij}$: Es el número de veces que el par $(x_i,y_j)$ aparece en la muestra.\n- **Frecuencia relativa** $f_{ij}$: Es la proporción de veces que el par $(x_i,y_j)$ aparece en la muestra.\n  \n$$f_{ij}=\\frac{n_{ij}}{n}$$\n:::\n\n:::{.callout-warning}\nPara las variables bidimensionales no tienen sentido las frecuencias acumuladas.\n:::\n\n### Distribución de frecuencias bidimensional\n\nAl conjunto de valores de la variable bidimensional y sus respectivas frecuencias muestrales se le denomina **distribución de frecuencias bidimensional**, y se representa mediante una **tabla de frecuencias bidimensional**.\n\n$$\\begin{array}{|c|ccccc|}\n\\hline\nX\\backslash Y & y_1 & \\cdots & y_j & \\cdots & y_q\\\\\n\\hline\nx_1 & n_{11} & \\cdots & n_{1j} & \\cdots & n_{1q}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\nx_i & n_{i1} & \\cdots & n_{ij} & \\cdots & n_{iq}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\nx_p & n_{p1} & \\cdots & n_{pj} & \\cdots & n_{pq}\\\\\n\\hline\n\\end{array}$$\n\n:::{#exm-datos-agrupados} \nLa estatura (en cm) y el peso (en Kg) de una muestra de 30 estudiantes es:\n\n<div style=\"text-align:center\">\n(179,85), (173,65), (181,71), (170,65), (158,51), (174,66),<br/>\n(172,62), (166,60), (194,90), (185,75), (162,55), (187,78),<br/>\n(198,109), (177,61), (178,70), (165,58), (154,50), (183,93),<br/>\n(166,51), (171,65), (175,70), (182,60), (167,59), (169,62),<br/>\n(172,70), (186,71), (172,54), (176,68),(168,67), (187,80).\n</div>\n\nLa tabla de frecuencias bidimensional es\n\n$$\\begin{array}{|c||c|c|c|c|c|c|}\n\\hline\n  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) \\\\\n  \\hline\\hline\n  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 \\\\\n  \\hline\n  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 \\\\\n  \\hline\n  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 \\\\\n  \\hline\n  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 \\\\\n  \\hline\n  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 \\\\\n  \\hline\n\\end{array}$$\n:::\n\n### Diagrama de dispersión\n\nLa distribución de frecuencias conjunta de una variable bidimensional puede representarse gráficamente mediante un **diagrama de dispersión**, donde los datos se representan como una colección de puntos en un plano cartesiano.\n\nHabitualmente la variable independiente se representa en el eje $X$ y la variable dependiente en el eje $Y$. Por cada par de valores $(x_i,y_j)$ en la muestra se dibuja un punto en el plano con esas coordenadas.\n\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de dispersión.](img/regresion/diagrama_dispersion.svg){width=500}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de dispersión.](img/regresion/diagrama_dispersion.pdf)\n:::\n\nEl resultado es un conjunto de puntos que se conoce como _nube de puntos_.\n\n:::{#exm-diagrama-dispersion}\nEl siguiente diagrama de dispersión representa la distribución conjunta de estaturas y pesos de la muestra anterior.\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de dispersión de estaturas y pesos.](img/regresion/diagrama_dispersion_estatura_peso.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de dispersión de estaturas y pesos.](img/regresion/diagrama_dispersion_estatura_peso.pdf)\n:::\n:::\n\n:::{.callout-note title=\"Interpretación\"}\nEl diagrama de dispersión da información visual sobre el tipo de relación entre las variables.\n\n:::{.content-visible when-format=\"html\"}\n![Diagramas de dispersión de diferentes tipos de relaciones.](img/regresion/diagrama_dispersion_tipos_relaciones.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagramas de dispersión de diferentes tipos de relaciones.](img/regresion/diagrama_dispersion_tipos_relaciones.pdf)\n:::\n:::\n\n### Distribuciones marginales\n\nA cada una de las distribuciones de las variables que conforman la variable bidimensional se les llama .\n\nLas distribuciones marginales se pueden obtener a partir de la tabla de frecuencias bidimensional, sumando las frecuencias por filas y columnas.\n\n$$\n\\begin{array}{|c|ccccc|c|}\n\\hline\nX\\backslash Y & y_1 & \\cdots & y_j & \\cdots & y_q & \\color{red}{n_x}\\\\\n\\hline\nx_1 & n_{11} & \\cdots & n_{1j} & \\cdots & n_{1q} & \\color{red}{n_{x_1}}\\\\\n\\vdots & \\vdots & \\vdots & \\downarrow + & \\vdots & \\vdots & \\color{red}{\\vdots} \\\\\nx_i & n_{i1} & \\stackrel{+}{\\rightarrow} & n_{ij} & \\stackrel{+}{\\rightarrow} & n_{iq} & \\color{red}{n_{x_i}}\\\\\n\\vdots & \\vdots & \\vdots & \\downarrow +  & \\vdots & \\vdots & \\color{red}{\\vdots}\\\\\nx_p & n_{p1} & \\cdots & n_{pj} & \\cdots & n_{pq} & \\color{red}{n_{x_p}} \\\\\n\\hline\n\\color{red}{n_y} & \\color{red}{n_{y_1}} & \\color{red}{\\cdots} & \\color{red}{n_{y_j}} & \\color{red}{\\cdots} & \\color{red}{n_{y_q}} & n\\\\\n\\hline\n\\end{array}\n$$\n\n:::{#exm-distribuciones-marginales}\nEn el ejemplo anterior de las estaturas y los pesos, las distribuciones marginales son\n\n$$\n\\begin{array}{|c||c|c|c|c|c|c|c|}\n\\hline\n  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) & \\color{red}{n_x}\\\\\n  \\hline\\hline\n  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 & \\color{red}{2}\\\\\n  \\hline\n  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 & \\color{red}{8}\\\\\n  \\hline\n  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 & \\color{red}{11} \\\\\n  \\hline\n  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 & \\color{red}{7} \\\\\n  \\hline\n  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 & \\color{red}{2}\\\\\n  \\hline\n  \\color{red}{n_y} & \\color{red}{7} & \\color{red}{11} & \\color{red}{7} & \\color{red}{2} & \\color{red}{2} & \\color{red}{1} & 30\\\\\n  \\hline\n\\end{array}\n$$\n\ny los estadísticos correspondientes son\n\n$$\n\\begin{array}{lllll}\n\\bar x = 174.67 \\mbox{ cm} & \\quad & s^2_x = 102.06 \\mbox{ cm}^2 & \\quad & s_x = 10.1 \\mbox{ cm}\\\\\n\\bar y = 69.67 \\mbox{ Kg} & & s^2_y = 164.42 \\mbox{ Kg}^2 & & s_y = 12.82 \\mbox{ Kg}\n\\end{array}\n$$\n:::\n\n## Covarianza\n\nPara analizar la relación entre dos variables cuantitativas es importante hacer un estudio conjunto de las desviaciones respecto de la media de cada variable.\n\n:::{.content-visible when-format=\"html\"}\n![Desviaciones de las medias en un diagrama de dispersión.](img/regresion/desviaciones_media.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Desviaciones de las medias en un diagrama de dispersión.](img/regresion/desviaciones_media.pdf)\n:::\n\nSi dividimos la nube de puntos del diagrama de dispersión en 4 cuadrantes centrados en el punto de medias $(\\bar x, \\bar y)$, el signo de las desviaciones será:\n\n   |**Cuadrante**|$(x_i-\\bar x)$|$(y_j-\\bar y)$|$(x_i-\\bar x)(y_j-\\bar y)$|\n   |:-----------:|:------------:|:------------:|:------------------------:|\n   |      1      |      $+$     |      $+$     |           $+$            |\n   |      2      |      $-$     |      $+$     |           $-$            |\n   |      3      |      $-$     |      $-$     |           $+$            |\n   |      4      |      $+$     |      $-$     |           $-$            |\n\n:::{.content-visible when-format=\"html\"}\n![Cuadrantes de un diagrama de dispersión.](img/regresion/cuadrantes_diagrama_dispersion.svg)\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Cuadrantes de un diagrama de dispersión.](img/regresion/cuadrantes_diagrama_dispersion.pdf)\n:::\n\nSi la relación entre las variables es _lineal y creciente_, entonces la mayor parte de los puntos estarán en los cuadrantes 1 y 3 y la suma de los productos de desviaciones será positiva.\n\n$$\\sum(x_i-\\bar x)(y_j-\\bar y) > 0$$\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de dispersión de una relación lineal creciente.](img/regresion/diagrama_dispersion_lineal_creciente.svg)\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de dispersión de una relación lineal creciente.](img/regresion/diagrama_dispersion_lineal_creciente.pdf)\n:::\n\nSi la relación entre las variables es _lineal y decreciente_, entonces la mayor parte de los puntos estarán en los cuadrantes 2 y 4 y la suma de los productos de desviaciones será negativa.\n\n$$\\sum(x_i-\\bar x)(y_j-\\bar y) = -$$\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de dispersión de una relación lineal decreciente.](img/regresion/diagrama_dispersion_lineal_decreciente.svg)\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de dispersión de una relación lineal decreciente.](img/regresion/diagrama_dispersion_lineal_decreciente.pdf)\n:::\n\nUsando el producto de las desviaciones respecto de las medias surge el siguiente estadístico.\n\n:::{#def-covarianza}\n## Covarianza muestral\nLa _covarianza muestral_ de una variable aleatoria bidimensional $(X,Y)$ se define como el promedio de los productos de las respectivas desviaciones respecto de las medias de $X$ e $Y$.\n\n$$s_{xy}=\\frac{\\sum (x_i-\\bar x)(y_j-\\bar y)n_{ij}}{n}$$\n:::\n\nTambién puede calcularse de manera más sencilla mediante la fórmula\n\n$$s_{xy}=\\frac{\\sum x_iy_jn_{ij}}{n}-\\bar x\\bar y.$$\n\n:::{.callout-note title=\"Interpretación\"}\nLa covarianza sirve para estudiar la relación lineal entre dos variables:\n\n- Si $s_{xy}>0$ existe una relación lineal creciente.\n- Si $s_{xy}<0$ existe una relación lineal decreciente.\n- Si $s_{xy}=0$ no existe relación lineal.\n:::\n\n:::{#exm-covarianza}\nUtilizando la tabla de frecuencias bidimensional de la muestra de estaturas y pesos\n\n$$\n\\begin{array}{|c||c|c|c|c|c|c|c|}\n\\hline\n  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) & n_x\\\\\n  \\hline\\hline\n  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 & 2\\\\\n  \\hline\n  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 & 8\\\\\n  \\hline\n  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 & 11 \\\\\n  \\hline\n  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 & 7 \\\\\n  \\hline\n  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 & 2\\\\\n  \\hline\n  n_y & 7 & 11 & 7 & 2 & 2 & 1 & 30\\\\\n  \\hline\n\\end{array}\n$$\n\n$$\\bar x = 174.67 \\mbox{ cm} \\qquad \\bar y = 69.67 \\mbox{ Kg}$$\n\nla covarianza vale\n\n\\begin{align*}\ns_{xy} &=\\frac{\\sum x_iy_jn_{ij}}{n}-\\bar x\\bar y =  \\frac{155\\cdot 55\\cdot 2 + 165\\cdot 55\\cdot 4 + \\cdots + 195\\cdot 105\\cdot 1}{30}-174.67\\cdot 69.67 =\\\\\n& = \\frac{368200}{30}-12169.26 = 104.07 \\mbox{ cm$\\cdot$ Kg}.\n\\end{align*}\n\nEsto indica que existe una relación lineal creciente entre la estatura y el peso.\n:::\n\n## Regresión\n\nEn muchos casos el objetivo de un estudio no es solo detectar una relación entre dos variables, sino explicarla mediante alguna función matemática $$y=f(x)$$ que permita predecir la variable dependiente para cada valor de la independiente.\n\nLa **regresión** es la parte de la Estadística encargada de construir esta función, que se conoce como **función de regresión** o **modelo de regresión**.\n\n### Modelos de regresión simple\n\nDependiendo de la forma de función de regresión, existen muchos tipos de\nregresión simple. Los más habituales son los que aparecen en la\nsiguiente tabla:\n\n| **Modelo**  |     **Ecuación**      |\n|:------------|:---------------------:|\n| Lineal      |       $y=a+bx$        |\n| Cuadrático  |     $y=a+bx+cx^2$     |\n| Cúbico      |  $y=a+bx+cx^2+dx^3$   |\n| Potencial   |    $y=a\\cdot x^b$     |\n| Exponencial |     $y=e^{a+bx}$      |\n| Logarítmico |     $y=a+b\\log x$     |\n| Inverso     |   $y=a+\\frac{b}{x}$   |\n| Sigmoidal   | $y=e^{a+\\frac{b}{x}}$ |\n\nLa elección de un tipo u otro depende de la forma que tenga la nube de puntos del diagrama de dispersión.\n\n### Residuos o errores predictivos\n\nUna vez elegida la familia de curvas que mejor se adapta a la nube de puntos, se determina, dentro de dicha familia, la curva que mejor se ajusta a la distribución, es decir, la función que mejor predice la variable dependiente.\n\nEl objetivo es encontrar la función de regresión que haga mínimas las distancias entre los valores de la variable dependiente observados en la muestra, y los predichos por la función de regresión. Estas distancias se conocen como _residuos_ o _errores predictivos_.\n\n:::{#def-resiudos}\n## Residuos o errores predictivos\nDado el modelo de regresión $y=f(x)$ para una variable bidimensional $(X,Y)$, el _residuo_ o _error predictivo_ de un valor $(x_i,y_j)$ observado en la muestra, es la diferencia entre el valor observado de la variable dependiente $y_j$ y el predicho por la función de regresión para $x_i$,\n\n$$e_{ij} = y_j-f(x_i).$$\n:::\n\n:::{.content-visible when-format=\"html\"}\n![Residuos de un modelo de regresión.](img/regresion/residuos_y.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Residuos de un modelo de regresión.](img/regresion/residuos_y.pdf)\n:::\n\n### Ajuste de mínimos cuadrados\n\nUna forma posible de obtener la función de regresión es mediante el método de _mínimos cuadrados_ que consiste en calcular la función que haga mínima la suma de los cuadrados de los residuos\n\n$$\\sum e_{ij}^2.$$\n\nEn el caso de un modelo de regresión lineal $f(x) = a + bx$, como la recta depende de dos parámetros (el término independiente $a$ y la pendiente $b$), la suma también dependerá de estos parámetros\n\n$$\\theta(a,b) = \\sum e_{ij}^2 =\\sum (y_j - f(x_i))^2 =\\sum (y_j-a-bx_i)^2.$$\n\nAsí pues, todo se reduce a buscar los valores $a$ y $b$ que hacen mínima esta suma.\n\nConsiderando la suma de los cuadrados de los residuos como una función de dos variables $\\theta(a,b)$, se pueden calcular los valores de los parámetros del modelo que hacen mínima esta suma derivando e igualando a 0 las derivadas con respecto a $a$ y $b$.\n\n\\begin{align*}\n\\frac{\\partial \\theta(a,b)}{\\partial a} &=  \\frac{\\partial \\sum (y_j-a-bx_i)^2 }{\\partial a} =0\\\\\n\\frac{\\partial \\theta(a,b)}{\\partial b} &=  \\frac{\\partial \\sum (y_j-a-bx_i)^2 }{\\partial b} =0\n\\end{align*}\n\nTras resolver el sistema se obtienen los valores\n\n$$\na= \\bar y - \\frac{s_{xy}}{s_x^2}\\bar x \\qquad b=\\frac{s_{xy}}{s_x^2}\n$$\n\nEstos valores hacen mínimos los residuos en $Y$ y por tanto dan la recta\nde regresión óptima.\n\n\n### Coeficiente de determinación\n\nA partir de la varianza residual se puede definir otro estadístico más sencillo de interpretar.\n\n:::{#def-coeficiente-determinacion}\n## Coeficiente de determinación muestral $r^2$\nDado un modelo de regresión simple $y=f(x)$ de una variable bidimensional $(X,Y)$, su _coeficiente de determinación muestral_ es\n\n$$r^2 = 1- \\frac{s_{ry}^2}{s_y^2}$$\n:::\n\n:::{.callout-warning}\nComo la varianza residual puede tomar valores entre 0 y $s_y^2$, se tiene que\n\n$$0\\leq r^2\\leq 1$$\n:::\n\n:::{.callout-note title=\"Interpretación\"}\nCuanto mayor sea $r^2$, mejor explicará el modelo de regresión la relación entre las variables, en particular:\n\n- Si $r^2 =0$ entonces no existe relación del tipo planteado por el modelo.\n- Si $r^2=1$ entonces la relación que plantea el modelo es perfecta.\n:::\n\n:::{.callout-warning}\nEn el caso de las rectas de regresión, el coeficiente de determinación puede calcularse con esta fórmula\n\n$$ r^2 =  \\frac{s_{xy}^2}{s_x^2s_y^2}.$$\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Demostración\n:::{.proof}\nCuando el modelo ajustado es la recta de regresión la varianza residual vale\n\n\\begin{align*}\ns_{ry}^2 & = \\sum e_{ij}^2f_{ij} = \\sum (y_j - f(x_i))^2f_{ij} = \\sum \\left(y_j - \\bar y -\\frac{s_{xy}}{s_x^2}(x_i-\\bar x) \\right)^2f_{ij}=\\\\\n& = \\sum \\left((y_j - \\bar y)^2 +\\frac{s_{xy}^2}{s_x^4}(x_i-\\bar x)^2 - 2\\frac{s_{xy}}{s_x^2}(x_i-\\bar x)(y_j -\\bar y)\\right)f_{ij} =\\\\\n& = \\sum (y_j - \\bar y)^2f_{ij} +\\frac{s_{xy}^2}{s_x^4}\\sum (x_i-\\bar x)^2f_{ij}- 2\\frac{s_{xy}}{s_x^2}\\sum (x_i-\\bar x)(y_j -\\bar y)f_{ij}=\\\\\n& = s_y^2 + \\frac{s_{xy}^2}{s_x^4}s_x^2 - 2 \\frac{s_{xy}}{s_x^2}s_{xy} = s_y^2 - \\frac{s_{xy}^2}{s_x^2}.\n\\end{align*}\n\ny, por tanto, el coeficiente de determinación lineal vale\n\n\\begin{align*}\nr^2 &= 1- \\frac{s_{ry}^2}{s_y^2} = 1- \\frac{s_y^2 - \\frac{s_{xy}^2}{s_x^2}}{s_y^2} = 1 - 1 + \\frac{s_{xy}^2}{s_x^2s_y^2} = \\frac{s_{xy}^2}{s_x^2s_y^2}.\n\\end{align*}\n:::\n:::\n\n:::{#exm-coeficiente-determinacion}\nEn el ejemplo de las estaturas y pesos se tenía\n\n$$\n\\begin{array}{lll}\n\\bar x = 174.67 \\mbox{ cm} & \\quad & s^2_x = 102.06 \\mbox{ cm}^2\\\\\n\\bar y = 69.67 \\mbox{ Kg} & & s^2_y = 164.42 \\mbox{ Kg}^2\\\\\ns_{xy} = 104.07 \\mbox{ cm$\\cdot$ Kg}\n\\end{array}\n$$\n\nDe modo que el coeficiente de determinación lineal vale\n\n$$\nr^2 \n= \\frac{s_{xy}^2}{s_x^2s_y^2} \n= \\frac{(104.07 \\mbox{ cm$ \\cdot$ Kg})^2}{102.06 \\mbox{ cm}^2 \\cdot 164.42 \\mbox{ Kg}^2} \n= 0.65.\n$$\n\nEsto indica que la recta de regresión del peso sobre la estatura explica el 65% de la variabilidad del peso, y de igual modo, la recta de regresión de la estatura sobre el peso explica el 65% de la variabilidad de la estatura.\n:::\n\n### Coeficiente de correlación lineal\n\n:::{#def-coeficiente-correlacion}\n## Coeficiente de correlación lineal muestral\nDada una variable bidimensional $(X,Y)$, el _coeficiente de correlación lineal muestral_ es la raíz cuadrada de su coeficiente de determinación lineal, con signo el de la covarianza\n\n$$\nr = \\sqrt{r^2} = \\dfrac{s_{xy}}{s_xs_y}.\n$$\n:::\n\n:::{.callout-warning}\nComo $r^2$ toma valores entre 0 y 1, $r$ tomará valores entre -1 y 1,\n\n$$-1\\leq r\\leq 1$$\n:::\n\n:::{.callout-note title=\"Interpretación\"}\nEl coeficiente de correlación lineal no sólo mide mide el grado de dependencia\nlineal sino también su dirección (creciente o decreciente):\n\n- Si $r =0$ entonces no existe relación lineal.\n- Si $r=1$ entonces existe una relación lineal creciente perfecta.\n- Si $r=-1$ entonces existe una relación lineal decreciente perfecta.\n:::\n\n:::{#exm-coeficiente-correlacion}\nEn el ejemplo de las estaturas y los pesos se tenía\n\n$$\n\\begin{array}{lll}\n\\bar x = 174.67 \\mbox{ cm} & \\quad & s^2_x = 102.06 \\mbox{ cm}^2\\\\\n\\bar y = 69.67 \\mbox{ Kg} & & s^2_y = 164.42 \\mbox{ Kg}^2\\\\\ns_{xy} = 104.07 \\mbox{ cm$\\cdot$ Kg}\n\\end{array}\n$$\n\nDe manera que el coeficiente de correlación lineal es\n\n$$\nr \n= \\frac{s_{xy}}{s_xs_y} \n= \\frac{104.07 \\mbox{ cm $\\cdot$ Kg}}{10.1 \\mbox{ cm} \\cdot 12.82 \\mbox{ Kg}} \n= +0.8.\n$$\n\nEsto indica que la relación lineal entre el peso y la estatura es fuerte, y además creciente.\n\n### Distintos grados de correlación\n\nLos siguientes diagramas de dispersión muestran modelos de regresión lineales con diferentes grados de correlación.\n\n:::{.content-visible when-format=\"html\"}\n![Modelos de regresión lineales con diferentes grados de correlación.](img/regresion/grados_correlacion.svg){width=700}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Modelos de regresión lineales con diferentes grados de correlación.](img/regresion/grados_correlacion.pdf){width=600}\n:::\n\n### Fiabilidad de las predicciones de un modelo de regresión\n\nAunque el coeficiente de determinación o el de correlación determinan la bondad de ajuste de un modelo de regresión, existen otros factores que influyen en la fiabilidad de las predicciones de un modelo de regresión:\n\n- El coeficiente de determinación: Cuanto mayor sea, menores serán los errores predictivos y mayor la fiabilidad de las predicciones.\n- La variabilidad de la población: Cuanto más variable es una población, más difícil es predecir y por tanto menos fiables serán las predicciones.\n- El tamaño muestral: Cuanto mayor sea, más información tendremos y, en consecuencia, más fiables serán las predicciones.\n\n:::{.callout-warning}\nAdemás, hay que tener en cuenta que un modelo de regresión es válido únicamente para el rango de valores observados en la muestra. Fuera de ese rango no hay información del tipo de relación entre las variables, por lo que no deben hacerse predicciones para valores lejos de los observados en la muestra.\n:::\n\n## Regresión no lineal\n\nEl ajuste de un modelo de regresión no lineal es similar al del modelo lineal y también puede realizarse mediante la técnica de mínimos cuadrados.\n\nNo obstante, en determinados casos un ajuste no lineal puede convertirse en un ajuste lineal mediante una sencilla transformación de alguna de las variables del modelo.\n\n### Transformación de modelos de regresión no lineales\n\n- **Logarítmico**: Un modelo logarítmico $y = a+b \\log x$ se convierte en un modelo lineal haciendo el cambio $t=\\log x$:\n\n    $$y=a+b\\log x = a+bt.$$\n\n- **Exponencial**: Un modelo exponencial $y = ae^{bx}$ se convierte en un modelo\n    lineal haciendo el cambio $z = \\log y$:\n\n    $$z = \\log y = \\log(ae^{bx}) =  \\log a + \\log e^{bx} = a^\\prime +bx.$$\n\n- **Potencial**: Un modelo potencial $y = ax^b$ se convierte en un modelo lineal\n    haciendo los cambios $t=\\log x$ y $z=\\log y$:\n\n    $$z = \\log y = \\log(ax^b) = \\log a + b \\log x = a^\\prime+bt.$$\n\n- **Inverso**: Un modelo inverso $y = a+b/x$ se convierte en un modelo lineal\n    haciendo el cambio $t=1/x$:\n\n    $$y = a + b(1/x) = a+bt.$$\n\n- **Sigmoidal**: Un modelo curva S $y = e^{a+b/x}$ se convierte en un modelo lineal haciendo los cambios $t=1/x$ y $z=\\log y$:\n\n    $$z = \\log y = \\log (e^{a+b/x}) = a+b(1/x) = a+bt.$$\n\n### Relación exponencial\n\n:::{#exm-regresion-exponencial}\nEl número de bacterias de un cultivo evoluciona con el tiempo según la\nsiguiente tabla:\n\n$$\\begin{array}{c|c}\n\\mbox{Horas} & \\mbox{Bacterias}\\\\\n\\hline\n0 &  25 \\\\\n1 & 28 \\\\\n2 &  47\\\\\n3 & 65 \\\\\n4 & 86\\\\\n5 & 121\\\\\n6 & 190\\\\\n7 & 290\\\\\n8 & 362\n\\end{array}\n$$\n\nEl diagrama de dispersión asociado es\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de dispersión de la evolución de bacterias.](img/regresion/evolucion_bacterias.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de dispersión de la evolución de bacterias.](img/regresion/evolucion_bacterias.pdf){width=600}\n:::\n\nSi realizamos un ajuste lineal, obtenemos la siguiente recta de regresión\n\n$$\\mbox{Bacterias} = -30.18+41,27\\,\\mbox{Horas, with } r^2=0.85.$$\n\n:::{.content-visible when-format=\"html\"}\n![Regresión lineal de la evolución de un cultivo de bacterias.](img/regresion/regresion_lineal_bacterias.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Regresión lineal de la evolución de un cultivo de bacterias.](img/regresion/regresion_lineal_bacterias.pdf){width=600}\n:::\n\n_¿Es un buen modelo?_\n\nAunque el modelo lineal no es malo, de acuerdo al diagrama de dispersión es más lógico construir un modelo exponencial o cuadrático.\n\nPara construir el modelo exponencial $y = ae^{bx}$ hay que realizar la\ntransformación $z=\\log y$, es decir, aplicar el logaritmo a la variable dependiente.\n\n$$\\begin{array}{c|c|c}\n\\mbox{Horas} & \\mbox{Bacterias} & \\mbox{$\\log$(Bacterias)}\\\\\n\\hline\n0 &  25 & 3.22\\\\\n1 & 28 & 3.33\\\\\n2 &  47 & 3.85\\\\\n3 & 65  & 4.17\\\\\n4 & 86 & 4.45\\\\\n5 & 121 & 4.80\\\\\n6 & 190 & 5.25\\\\\n7 & 290 & 5.67\\\\\n8 & 362 & 5.89\n\\end{array}\n$$\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de dispersión de la evolución del logarítmo de las bacterias de un cultivo.](/img/regresion/evolucion_log_bacterias.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de dispersión de la evolución del logarítmo de las bacterias de un cultivo.](/img/regresion/evolucion_log_bacterias.pdf){width=600}\n:::\n\nAhora sólo queda calcular la recta de regresión del logaritmo de Bacterias sobre Horas\n\n$$\\mbox{Log Bacterias} = 3.107 + 0.352\\, \\mbox{Horas}.$$\n\nY, deshaciendo el cambio de variable, se obtiene el modelo exponencial\n\n$$\\mbox{Bacterias} = e^{3.107+0.352\\,\\textrm{Horas}}, \\mbox{ con } r^2=0.99.$$\n\n:::{.content-visible when-format=\"html\"}\n![Regresión exponencial de la evolución de las bacterias de un cultivo.](img/regresion/regresion_exponencial_bacterias.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Regresión exponencial de la evolución de las bacterias de un cultivo.](img/regresion/regresion_exponencial_bacterias.pdf){width=600}\n:::\n\nComo se puede apreciar, el modelo exponencial se ajusta mucho mejor que el modelo lineal.\n\n## Riesgos de la regresión\n\n### La falta de ajuste no significa independencia\n\nEs importante señalar que cada modelo de regresión tiene su propio coeficiente de determinación.\n\n:::{.callout-warning}\nAsí, un coeficiente de determinación cercano a cero significa que no existe relación entre las variables del tipo planteado por el modelo, pero _eso no quiere decir que las variables sean independientes_, ya que puede existir relación de otro tipo.\n:::\n\n:::{.content-visible when-format=\"html\"}\n::: {layout-ncol=2}\n![Modelo de regresión lineal en una relación cuadrática.](img/regresion/regresion_lineal_relacion_cuadratica.svg){width=500}\n\n![Modelo de regresión cuadrático en una relación cuadrática.](img/regresion/regresion_cuadratica.svg){width=500}\n:::\n:::\n\n:::{.content-visible unless-format=\"html\"}\n::: {layout-ncol=2}\n![Modelo de regresión lineal en una relación cuadrática.](img/regresion/regresion_lineal_relacion_cuadratica.pdf){width=500}\n\n![Modelo de regresión cuadrático en una relación cuadrática.](img/regresion/regresion_cuadratica.pdf){width=500}\n:::\n:::\n\n### Datos atípicos en regresión\n\nLos _datos atípicos_ en un estudio de regresión son los puntos que claramente no siguen la tendencia del resto de los puntos en el diagrama de dispersión, incluso si los valores del par no se pueden considerar atípicos para cada variable por separado.\n\n:::{.content-visible when-format=\"html\"}\n![Diagrama de dispersión con un dato atípico.](img/regresion/diagrama_dispersion_con_datos_atipicos.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Diagrama de dispersión con un dato atípico.](img/regresion/diagrama_dispersion_con_datos_atipicos.pdf){width=600}\n:::\n\n\n:::{.callout-warning}\nLos datos atípicos en regresión suelen provocar cambios drásticos en el ajuste de los modelos de regresión, y por tanto, habrá que tener mucho cuidado con ellos.\n:::\n\n:::{.content-visible when-format=\"html\"}\n::: {layout-ncol=2}\n![Modelo de regresión lineal con datos atípicos.](img/regresion/regresion_lineal_con_datos_atipicos.svg){width=500}\n\n![Modelo de regresión lineal sin datos atípicos.](img/regresion/regresion_lineal_sin_datos_atipicos.svg){width=500}\n:::\n:::\n\n:::{.content-visible unless-format=\"html\"}\n::: {layout-ncol=2}\n![Modelo de regresión lineal con datos atípicos.](img/regresion/regresion_lineal_con_datos_atipicos.pdf){width=500}\n\n![Modelo de regresión lineal sin datos atípicos.](img/regresion/regresion_lineal_sin_datos_atipicos.pdf){width=500}\n:::\n:::\n\n### La paradoja de Simpson\n\nA veces, una tendencia desaparece o incluso se revierte cuando se divide la muestra en grupos de acuerdo a una variable cualitativa que está relacionada con la variable dependiente.\nEsto se conoce como la _paradoja de Simpson_.\n\n:::{#exm-paradoja-simpson}\nEl siguiente diagrama de dispersión muestra una relación inversa entre entre las horas de estudio preparando un examen y la nota del examen.\n\n:::{.content-visible when-format=\"html\"}\n![Paradoja de Simpson. Relación inversa entre las horas de estudio para un examen y la nota obtenida.](img/regresion/paradoja_simpson_1.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Paradoja de Simpson. Relación inversa entre las horas de estudio para un examen y la nota obtenida.](img/regresion/paradoja_simpson_1.pdf){width=600}\n:::\n\nPero si se divide la muestra en dos grupos (buenos y malos estudiantes) se obtienen diferentes tendencias y ahora la relación es directa, lo que tiene más lógica.\n\n:::{.content-visible when-format=\"html\"}\n![Paradoja de Simpson. Relación directa entre las horas de estudio para un examen y la nota obtenida.](img/regresion/paradoja_simpson_2.svg){width=600}\n:::\n\n:::{.content-visible unless-format=\"html\"}\n![Paradoja de Simpson. Relación directa entre las horas de estudio para un examen y la nota obtenida.](img/regresion/paradoja_simpson_2.pdf){width=600}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}